This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
public/
  favicon.svg
scripts/
  fetch-registry.mjs
src/
  data/
    registry.json
  pages/
    models/
      [id].astro
    index.astro
    models.astro
.gitignore
astro.config.mjs
LICENSE
package.json
README.md
tsconfig.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="public/favicon.svg">
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32" fill="none">
  <rect width="32" height="32" rx="6" fill="#7c3aed"/>
  <text x="16" y="23" text-anchor="middle" font-family="system-ui" font-weight="800" font-size="16" fill="white">m</text>
</svg>
</file>

<file path="scripts/fetch-registry.mjs">
#!/usr/bin/env node
/**
 * Pre-build script: fetches the latest registry index.json before Astro builds.
 * Used in CI/CD (Cloudflare Pages) so we never commit stale registry data.
 *
 * Fallback: if fetch fails, uses existing src/data/registry.json (local dev).
 */

import { writeFileSync, existsSync } from 'fs';
import { join, dirname } from 'path';
import { fileURLToPath } from 'url';

const __dirname = dirname(fileURLToPath(import.meta.url));
const OUTPUT = join(__dirname, '..', 'src', 'data', 'registry.json');

const REGISTRY_URL =
  process.env.REGISTRY_URL ||
  'https://raw.githubusercontent.com/modshq-org/mods-registry/main/index.json';

async function main() {
  console.log(`[fetch-registry] Fetching ${REGISTRY_URL}`);

  try {
    const res = await fetch(REGISTRY_URL);
    if (!res.ok) throw new Error(`HTTP ${res.status}`);
    const data = await res.json();
    writeFileSync(OUTPUT, JSON.stringify(data, null, 2));
    console.log(`[fetch-registry] âœ“ ${data.items.length} models â†’ ${OUTPUT}`);
  } catch (err) {
    if (existsSync(OUTPUT)) {
      console.warn(`[fetch-registry] âš  Fetch failed (${err.message}), using existing local copy`);
    } else {
      console.error(`[fetch-registry] âœ— Fetch failed and no local copy exists`);
      process.exit(1);
    }
  }
}

main();
</file>

<file path="src/pages/models/[id].astro">
---
import registry from '../../data/registry.json';

interface RegistryItem {
  id: string;
  name: string;
  type: string;
  architecture?: string;
  author: string;
  license: string;
  homepage: string;
  description: string;
  file?: { url: string; sha256: string; size: number; format: string };
  variants?: { id: string; label?: string; file: { url: string; sha256: string; size: number; format: string }; vram_required?: number; vram_recommended?: number; note?: string; precision?: string }[];
  tags: string[];
  rating: number;
  downloads: number;
  added: string;
  updated: string;
  requires?: { id: string; type: string; reason: string }[];
  preview_images?: string[];
  base_models?: string[];
  auth?: { provider: string; gated: boolean; terms_url: string };
  defaults?: Record<string, any>;
}

export function getStaticPaths() {
  return (registry.items as RegistryItem[]).map((item) => ({
    params: { id: item.id },
    props: { model: item },
  }));
}

const { model } = Astro.props as { model: RegistryItem };
const allItems = registry.items as RegistryItem[];

// Helpers
function formatSize(bytes: number): string {
  if (bytes >= 1e9) return `${(bytes / 1e9).toFixed(1)} GB`;
  if (bytes >= 1e6) return `${(bytes / 1e6).toFixed(0)} MB`;
  return `${(bytes / 1e3).toFixed(0)} KB`;
}

function formatDownloads(n: number | undefined): string {
  if (!n) return 'â€”';
  if (n >= 1e6) return `${(n / 1e6).toFixed(1)}M`;
  if (n >= 1e3) return `${(n / 1e3).toFixed(0)}K`;
  return n.toLocaleString();
}

function formatRating(r: number | undefined): string {
  if (!r) return 'â˜†â˜†â˜†â˜†â˜†';
  const full = Math.floor(r);
  const half = r - full >= 0.3;
  return 'â˜…'.repeat(full) + (half ? 'Â½' : '') + 'â˜†'.repeat(5 - full - (half ? 1 : 0));
}

function getSize(item: RegistryItem): string {
  if (item.file) return formatSize(item.file.size);
  if (item.variants && item.variants.length > 0) {
    const sizes = item.variants.map((v: any) => v.file?.size).filter((s: any) => s > 0);
    if (sizes.length === 0) return 'â€”';
    const min = Math.min(...sizes);
    const max = Math.max(...sizes);
    if (min === max) return formatSize(min);
    return `${formatSize(min)}â€“${formatSize(max)}`;
  }
  return 'â€”';
}

const typeColors: Record<string, string> = {
  checkpoint: '#3b82f6',
  diffusion_model: '#8b5cf6',
  text_encoder: '#06b6d4',
  lora: '#f59e0b',
  controlnet: '#10b981',
  ipadapter: '#ec4899',
  upscaler: '#f97316',
  vae: '#6366f1',
  embedding: '#84cc16',
  segmentation: '#14b8a6',
};

const typeLabels: Record<string, string> = {
  checkpoint: 'Checkpoint',
  diffusion_model: 'Diffusion Model',
  text_encoder: 'Text Encoder',
  lora: 'LoRA',
  controlnet: 'ControlNet',
  ipadapter: 'IP-Adapter',
  upscaler: 'Upscaler',
  vae: 'VAE',
  embedding: 'Embedding',
  segmentation: 'Segmentation',
};

const typeColor = typeColors[model.type] || '#888';
const typeLabel = typeLabels[model.type] || model.type;
const description = model.description || '';
const descLines = description.split('\n').filter((l: string) => l.trim());

// Resolve dependency models
const deps = (model.requires || []).map(dep => {
  const resolved = allItems.find(i => i.id === dep.id);
  return { ...dep, resolved };
});

// Find models that depend on this one
const dependents = allItems.filter(item =>
  (item.requires || []).some(r => r.id === model.id)
);

// Install command
const installCmd = model.variants && model.variants.length > 1
  ? `mods install ${model.id}`
  : `mods install ${model.id}`;
---

<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>{model.name} â€” mods registry</title>
  <meta name="description" content={`Install ${model.name} with mods. ${descLines[0] || ''}`} />
  <meta property="og:title" content={`${model.name} â€” mods`} />
  <meta property="og:description" content={`Install ${model.name} with a single command: mods install ${model.id}`} />
  <meta property="og:type" content="website" />
  <meta property="og:url" content={`https://mods.sh/models/${model.id}`} />
  <link rel="canonical" href={`https://mods.sh/models/${model.id}`} />
  <link rel="icon" type="image/svg+xml" href="/favicon.svg" />
  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-3341W5L66B"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-3341W5L66B');
  </script>
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;700&family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet" />
</head>
<body>

  <!-- Nav -->
  <nav class="nav">
    <div class="nav-inner">
      <a href="/" class="nav-brand"><span class="gradient-text">mods</span></a>
      <div class="nav-links">
        <a href="/models" class="nav-link active">Models</a>
        <a href="https://github.com/modshq-org/mods" class="nav-link" target="_blank" rel="noopener">Docs</a>
        <a href="https://github.com/modshq-org/mods" class="nav-link" target="_blank" rel="noopener">GitHub</a>
      </div>
    </div>
  </nav>

  <main class="container">
    <!-- Breadcrumb -->
    <nav class="breadcrumb">
      <a href="/models">Models</a>
      <span class="sep">/</span>
      <span>{model.id}</span>
    </nav>

    <!-- Model Header -->
    <header class="model-header">
      <div class="header-top">
        <span
          class="type-badge"
          style={`background: ${typeColor}18; color: ${typeColor}; border-color: ${typeColor}30`}
        >
          {typeLabel}
        </span>
        {model.auth?.gated && (
          <span class="gated-badge">ðŸ”’ Gated</span>
        )}
      </div>

      <h1>{model.name}</h1>

      <div class="header-meta">
        <span class="author">by <a href={model.homepage} target="_blank" rel="noopener">{model.author}</a></span>
        <span class="meta-sep">Â·</span>
        <span class="rating" title={`${model.rating}/5`}>{formatRating(model.rating)} {model.rating}</span>
        <span class="meta-sep">Â·</span>
        <span class="downloads">{formatDownloads(model.downloads)} downloads</span>
        <span class="meta-sep">Â·</span>
        <span class="size">{getSize(model)}</span>
        {model.architecture && (
          <>
            <span class="meta-sep">Â·</span>
            <span class="arch">{model.architecture}</span>
          </>
        )}
      </div>
    </header>

    <!-- Install Box -->
    <section class="install-box">
      <div class="install-label">Install with mods</div>
      <div class="install-cmd">
        <code id="install-cmd">{installCmd}</code>
        <button class="copy-btn" id="copy-btn" title="Copy to clipboard">
          <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect width="14" height="14" x="8" y="8" rx="2"/><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"/></svg>
        </button>
      </div>
      {model.variants && model.variants.length > 1 && (
        <div class="variant-hint">
          <div class="variant-default">
            <span class="default-icon">âš¡</span>
            <span>Running <code>mods install {model.id}</code> without <code>--variant</code> auto-selects the best variant for your GPU VRAM.</span>
          </div>
          <div class="variant-list-hint">
            Available variants: {model.variants.map((v: any) => v.id).join(', ')}
            <br/>
            <code class="variant-example">mods install {model.id} --variant {model.variants[0].id}</code>
          </div>
        </div>
      )}
      {model.auth?.gated && (
        <div class="gated-notice">
          <span class="gated-icon">ðŸ”’</span>
          This model requires accepting terms on <a href={model.auth.terms_url} target="_blank" rel="noopener">HuggingFace</a> and running <code>mods auth huggingface</code> first.
        </div>
      )}
    </section>

    <!-- Two-column layout -->
    <div class="content-grid">
      <!-- Left: Description + Details -->
      <div class="content-main">
        <section class="section">
          <h2>About</h2>
          <div class="description">
            {descLines.map((line: string) => <p>{line}</p>)}
          </div>
        </section>

        {model.preview_images && model.preview_images.length > 0 && (
          <section class="section">
            <h2>Preview</h2>
            <div class="preview-grid">
              {model.preview_images.map((url: string) => (
                <img src={url} alt={`${model.name} preview`} loading="lazy" class="preview-img" />
              ))}
            </div>
          </section>
        )}

        {model.variants && model.variants.length > 0 && (
          <section class="section">
            <h2>Variants</h2>
            <div class="variants-table">
              <table>
                <thead>
                  <tr>
                    <th>Variant</th>
                    <th>Format</th>
                    <th>Size</th>
                    <th>VRAM</th>
                    <th>Install</th>
                  </tr>
                </thead>
                <tbody>
                  {model.variants.map((v: any) => (
                    <tr>
                      <td class="variant-id">
                        {v.id}
                        {v.note && <span class="variant-note" title={v.note}>â“˜</span>}
                      </td>
                      <td>{v.file?.format || v.format || 'â€”'}</td>
                      <td class="mono">{v.file?.size ? formatSize(v.file.size) : (v.size ? formatSize(v.size) : 'â€”')}</td>
                      <td class="mono vram-cell">
                        {v.vram_required
                          ? `${(v.vram_required / 1024).toFixed(0)}+ GB`
                          : 'â€”'}
                      </td>
                      <td>
                        <code class="variant-cmd">mods install {model.id} --variant {v.id}</code>
                      </td>
                    </tr>
                  ))}
                </tbody>
              </table>
            </div>
          </section>
        )}

        {deps.length > 0 && (
          <section class="section">
            <h2>Dependencies</h2>
            <div class="auto-deps-notice">
              <span class="auto-deps-icon">ðŸ“¦</span>
              <span>These models are <strong>automatically installed</strong> when you run <code>mods install {model.id}</code>. No extra steps needed â€” mods resolves and downloads all dependencies for you.</span>
            </div>
            <div class="dep-cards">
              {deps.map(dep => (
                <a href={dep.resolved ? `/models/${dep.id}` : '#'} class="dep-card">
                  <span class="dep-type">{dep.type.replace('_', ' ')}</span>
                  <span class="dep-name">{dep.resolved?.name || dep.id}</span>
                  <span class="dep-reason">{dep.reason}</span>
                </a>
              ))}
            </div>
          </section>
        )}

        {dependents.length > 0 && (
          <section class="section">
            <h2>Used By</h2>
            <div class="dep-cards">
              {dependents.map(item => (
                <a href={`/models/${item.id}`} class="dep-card">
                  <span class="dep-type">{item.type.replace('_', ' ')}</span>
                  <span class="dep-name">{item.name}</span>
                </a>
              ))}
            </div>
          </section>
        )}
      </div>

      <!-- Right: Sidebar -->
      <aside class="content-sidebar">
        <div class="sidebar-card">
          <h3>Details</h3>
          <dl class="details-list">
            <dt>License</dt>
            <dd>{model.license}</dd>
            <dt>Type</dt>
            <dd>{typeLabel}</dd>
            {model.architecture && (
              <>
                <dt>Architecture</dt>
                <dd>{model.architecture}</dd>
              </>
            )}
            {model.file?.format && (
              <>
                <dt>Format</dt>
                <dd>{model.file.format}</dd>
              </>
            )}
            <dt>Added</dt>
            <dd>{model.added}</dd>
            <dt>Updated</dt>
            <dd>{model.updated}</dd>
          </dl>
        </div>

        <div class="sidebar-card">
          <h3>Tags</h3>
          <div class="tag-list">
            {model.tags.map((tag: string) => (
              <span class="tag">#{tag}</span>
            ))}
          </div>
        </div>

        <div class="sidebar-card">
          <h3>Links</h3>
          <ul class="links-list">
            <li><a href={model.homepage} target="_blank" rel="noopener">Homepage â†—</a></li>
            {model.file?.url && (
              <li><a href={model.file.url} target="_blank" rel="noopener">Direct download â†—</a></li>
            )}
            <li><a href={`https://github.com/modshq-org/mods-registry/blob/main/manifests/${model.type === 'diffusion_model' ? 'diffusion_models' : model.type + 's'}/${model.id}.yaml`} target="_blank" rel="noopener">Manifest source â†—</a></li>
          </ul>
        </div>
      </aside>
    </div>
  </main>

  <!-- Footer -->
  <footer>
    <div class="container">
      <div class="footer-content">
        <div class="footer-brand">
          <span class="gradient-text footer-logo">mods</span>
          <p>Open source model manager for AI image generation.</p>
        </div>
        <div class="footer-links">
          <a href="/">Home</a>
          <a href="/models">Models</a>
          <a href="https://github.com/modshq-org/mods" target="_blank" rel="noopener">CLI</a>
          <a href="https://github.com/modshq-org/mods-registry" target="_blank" rel="noopener">Registry</a>
        </div>
      </div>
      <div class="footer-bottom">
        <p>Built with Rust. Licensed MIT.</p>
      </div>
    </div>
  </footer>

  <script>
    const copyBtn = document.getElementById('copy-btn')!;
    const cmdEl = document.getElementById('install-cmd')!;
    copyBtn.addEventListener('click', async () => {
      await navigator.clipboard.writeText(cmdEl.textContent || '');
      const svg = copyBtn.querySelector('svg')!;
      const orig = svg.outerHTML;
      svg.outerHTML = '<svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="#22c55e" stroke-width="2"><polyline points="20 6 9 17 4 12"/></svg>';
      setTimeout(() => { copyBtn.querySelector('svg')!.outerHTML = orig; }, 1500);
    });
  </script>
</body>
</html>

<style>
  :root {
    --bg: #0a0a0f;
    --bg-card: #12121a;
    --bg-card-hover: #1a1a28;
    --border: #1e1e2e;
    --border-light: #2a2a3e;
    --text: #e4e4ef;
    --text-dim: #8888a0;
    --text-dimmer: #555570;
    --accent: #7c3aed;
    --accent-light: #a78bfa;
    --success: #22c55e;
    --warning: #eab308;
    --font-sans: 'Inter', system-ui, -apple-system, sans-serif;
    --font-mono: 'JetBrains Mono', 'Fira Code', monospace;
  }

  * { margin: 0; padding: 0; box-sizing: border-box; }
  html { scroll-behavior: smooth; }

  body {
    background: var(--bg);
    color: var(--text);
    font-family: var(--font-sans);
    line-height: 1.6;
    -webkit-font-smoothing: antialiased;
    min-height: 100vh;
    display: flex;
    flex-direction: column;
  }

  main { flex: 1; }

  .container {
    max-width: 1200px;
    margin: 0 auto;
    padding: 0 24px;
  }

  .gradient-text {
    background: linear-gradient(135deg, var(--accent-light), #c084fc, #f472b6);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
  }

  /* Nav */
  .nav {
    position: sticky;
    top: 0;
    z-index: 100;
    background: rgba(10, 10, 15, 0.85);
    backdrop-filter: blur(12px);
    border-bottom: 1px solid var(--border);
  }

  .nav-inner {
    max-width: 1200px;
    margin: 0 auto;
    padding: 0 24px;
    height: 60px;
    display: flex;
    align-items: center;
    justify-content: space-between;
  }

  .nav-brand { font-size: 1.5rem; font-weight: 800; text-decoration: none; }
  .nav-links { display: flex; gap: 24px; }
  .nav-link { color: var(--text-dim); text-decoration: none; font-size: 0.9rem; font-weight: 500; transition: color 0.2s; }
  .nav-link:hover, .nav-link.active { color: var(--accent-light); }

  /* Breadcrumb */
  .breadcrumb {
    padding: 20px 0;
    font-size: 0.85rem;
    color: var(--text-dimmer);
  }

  .breadcrumb a {
    color: var(--accent-light);
    text-decoration: none;
  }

  .breadcrumb a:hover { text-decoration: underline; }
  .breadcrumb .sep { margin: 0 8px; }

  /* Model Header */
  .model-header {
    padding-bottom: 32px;
    border-bottom: 1px solid var(--border);
    margin-bottom: 32px;
  }

  .header-top {
    display: flex;
    align-items: center;
    gap: 8px;
    margin-bottom: 16px;
  }

  .type-badge {
    display: inline-block;
    padding: 4px 12px;
    border-radius: 100px;
    font-size: 0.75rem;
    font-weight: 600;
    letter-spacing: 0.02em;
    text-transform: uppercase;
    border: 1px solid;
  }

  .gated-badge {
    font-size: 0.75rem;
    color: var(--warning);
    background: rgba(234, 179, 8, 0.1);
    padding: 4px 12px;
    border-radius: 100px;
    border: 1px solid rgba(234, 179, 8, 0.2);
  }

  .model-header h1 {
    font-size: 2.5rem;
    font-weight: 800;
    letter-spacing: -0.03em;
    line-height: 1.2;
    margin-bottom: 12px;
  }

  .header-meta {
    display: flex;
    align-items: center;
    flex-wrap: wrap;
    gap: 4px;
    font-size: 0.9rem;
    color: var(--text-dim);
  }

  .header-meta a { color: var(--accent-light); text-decoration: none; }
  .header-meta a:hover { text-decoration: underline; }
  .meta-sep { color: var(--text-dimmer); margin: 0 4px; }
  .rating { color: var(--warning); }
  .arch { font-family: var(--font-mono); font-size: 0.85rem; color: var(--text-dimmer); }

  /* Install Box */
  .install-box {
    background: var(--bg-card);
    border: 1px solid var(--border);
    border-radius: 14px;
    padding: 24px;
    margin-bottom: 32px;
  }

  .install-label {
    font-size: 0.8rem;
    color: var(--text-dimmer);
    text-transform: uppercase;
    letter-spacing: 0.05em;
    font-weight: 600;
    margin-bottom: 12px;
  }

  .install-cmd {
    display: flex;
    align-items: center;
    gap: 12px;
    padding: 16px 20px;
    background: #0a0a12;
    border: 1px solid var(--border);
    border-radius: 10px;
  }

  .install-cmd code {
    flex: 1;
    font-family: var(--font-mono);
    font-size: 1.05rem;
    color: var(--accent-light);
    font-weight: 500;
  }

  .copy-btn {
    background: none;
    border: none;
    color: var(--text-dimmer);
    cursor: pointer;
    padding: 8px;
    border-radius: 6px;
    transition: color 0.15s, background 0.15s;
    display: flex;
    align-items: center;
    flex-shrink: 0;
  }

  .copy-btn:hover {
    color: var(--text);
    background: rgba(255,255,255,0.05);
  }

  .variant-hint {
    margin-top: 12px;
    font-size: 0.82rem;
    color: var(--text-dimmer);
    line-height: 1.8;
  }

  .variant-default {
    display: flex;
    align-items: center;
    gap: 8px;
    padding: 10px 14px;
    background: rgba(34, 197, 94, 0.06);
    border: 1px solid rgba(34, 197, 94, 0.15);
    border-radius: 8px;
    margin-bottom: 8px;
    font-size: 0.82rem;
    color: var(--text-dim);
  }

  .variant-default code {
    font-family: var(--font-mono);
    color: var(--accent-light);
    font-size: 0.78rem;
  }

  .default-icon { font-size: 1rem; }

  .variant-list-hint {
    font-size: 0.82rem;
    color: var(--text-dimmer);
    line-height: 1.8;
  }

  .variant-hint code, .variant-example {
    font-family: var(--font-mono);
    color: var(--text-dim);
    font-size: 0.78rem;
  }

  .variant-note {
    margin-left: 4px;
    font-size: 0.7rem;
    color: var(--text-dimmer);
    cursor: help;
  }

  .vram-cell {
    color: var(--success);
    font-size: 0.78rem;
  }

  .auto-deps-notice {
    display: flex;
    align-items: flex-start;
    gap: 10px;
    padding: 12px 16px;
    background: rgba(124, 58, 237, 0.06);
    border: 1px solid rgba(124, 58, 237, 0.15);
    border-radius: 8px;
    font-size: 0.85rem;
    color: var(--text-dim);
    margin-bottom: 14px;
  }

  .auto-deps-notice strong { color: var(--text); }
  .auto-deps-notice code { font-family: var(--font-mono); color: var(--accent-light); font-size: 0.8rem; }
  .auto-deps-icon { font-size: 1.1rem; flex-shrink: 0; margin-top: 1px; }

  .gated-notice {
    margin-top: 14px;
    padding: 12px 16px;
    background: rgba(234, 179, 8, 0.06);
    border: 1px solid rgba(234, 179, 8, 0.15);
    border-radius: 8px;
    font-size: 0.85rem;
    color: var(--text-dim);
    display: flex;
    align-items: center;
    gap: 8px;
  }

  .gated-notice a { color: var(--accent-light); }
  .gated-notice code { font-family: var(--font-mono); color: var(--accent-light); font-size: 0.8rem; }

  /* Content Grid */
  .content-grid {
    display: grid;
    grid-template-columns: 1fr 320px;
    gap: 32px;
    padding-bottom: 64px;
  }

  .section {
    margin-bottom: 36px;
  }

  .section h2 {
    font-size: 1.3rem;
    font-weight: 700;
    margin-bottom: 16px;
    text-align: left;
    letter-spacing: -0.01em;
  }

  .section-note {
    font-size: 0.85rem;
    color: var(--text-dimmer);
    margin-bottom: 14px;
  }

  .description p {
    color: var(--text-dim);
    font-size: 0.95rem;
    line-height: 1.7;
    margin-bottom: 8px;
  }

  /* Preview Images */
  .preview-grid {
    display: grid;
    grid-template-columns: repeat(auto-fill, minmax(280px, 1fr));
    gap: 12px;
  }

  .preview-img {
    width: 100%;
    border-radius: 10px;
    border: 1px solid var(--border);
    background: var(--bg-card);
  }

  /* Variants Table */
  .variants-table {
    overflow-x: auto;
    border: 1px solid var(--border);
    border-radius: 10px;
  }

  .variants-table table {
    width: 100%;
    border-collapse: collapse;
    font-size: 0.85rem;
  }

  .variants-table th {
    background: var(--bg-card);
    color: var(--text-dimmer);
    font-weight: 600;
    text-transform: uppercase;
    font-size: 0.72rem;
    letter-spacing: 0.04em;
    padding: 10px 16px;
    text-align: left;
    border-bottom: 1px solid var(--border);
  }

  .variants-table td {
    padding: 10px 16px;
    border-bottom: 1px solid var(--border);
    color: var(--text-dim);
  }

  .variants-table tr:last-child td { border-bottom: none; }
  .variant-id { font-weight: 600; color: var(--text); }
  .mono { font-family: var(--font-mono); font-size: 0.8rem; }
  .variant-cmd { font-family: var(--font-mono); font-size: 0.75rem; color: var(--accent-light); }

  /* Dependency Cards */
  .dep-cards {
    display: grid;
    grid-template-columns: repeat(auto-fill, minmax(220px, 1fr));
    gap: 12px;
  }

  .dep-card {
    display: flex;
    flex-direction: column;
    gap: 4px;
    padding: 16px;
    background: var(--bg-card);
    border: 1px solid var(--border);
    border-radius: 10px;
    text-decoration: none;
    transition: all 0.15s;
  }

  .dep-card:hover {
    border-color: var(--border-light);
    background: var(--bg-card-hover);
    transform: translateY(-1px);
  }

  .dep-type {
    font-size: 0.7rem;
    color: var(--text-dimmer);
    text-transform: uppercase;
    font-weight: 600;
    letter-spacing: 0.03em;
  }

  .dep-name {
    font-size: 0.95rem;
    font-weight: 600;
    color: var(--text);
  }

  .dep-reason {
    font-size: 0.78rem;
    color: var(--text-dimmer);
  }

  /* Sidebar */
  .sidebar-card {
    background: var(--bg-card);
    border: 1px solid var(--border);
    border-radius: 14px;
    padding: 20px;
    margin-bottom: 16px;
  }

  .sidebar-card h3 {
    font-size: 0.82rem;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.04em;
    color: var(--text-dimmer);
    margin-bottom: 14px;
  }

  .details-list {
    display: grid;
    grid-template-columns: auto 1fr;
    gap: 8px 16px;
    font-size: 0.85rem;
  }

  .details-list dt {
    color: var(--text-dimmer);
    font-weight: 500;
  }

  .details-list dd {
    color: var(--text-dim);
    font-family: var(--font-mono);
    font-size: 0.82rem;
  }

  .tag-list {
    display: flex;
    flex-wrap: wrap;
    gap: 6px;
  }

  .tag {
    font-size: 0.75rem;
    color: var(--text-dimmer);
    background: rgba(255, 255, 255, 0.03);
    padding: 3px 10px;
    border-radius: 4px;
  }

  .links-list {
    list-style: none;
  }

  .links-list li {
    padding: 6px 0;
    border-bottom: 1px solid var(--border);
  }

  .links-list li:last-child { border-bottom: none; }

  .links-list a {
    color: var(--accent-light);
    text-decoration: none;
    font-size: 0.85rem;
    transition: color 0.15s;
  }

  .links-list a:hover { color: var(--text); }

  /* Footer */
  footer {
    padding: 60px 0 40px;
    border-top: 1px solid var(--border);
    margin-top: auto;
  }

  .footer-content {
    display: flex;
    justify-content: space-between;
    align-items: flex-start;
    margin-bottom: 32px;
  }

  .footer-logo { font-size: 1.8rem; font-weight: 800; display: block; margin-bottom: 8px; }
  .footer-brand p { color: var(--text-dim); font-size: 0.9rem; }
  .footer-links { display: flex; gap: 24px; }
  .footer-links a { color: var(--text-dim); text-decoration: none; font-size: 0.9rem; transition: color 0.2s; }
  .footer-links a:hover { color: var(--accent-light); }
  .footer-bottom { text-align: center; color: var(--text-dim); font-size: 0.8rem; padding-top: 24px; border-top: 1px solid var(--border); }

  /* Responsive */
  @media (max-width: 768px) {
    .model-header h1 { font-size: 1.8rem; }
    .content-grid { grid-template-columns: 1fr; }
    .content-sidebar { order: -1; }
    .header-meta { font-size: 0.8rem; }
    .install-cmd code { font-size: 0.85rem; }
    .footer-content { flex-direction: column; gap: 24px; }
  }
</style>
</file>

<file path=".gitignore">
# build output
dist/
# generated types
.astro/

# dependencies
node_modules/

# logs
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*


# environment variables
.env
.env.production

# macOS-specific files
.DS_Store

# jetbrains setting folder
.idea/
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2026 modshq-org

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="tsconfig.json">
{
  "extends": "astro/tsconfigs/strict",
  "include": [".astro/types.d.ts", "**/*"],
  "exclude": ["dist"]
}
</file>

<file path="astro.config.mjs">
// @ts-check
import { defineConfig } from 'astro/config';

// https://astro.build/config
export default defineConfig({
  site: 'https://mods.sh',
  server: {
    host: '0.0.0.0',
  },
  build: {
    assets: '_assets',
  },
});
</file>

<file path="package.json">
{
  "name": "mods-site",
  "type": "module",
  "version": "0.0.1",
  "scripts": {
    "dev": "astro dev",
    "prebuild": "node scripts/fetch-registry.mjs",
    "build": "astro build",
    "preview": "astro preview",
    "astro": "astro"
  },
  "dependencies": {
    "astro": "^5.17.1"
  }
}
</file>

<file path="README.md">
# mods-site

Website for [mods](https://github.com/modshq-org/mods) â€” the CLI model manager for AI image generation. Browse, search, and discover models at [mods.sh](https://mods.sh).

## What's here

- **Landing page** â€” product overview, features, install instructions
- **Model browser** (`/models`) â€” search, filter by type, sort 100+ models
- **Model detail pages** (`/models/[id]`) â€” variants, VRAM requirements, dependencies, install commands

## Development

```bash
npm install
npm run dev          # Dev server at localhost:4321
npm run build        # Production build to ./dist/
npm run preview      # Preview production build
```

The `prebuild` step fetches the latest registry data from [mods-registry](https://github.com/modshq-org/mods-registry) before building.

## Tech stack

- [Astro](https://astro.build) 5 â€” static site generation
- TypeScript
- Vanilla CSS with CSS custom properties

## Related

- [mods](https://github.com/modshq-org/mods) â€” CLI tool
- [mods-registry](https://github.com/modshq-org/mods-registry) â€” model manifests
</file>

<file path="src/data/registry.json">
{
  "version": 1,
  "items": [
    {
      "id": "4x-ultrasharp",
      "name": "4x UltraSharp Upscaler",
      "type": "upscaler",
      "author": "Kim2091",
      "license": "cc-by-nc-sa-4.0",
      "homepage": "https://openmodeldb.info/models/4x-UltraSharp",
      "description": "High-quality 4x upscaler. Excellent for photo-realistic upscaling.\nOne of the most popular upscalers in the AI image generation community.\n",
      "scale_factor": 4,
      "file": {
        "url": "https://huggingface.co/Kim2091/UltraSharp/resolve/main/4x-UltraSharp.pth",
        "sha256": "a5812231fc936b42af08a5edba784195495d303d5b3248c24489ef0c4021fe01",
        "size": 66921375,
        "format": "pth"
      },
      "tags": [
        "upscaler",
        "4x",
        "photo-realistic",
        "esrgan"
      ],
      "rating": 4.9,
      "downloads": 3200000,
      "added": "2023-01-01",
      "updated": "2024-01-01"
    },
    {
      "id": "birefnet-dis",
      "name": "BiRefNet DIS (Dichotomous Image Segmentation)",
      "type": "segmentation",
      "architecture": "birefnet",
      "author": "ViperYX",
      "license": "mit",
      "homepage": "https://huggingface.co/ViperYX/BiRefNet",
      "description": "High-quality background removal / image segmentation model.\nBiRefNet (Bilateral Reference Network) trained on DIS5K dataset for\ndichotomous image segmentation â€” excels at precise foreground/background separation.\nUsed in ComfyUI via ComfyUI_BiRefNet_ll custom node.\n",
      "file": {
        "url": "https://huggingface.co/ViperYX/BiRefNet/resolve/main/BiRefNet-DIS_ep580.pth",
        "sha256": "9b4510f31d72e41507a4b75c4e62206b1d7e2223e0125b29644acd4b142793b0",
        "size": 889192448,
        "format": "pth"
      },
      "tags": [
        "segmentation",
        "background-removal",
        "birefnet",
        "matting",
        "comfyui"
      ],
      "rating": 4.8,
      "downloads": 65000,
      "added": "2024-03-01",
      "updated": "2024-03-01"
    },
    {
      "id": "bsrganx2",
      "name": "BSRGANx2 Upscaler",
      "type": "upscaler",
      "author": "cszn",
      "license": "apache-2.0",
      "homepage": "https://github.com/cszn/BSRGAN",
      "description": "2x upscaler based on BSRGAN (Blind Super-Resolution GAN).\nBetter than bicubic for real-world degraded/noisy/compressed images.\nHandles JPEG artifacts, noise, and blur well â€” good for product photos\nthat have been through lossy compression.\n",
      "scale_factor": 2,
      "file": {
        "url": "https://huggingface.co/uwg/upscaler/resolve/main/ESRGAN/BSRGANx2.pth",
        "sha256": "eef929a6e8c39f68d8c921bd9d5fd2ae7202f32d982acf514e235a18f7f3d4f6",
        "size": 71849987,
        "format": "pth"
      },
      "tags": [
        "upscaler",
        "2x",
        "bsrgan",
        "denoising",
        "real-world",
        "esrgan"
      ],
      "rating": 4.6,
      "downloads": 450000,
      "added": "2022-01-01",
      "updated": "2022-01-01"
    },
    {
      "id": "clip-l",
      "name": "CLIP-L Text Encoder",
      "type": "text_encoder",
      "architecture": "clip",
      "author": "comfyanonymous",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/comfyanonymous/flux_text_encoders",
      "description": "CLIP-L (Large) text encoder. Used as secondary text encoder by FLUX models\nalongside T5-XXL. Small and lightweight â€” 246 MB.\n",
      "file": {
        "url": "https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors",
        "sha256": "660c6f5b1abae9dc498ac2d21e1347d2abdb0cf6c0c0c8576cd796491d9a6cdd",
        "size": 246144152,
        "format": "safetensors"
      },
      "tags": [
        "clip",
        "text-encoder",
        "flux",
        "lightweight"
      ],
      "rating": 5.0,
      "downloads": 2600000,
      "added": "2024-08-01",
      "updated": "2024-08-01"
    },
    {
      "id": "flux-canny-dev",
      "name": "FLUX.1 Canny Dev",
      "type": "diffusion_model",
      "architecture": "flux",
      "author": "black-forest-labs",
      "license": "flux-1-dev-non-commercial-license",
      "homepage": "https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev",
      "description": "FLUX.1 Canny Dev â€” canny edge conditioned generation model on FLUX architecture.\nStructural control via canny edge maps. Gated model.\n",
      "auth": {
        "provider": "huggingface",
        "gated": true,
        "terms_url": "https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev"
      },
      "file": {
        "url": "https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev/resolve/main/flux1-canny-dev.safetensors",
        "sha256": "996876670169591cb412b937fbd46ea14cbed6933aef17c48a2dcd9685c98cdb",
        "size": 23803351736,
        "format": "safetensors"
      },
      "requires": [
        {
          "id": "clip-l",
          "type": "text_encoder",
          "reason": "CLIP-L text encoder for FLUX"
        },
        {
          "id": "t5-xxl-fp8",
          "type": "text_encoder",
          "reason": "T5-XXL text encoder for FLUX"
        },
        {
          "id": "flux-vae",
          "type": "vae",
          "reason": "FLUX VAE"
        }
      ],
      "tags": [
        "flux",
        "controlnet",
        "canny",
        "edge-detection"
      ],
      "rating": 4.7,
      "downloads": 480000,
      "added": "2024-10-01",
      "updated": "2025-01-01"
    },
    {
      "id": "flux-depth-controlnet",
      "name": "FLUX.1 Depth ControlNet",
      "type": "controlnet",
      "architecture": "flux",
      "author": "InstantX",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/InstantX/FLUX.1-dev-Controlnet-Union",
      "description": "Depth-conditioned ControlNet for FLUX.1 Dev. Allows generating images\nthat follow the depth structure of an input image.\n",
      "base_models": [
        "flux-dev"
      ],
      "preprocessor": "depth_midas",
      "file": {
        "url": "https://huggingface.co/InstantX/FLUX.1-dev-Controlnet-Union/resolve/main/diffusion_pytorch_model.safetensors",
        "sha256": "0ec7deeff0e8d407744dbc96ce867b842ff11d51cc0c6132a3bd271091f041e1",
        "size": 6596901888,
        "format": "safetensors"
      },
      "tags": [
        "flux",
        "controlnet",
        "depth",
        "instantx"
      ],
      "rating": 4.5,
      "downloads": 380000,
      "added": "2024-10-01",
      "updated": "2025-01-01"
    },
    {
      "id": "flux-dev",
      "name": "FLUX.1 Dev",
      "type": "checkpoint",
      "architecture": "flux",
      "author": "black-forest-labs",
      "license": "flux-1-dev-non-commercial",
      "homepage": "https://huggingface.co/black-forest-labs/FLUX.1-dev",
      "description": "High-quality text-to-image model from Black Forest Labs.\nBest results with 20-30 steps, CFG 3.5-4.0, euler sampler.\nNon-commercial license. Requires accepting terms on HuggingFace.\n",
      "variants": [
        {
          "id": "fp16",
          "file": "flux1-dev.safetensors",
          "url": "https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors",
          "sha256": "4610115bb0c89560703c892c59ac2742fa821e60ef5871b33493ba544683abd7",
          "size": 23802932552,
          "format": "safetensors",
          "precision": "fp16",
          "vram_required": 24576,
          "vram_recommended": 24576
        },
        {
          "id": "fp8",
          "file": "flux1-dev-fp8-e4m3fn.safetensors",
          "url": "https://huggingface.co/Kijai/flux-fp8/resolve/main/flux1-dev-fp8-e4m3fn.safetensors",
          "sha256": "e2c62660c9d75e928da6c0b9930993190f152f3c5c6f0cca69e77c21db87dd58",
          "size": 11903959040,
          "format": "safetensors",
          "precision": "fp8-e4m3fn",
          "vram_required": 12288,
          "vram_recommended": 16384,
          "note": "Quantized to fp8. Slight quality reduction vs fp16."
        }
      ],
      "requires": [
        {
          "id": "flux-vae",
          "type": "vae",
          "reason": "Flux models require the Flux-specific VAE (ae.safetensors)"
        },
        {
          "id": "t5-xxl-fp16",
          "type": "text_encoder",
          "reason": "T5-XXL for prompt processing",
          "optional_variant": "t5-xxl-fp8"
        },
        {
          "id": "clip-l",
          "type": "text_encoder",
          "reason": "CLIP-L for secondary text encoding"
        }
      ],
      "auth": {
        "provider": "huggingface",
        "terms_url": "https://huggingface.co/black-forest-labs/FLUX.1-dev",
        "gated": true
      },
      "defaults": {
        "steps": 20,
        "cfg": 3.5,
        "sampler": "euler",
        "scheduler": "normal"
      },
      "tags": [
        "flux",
        "text-to-image",
        "high-quality",
        "bfl"
      ],
      "rating": 4.9,
      "downloads": 2850000,
      "added": "2024-08-01",
      "updated": "2025-01-15",
      "preview_images": [
        "https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/poster.png"
      ]
    },
    {
      "id": "flux-fill-dev",
      "name": "FLUX.1 Fill Dev",
      "type": "diffusion_model",
      "architecture": "flux",
      "author": "black-forest-labs",
      "license": "flux-1-dev-non-commercial-license",
      "homepage": "https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev",
      "description": "FLUX.1 Fill Dev â€” inpainting and outpainting model based on FLUX architecture.\nSupports masked image fill with text-guided generation. Gated model.\n",
      "auth": {
        "provider": "huggingface",
        "gated": true,
        "terms_url": "https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev"
      },
      "file": {
        "url": "https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev/resolve/main/flux1-fill-dev.safetensors",
        "sha256": "03e289f530df51d014f48e675a9ffa2141bc003259bf5f25d75b957e920a41ca",
        "size": 23804922408,
        "format": "safetensors"
      },
      "requires": [
        {
          "id": "clip-l",
          "type": "text_encoder",
          "reason": "CLIP-L text encoder for FLUX"
        },
        {
          "id": "t5-xxl-fp8",
          "type": "text_encoder",
          "reason": "T5-XXL text encoder for FLUX"
        },
        {
          "id": "flux-vae",
          "type": "vae",
          "reason": "FLUX VAE"
        }
      ],
      "tags": [
        "flux",
        "inpainting",
        "outpainting",
        "fill"
      ],
      "rating": 4.8,
      "downloads": 650000,
      "added": "2024-10-01",
      "updated": "2025-01-01"
    },
    {
      "id": "flux-redux-dev",
      "name": "FLUX.1 Redux Dev",
      "type": "ipadapter",
      "architecture": "flux",
      "author": "black-forest-labs",
      "license": "flux-1-dev-non-commercial-license",
      "homepage": "https://huggingface.co/black-forest-labs/FLUX.1-Redux-dev",
      "description": "FLUX.1 Redux Dev â€” image variation adapter for FLUX. Takes image input and\ngenerates variations, similar to IP-Adapter functionality. Gated model.\n",
      "auth": {
        "provider": "huggingface",
        "gated": true,
        "terms_url": "https://huggingface.co/black-forest-labs/FLUX.1-Redux-dev"
      },
      "file": {
        "url": "https://huggingface.co/black-forest-labs/FLUX.1-Redux-dev/resolve/main/flux1-redux-dev.safetensors",
        "sha256": "a1b3bdcb4bdc58ce04874b9ca776d61fc3e914bb6beab41efb63e4e2694dca45",
        "size": 129063232,
        "format": "safetensors"
      },
      "tags": [
        "flux",
        "ipadapter",
        "image-variation",
        "redux"
      ],
      "rating": 4.6,
      "downloads": 320000,
      "added": "2024-10-01",
      "updated": "2025-01-01"
    },
    {
      "id": "flux-schnell",
      "name": "FLUX.1 Schnell",
      "type": "checkpoint",
      "architecture": "flux",
      "author": "black-forest-labs",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/black-forest-labs/FLUX.1-schnell",
      "description": "Fast text-to-image model from Black Forest Labs.\nOptimized for speed â€” best with 1-4 steps, CFG 0 (distilled model).\nApache 2.0 license â€” free for commercial use.\n",
      "variants": [
        {
          "id": "fp16",
          "file": "flux1-schnell.safetensors",
          "url": "https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/flux1-schnell.safetensors",
          "sha256": "9403429e0052277ac2a87ad800adece5481eecefd9ed334e1f348723621d2a0a",
          "size": 23782506688,
          "format": "safetensors",
          "precision": "fp16",
          "vram_required": 24576,
          "vram_recommended": 24576
        },
        {
          "id": "fp8",
          "file": "flux1-schnell-fp8-e4m3fn.safetensors",
          "url": "https://huggingface.co/Kijai/flux-fp8/resolve/main/flux1-schnell-fp8-e4m3fn.safetensors",
          "sha256": "722c8f4c3a7a57f7c08998ef273e142e2f4362e0de704bc5848fa8adfbbfb96c",
          "size": 11903959040,
          "format": "safetensors",
          "precision": "fp8-e4m3fn",
          "vram_required": 12288,
          "vram_recommended": 16384,
          "note": "Quantized to fp8. Slight quality reduction vs fp16."
        }
      ],
      "requires": [
        {
          "id": "flux-vae",
          "type": "vae",
          "reason": "Flux models require the Flux-specific VAE (ae.safetensors)"
        },
        {
          "id": "t5-xxl-fp16",
          "type": "text_encoder",
          "reason": "T5-XXL for prompt processing",
          "optional_variant": "t5-xxl-fp8"
        },
        {
          "id": "clip-l",
          "type": "text_encoder",
          "reason": "CLIP-L for secondary text encoding"
        }
      ],
      "auth": {
        "provider": "huggingface",
        "terms_url": "https://huggingface.co/black-forest-labs/FLUX.1-schnell",
        "gated": true
      },
      "defaults": {
        "steps": 4,
        "cfg": 0,
        "sampler": "euler",
        "scheduler": "normal"
      },
      "tags": [
        "flux",
        "text-to-image",
        "fast",
        "distilled",
        "bfl"
      ],
      "rating": 4.7,
      "downloads": 1920000,
      "added": "2024-08-01",
      "updated": "2025-01-15"
    },
    {
      "id": "flux-vae",
      "name": "FLUX VAE (ae.safetensors)",
      "type": "vae",
      "architecture": "flux",
      "author": "black-forest-labs",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/black-forest-labs/FLUX.1-schnell",
      "description": "The VAE used by all FLUX models. Required for FLUX.1 Dev and Schnell.\nSame file (ae.safetensors) is bundled in both the Dev and Schnell repos.\n",
      "file": {
        "url": "https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors",
        "sha256": "f5b59a26851551b67ae1fe58d32e76486e1e812def4696a4bea97f16604d40a3",
        "size": 335304388,
        "format": "safetensors"
      },
      "tags": [
        "flux",
        "vae",
        "bfl"
      ],
      "rating": 5.0,
      "downloads": 3200000,
      "added": "2024-08-01",
      "updated": "2024-08-01"
    },
    {
      "id": "flux2-dev",
      "name": "FLUX.2 Dev",
      "type": "diffusion_model",
      "architecture": "flux2",
      "author": "black-forest-labs / Comfy-Org / unsloth",
      "license": "flux-2-dev-non-commercial",
      "homepage": "https://huggingface.co/Comfy-Org/flux2-dev",
      "description": "FLUX.2 Dev â€” next-generation image model from Black Forest Labs.\nUp to 4MP photorealistic output with improved lighting, skin, fabric, and hand detail.\nMulti-reference consistency (up to 10 images), improved editing precision,\nbetter visual understanding, and professional-class text rendering.\nUses Mistral 3 Small text encoder and FLUX.2 VAE.\nOpen-source (non-commercial license).\n",
      "variants": [
        {
          "id": "fp8mixed",
          "file": "flux2_dev_fp8mixed.safetensors",
          "url": "https://huggingface.co/Comfy-Org/flux2-dev/resolve/main/split_files/diffusion_models/flux2_dev_fp8mixed.safetensors",
          "sha256": "2f159bd5e5a9511d9470e23f303b5e44da33c6c7ee037f29777679178a3430ac",
          "size": 12000000000,
          "format": "safetensors",
          "precision": "fp8-mixed",
          "vram_required": 12288,
          "vram_recommended": 16384,
          "note": "fp8 mixed precision quantization from Comfy-Org."
        },
        {
          "id": "bf16",
          "file": "flux2-dev-BF16.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-dev-GGUF/resolve/main/flux2-dev-BF16.gguf",
          "sha256": "98cdfcb10d6ed5a860f7169fb234138560cda4af158a70848d155d203f0acac4",
          "size": 64446616544,
          "format": "gguf",
          "precision": "bf16",
          "vram_required": 65536,
          "vram_recommended": 65536,
          "note": "Full precision bf16 GGUF. Best quality, requires high-end GPU."
        },
        {
          "id": "gguf-q8-0",
          "file": "flux2-dev-Q8_0.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-dev-GGUF/resolve/main/flux2-dev-Q8_0.gguf",
          "sha256": "09d005300dd8dcbbd489bb75ada6254145c84c2c9c3d7cc1829e3c5dedcb42ce",
          "size": 35002602464,
          "format": "gguf",
          "precision": "q8_0",
          "vram_required": 36864,
          "vram_recommended": 40960,
          "note": "GGUF Q8_0 â€” highest quality quantization."
        },
        {
          "id": "gguf-q6-k",
          "file": "flux2-dev-Q6_K.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-dev-GGUF/resolve/main/flux2-dev-Q6_K.gguf",
          "sha256": "0a662e0303d65b7da4741c7bc54bbccd4d7fc17b23e71ced36d177467f4a0ef1",
          "size": 27396232160,
          "format": "gguf",
          "precision": "q6_k",
          "vram_required": 28672,
          "vram_recommended": 32768,
          "note": "GGUF Q6_K â€” very good quality/size balance."
        },
        {
          "id": "gguf-q5-k-m",
          "file": "flux2-dev-Q5_K_M.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-dev-GGUF/resolve/main/flux2-dev-Q5_K_M.gguf",
          "sha256": "da8438ae213aa141cec9803551af576abb678715a2d92c9a14712c8692f9908d",
          "size": 23926887392,
          "format": "gguf",
          "precision": "q5_k_m",
          "vram_required": 24576,
          "vram_recommended": 28672,
          "note": "GGUF Q5_K_M â€” recommended for 24GB+ GPUs."
        },
        {
          "id": "gguf-q4-k-m",
          "file": "flux2-dev-Q4_K_M.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-dev-GGUF/resolve/main/flux2-dev-Q4_K_M.gguf",
          "sha256": "5f7ac6649e2f5e21a49a6f83931a67530bd887e2d34379c3da1d0f0406501de1",
          "size": 19959731168,
          "format": "gguf",
          "precision": "q4_k_m",
          "vram_required": 20480,
          "vram_recommended": 24576,
          "note": "GGUF Q4_K_M â€” good for 24GB GPUs."
        },
        {
          "id": "gguf-q3-k-m",
          "file": "flux2-dev-Q3_K_M.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-dev-GGUF/resolve/main/flux2-dev-Q3_K_M.gguf",
          "sha256": "f7bff7c167402f917cdb77c0c619a833415233f2739c67501abc01003e44f57a",
          "size": 15829783520,
          "format": "gguf",
          "precision": "q3_k_m",
          "vram_required": 16384,
          "vram_recommended": 20480,
          "note": "GGUF Q3_K_M â€” for 16GB GPUs. Some quality loss."
        },
        {
          "id": "gguf-q2-k",
          "file": "flux2-dev-Q2_K.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-dev-GGUF/resolve/main/flux2-dev-Q2_K.gguf",
          "sha256": "fcbae186d445c80b216387bb596179813f5c9d480e22c820750b6a04be1fb20d",
          "size": 12858250208,
          "format": "gguf",
          "precision": "q2_k",
          "vram_required": 14336,
          "vram_recommended": 16384,
          "note": "GGUF Q2_K â€” smallest, noticeable quality loss."
        }
      ],
      "requires": [
        {
          "id": "flux2-vae",
          "type": "vae",
          "reason": "FLUX.2 VAE for decoding latents to images"
        },
        {
          "id": "flux2-mistral-text-encoder",
          "type": "text_encoder",
          "reason": "Mistral 3 Small text encoder for prompt processing"
        }
      ],
      "auth": {
        "provider": "huggingface",
        "terms_url": "https://huggingface.co/black-forest-labs/FLUX.2-dev",
        "gated": true
      },
      "defaults": {
        "steps": 20,
        "cfg": 3.5,
        "sampler": "euler",
        "scheduler": "normal"
      },
      "tags": [
        "flux2",
        "text-to-image",
        "high-quality",
        "photorealistic",
        "multi-reference",
        "gguf",
        "bfl",
        "comfyui"
      ],
      "rating": 4.9,
      "downloads": 0,
      "added": "2025-07-01",
      "updated": "2026-02-23"
    },
    {
      "id": "flux2-klein-4b",
      "name": "FLUX.2 Klein 4B",
      "type": "diffusion_model",
      "architecture": "flux2",
      "author": "black-forest-labs / Comfy-Org / unsloth",
      "license": "flux-2-klein-non-commercial",
      "homepage": "https://huggingface.co/Comfy-Org/flux2-klein-4B",
      "description": "FLUX.2 Klein 4B â€” the fastest model in the FLUX family.\nUnifies text-to-image and image editing in one compact architecture.\nTwo variants: Base (undistilled, best for fine-tuning) and Distilled (4-step, ~1.2s on 5090).\nOnly 8.4 GB VRAM for distilled variant. Supports style transforms, semantic edits,\nobject replacement/removal, multi-reference composition, and iterative edits.\n",
      "variants": [
        {
          "id": "distilled-fp8",
          "file": "flux-2-klein-4b-fp8.safetensors",
          "url": "https://huggingface.co/black-forest-labs/FLUX.2-klein-4b-fp8/resolve/main/flux-2-klein-4b-fp8.safetensors",
          "sha256": "15005cf50d1361f75c61f7d213d7969063e2aaea7523beefe5d1e085d173568d",
          "size": 4500000000,
          "format": "safetensors",
          "precision": "fp8",
          "vram_required": 8192,
          "vram_recommended": 10240,
          "note": "4-step distilled. ~1.2s on RTX 5090. Best for speed."
        },
        {
          "id": "base-fp8",
          "file": "flux-2-klein-base-4b-fp8.safetensors",
          "url": "https://huggingface.co/black-forest-labs/FLUX.2-klein-base-4b-fp8/resolve/main/flux-2-klein-base-4b-fp8.safetensors",
          "sha256": "14cf50adf6e3837c4454b79520a5c73a8977bce4a7bb210eeb910ce59acbb83d",
          "size": 4500000000,
          "format": "safetensors",
          "precision": "fp8",
          "vram_required": 10240,
          "vram_recommended": 12288,
          "note": "Undistilled base. Better for fine-tuning and maximum flexibility."
        },
        {
          "id": "bf16",
          "file": "flux-2-klein-4b-BF16.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-klein-4B-GGUF/resolve/main/flux-2-klein-4b-BF16.gguf",
          "sha256": "45e92cdad39270a6b4c6055daad75f5817c5ec81a93a7622082342b45e26a53c",
          "size": 7751115328,
          "format": "gguf",
          "precision": "bf16",
          "vram_required": 10240,
          "vram_recommended": 12288,
          "note": "Full precision bf16 GGUF. Best quality."
        },
        {
          "id": "gguf-q8-0",
          "file": "flux-2-klein-4b-Q8_0.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-klein-4B-GGUF/resolve/main/flux-2-klein-4b-Q8_0.gguf",
          "sha256": "24a812e8f9b640e21c784164ea48571d8017ca22873696f4badeeabb006d509c",
          "size": 4300644928,
          "format": "gguf",
          "precision": "q8_0",
          "vram_required": 6144,
          "vram_recommended": 8192,
          "note": "GGUF Q8_0 â€” highest quality quantization."
        },
        {
          "id": "gguf-q6-k",
          "file": "flux-2-klein-4b-Q6_K.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-klein-4B-GGUF/resolve/main/flux-2-klein-4b-Q6_K.gguf",
          "sha256": "fb3b5430d9229f982616c7db084deff1db5035d0cb6558f2d4f704cfe5e609e2",
          "size": 3409273408,
          "format": "gguf",
          "precision": "q6_k",
          "vram_required": 6144,
          "vram_recommended": 8192,
          "note": "GGUF Q6_K â€” very good quality/size balance."
        },
        {
          "id": "gguf-q5-k-m",
          "file": "flux-2-klein-4b-Q5_K_M.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-klein-4B-GGUF/resolve/main/flux-2-klein-4b-Q5_K_M.gguf",
          "sha256": "58c01c75fee2272eadb127f53e26dd05a5a8e7f812e37c36fa44603301f91e54",
          "size": 3073368640,
          "format": "gguf",
          "precision": "q5_k_m",
          "vram_required": 6144,
          "vram_recommended": 8192,
          "note": "GGUF Q5_K_M â€” recommended for 8GB GPUs."
        },
        {
          "id": "gguf-q4-k-m",
          "file": "flux-2-klein-4b-Q4_K_M.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-klein-4B-GGUF/resolve/main/flux-2-klein-4b-Q4_K_M.gguf",
          "sha256": "0b25d143c8469b342bc5af3bce92b783bf6b0636d285f7b2f75e38af63af9a15",
          "size": 2604311104,
          "format": "gguf",
          "precision": "q4_k_m",
          "vram_required": 4096,
          "vram_recommended": 6144,
          "note": "GGUF Q4_K_M â€” good for 6GB GPUs."
        },
        {
          "id": "gguf-q3-k-m",
          "file": "flux-2-klein-4b-Q3_K_M.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-klein-4B-GGUF/resolve/main/flux-2-klein-4b-Q3_K_M.gguf",
          "sha256": "0f27104f7b5842b7bfd5909645c60208cbb6b8d43f5d65f58b085f45ed18c9e1",
          "size": 2124489280,
          "format": "gguf",
          "precision": "q3_k_m",
          "vram_required": 4096,
          "vram_recommended": 6144,
          "note": "GGUF Q3_K_M â€” for 6GB GPUs. Some quality loss."
        },
        {
          "id": "gguf-q2-k",
          "file": "flux-2-klein-4b-Q2_K.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-klein-4B-GGUF/resolve/main/flux-2-klein-4b-Q2_K.gguf",
          "sha256": "681423838cec5788fb2d6227a2d79a254f72cebba75806811fb408181970eaef",
          "size": 1827807808,
          "format": "gguf",
          "precision": "q2_k",
          "vram_required": 4096,
          "vram_recommended": 6144,
          "note": "GGUF Q2_K â€” smallest, noticeable quality loss."
        }
      ],
      "requires": [
        {
          "id": "flux2-vae",
          "type": "vae",
          "reason": "FLUX.2 VAE for decoding latents to images"
        },
        {
          "id": "flux2-qwen3-4b-text-encoder",
          "type": "text_encoder",
          "reason": "Qwen 3 4B text encoder for prompt processing"
        }
      ],
      "auth": {
        "provider": "huggingface",
        "terms_url": "https://huggingface.co/black-forest-labs/FLUX.2-klein-4b-fp8",
        "gated": true
      },
      "defaults": {
        "steps": 4,
        "cfg": 1.0,
        "sampler": "euler",
        "scheduler": "simple"
      },
      "tags": [
        "flux2",
        "klein",
        "text-to-image",
        "image-editing",
        "fast-inference",
        "gguf",
        "4b",
        "comfyui"
      ],
      "rating": 4.8,
      "downloads": 0,
      "added": "2025-07-01",
      "updated": "2026-02-23"
    },
    {
      "id": "flux2-klein-9b",
      "name": "FLUX.2 Klein 9B",
      "type": "diffusion_model",
      "architecture": "flux",
      "author": "black-forest-labs / unsloth",
      "license": "flux-2-klein-non-commercial",
      "homepage": "https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF",
      "description": "FLUX.2 Klein 9B â€” mid-size model in the FLUX.2 Klein family.\nSuccessor to FLUX.1 with improved image quality and editing capabilities.\nSupports text-to-image and image editing workflows.\nUses FLUX.2 VAE and Qwen 3 8B text encoder.\nAvailable as BFL fp8 safetensors (distilled + base) and GGUF quantizations.\nRequires ComfyUI-GGUF custom node for GGUF variants.\n",
      "variants": [
        {
          "id": "distilled-fp8",
          "file": "flux-2-klein-9b-fp8.safetensors",
          "url": "https://huggingface.co/black-forest-labs/FLUX.2-klein-9b-fp8/resolve/main/flux-2-klein-9b-fp8.safetensors",
          "sha256": "865ba09f5b4c3cbd3468a4bd3acb9fcb2f8740c54317482f0bcd4ed1d3655cee",
          "size": 9433061528,
          "format": "safetensors",
          "precision": "fp8",
          "vram_required": 12288,
          "vram_recommended": 16384,
          "note": "4-step distilled fp8 from BFL. Fast inference."
        },
        {
          "id": "base-fp8",
          "file": "flux-2-klein-base-9b-fp8.safetensors",
          "url": "https://huggingface.co/black-forest-labs/FLUX.2-klein-base-9b-fp8/resolve/main/flux-2-klein-base-9b-fp8.safetensors",
          "sha256": "a9f5028c24a7a96f4f45beb883aad287d9bccc246227a6803edc898ddda42cf4",
          "size": 9567278472,
          "format": "safetensors",
          "precision": "fp8",
          "vram_required": 14336,
          "vram_recommended": 16384,
          "note": "Undistilled base fp8 from BFL. Better for fine-tuning."
        },
        {
          "id": "bf16",
          "file": "flux-2-klein-9b-BF16.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-BF16.gguf",
          "sha256": "d4a80a1885c8b952d8f8d171ab3d1cac166ae7223d5ff4b5ee2d40932ce9fd58",
          "size": 18200000000,
          "format": "gguf",
          "precision": "bf16",
          "vram_required": 20480,
          "vram_recommended": 24576,
          "note": "Full precision bf16 GGUF. Best quality."
        },
        {
          "id": "f16",
          "file": "flux-2-klein-9b-F16.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-F16.gguf",
          "sha256": "f82a83fd13542f3d6ac76e67e1fe60c08cdceddab856337f1b9ec534d9b5ca6f",
          "size": 18200000000,
          "format": "gguf",
          "precision": "f16",
          "vram_required": 20480,
          "vram_recommended": 24576,
          "note": "Full precision fp16 GGUF."
        },
        {
          "id": "gguf-q8-0",
          "file": "flux-2-klein-9b-Q8_0.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-Q8_0.gguf",
          "sha256": "824b14b3d89f62779db9bcfe6af9084a52e1a3880e7ff93d50943969f9e25b27",
          "size": 9980000000,
          "format": "gguf",
          "precision": "q8_0",
          "vram_required": 12288,
          "vram_recommended": 16384,
          "note": "GGUF Q8_0 â€” highest quality quantization."
        },
        {
          "id": "gguf-q6-k",
          "file": "flux-2-klein-9b-Q6_K.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-Q6_K.gguf",
          "sha256": "e19f643ba4b66ed5b779bc6283f0629363cabb0490cb06823cadb30a549eda5d",
          "size": 7870000000,
          "format": "gguf",
          "precision": "q6_k",
          "vram_required": 10240,
          "vram_recommended": 12288,
          "note": "GGUF Q6_K â€” very good quality/size balance."
        },
        {
          "id": "gguf-q5-k-m",
          "file": "flux-2-klein-9b-Q5_K_M.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-Q5_K_M.gguf",
          "sha256": "bac097be5094c7c3fba993a1e57c6bb9cfc1dd0b33d608cc419311334aa4d015",
          "size": 7020000000,
          "format": "gguf",
          "precision": "q5_k_m",
          "vram_required": 8192,
          "vram_recommended": 12288,
          "note": "GGUF Q5_K_M â€” recommended for 12GB+ GPUs."
        },
        {
          "id": "gguf-q5-k-s",
          "file": "flux-2-klein-9b-Q5_K_S.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-Q5_K_S.gguf",
          "sha256": "5cd600987443bafb0c04ccde87f5e7b083a8fcab26cdd8c8f2b331dde25cd010",
          "size": 6940000000,
          "format": "gguf",
          "precision": "q5_k_s",
          "vram_required": 8192,
          "vram_recommended": 12288,
          "note": "GGUF Q5_K_S â€” smaller Q5 variant."
        },
        {
          "id": "gguf-q4-k-m",
          "file": "flux-2-klein-9b-Q4_K_M.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-Q4_K_M.gguf",
          "sha256": "262df1a3a98bc328911e03d4d0f5d7e3b1397477a2c485b24509b6d115259aef",
          "size": 5910000000,
          "format": "gguf",
          "precision": "q4_k_m",
          "vram_required": 8192,
          "vram_recommended": 10240,
          "note": "GGUF Q4_K_M â€” good for 8GB GPUs."
        },
        {
          "id": "gguf-q4-k-s",
          "file": "flux-2-klein-9b-Q4_K_S.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-Q4_K_S.gguf",
          "sha256": "ee1c0373abb438f1fda472ac15775a0b538bf6f464ec978eaaf8a2a96a42cac3",
          "size": 5830000000,
          "format": "gguf",
          "precision": "q4_k_s",
          "vram_required": 8192,
          "vram_recommended": 10240,
          "note": "GGUF Q4_K_S â€” smaller Q4 variant."
        },
        {
          "id": "gguf-q4-1",
          "file": "flux-2-klein-9b-Q4_1.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-Q4_1.gguf",
          "sha256": "96c219d318d3ef9000159a11a6098e46da30a03848fdd375b7dfa3fc84199895",
          "size": 6160000000,
          "format": "gguf",
          "precision": "q4_1",
          "vram_required": 8192,
          "vram_recommended": 10240,
          "note": "GGUF Q4_1."
        },
        {
          "id": "gguf-q4-0",
          "file": "flux-2-klein-9b-Q4_0.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-Q4_0.gguf",
          "sha256": "3d5e58acdf68308bddf325a6f7045c0c36a9bce6832bb8a59f5ce193ddf979f5",
          "size": 5620000000,
          "format": "gguf",
          "precision": "q4_0",
          "vram_required": 8192,
          "vram_recommended": 10240,
          "note": "GGUF Q4_0."
        },
        {
          "id": "gguf-q3-k-m",
          "file": "flux-2-klein-9b-Q3_K_M.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-Q3_K_M.gguf",
          "sha256": "93ed5cf37c8dd0b379cfe4cd3feaa03683d8a56492a62baa5ecbbab232277573",
          "size": 4770000000,
          "format": "gguf",
          "precision": "q3_k_m",
          "vram_required": 6144,
          "vram_recommended": 8192,
          "note": "GGUF Q3_K_M â€” for 8GB GPUs. Some quality loss."
        },
        {
          "id": "gguf-q3-k-s",
          "file": "flux-2-klein-9b-Q3_K_S.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-Q3_K_S.gguf",
          "sha256": "41f9255c4eda7abf6b06a50826a88e70090a523d0c65785487ae300e9f8d4362",
          "size": 4690000000,
          "format": "gguf",
          "precision": "q3_k_s",
          "vram_required": 6144,
          "vram_recommended": 8192,
          "note": "GGUF Q3_K_S â€” smaller Q3 variant."
        },
        {
          "id": "gguf-q2-k",
          "file": "flux-2-klein-9b-Q2_K.gguf",
          "url": "https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-Q2_K.gguf",
          "sha256": "0bcc795ca67361e53db3ceeceda02fd58bf117d20ee8cc30b257cc06f6a3027f",
          "size": 3980000000,
          "format": "gguf",
          "precision": "q2_k",
          "vram_required": 6144,
          "vram_recommended": 8192,
          "note": "GGUF Q2_K â€” worst quality but smallest. Noticeable degradation."
        }
      ],
      "requires": [
        {
          "id": "flux2-vae",
          "type": "vae",
          "reason": "FLUX.2 VAE for decoding latents to images"
        },
        {
          "id": "flux2-qwen3-8b-text-encoder",
          "type": "text_encoder",
          "reason": "Qwen 3 8B text encoder for prompt processing"
        }
      ],
      "tags": [
        "flux",
        "text-to-image",
        "image-editing",
        "gguf",
        "comfyui",
        "unsloth"
      ],
      "rating": 4.7,
      "downloads": 0,
      "added": "2025-07-01",
      "updated": "2026-02-23",
      "preview_images": [
        "https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/assets/flux2klein9b.png"
      ]
    },
    {
      "id": "flux2-mistral-text-encoder",
      "name": "FLUX.2 Mistral 3 Small Text Encoder",
      "type": "text_encoder",
      "architecture": "mistral",
      "author": "Comfy-Org",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/Comfy-Org/flux2-dev",
      "description": "Mistral 3 Small text encoder (bf16) for FLUX.2 Dev.\nRequired for FLUX.2 Dev text-to-image generation.\n",
      "file": {
        "url": "https://huggingface.co/Comfy-Org/flux2-dev/resolve/main/split_files/text_encoders/mistral_3_small_flux2_bf16.safetensors",
        "sha256": "a8711134ac2d06dd36b03914220ee54a43e7dd9a23eecbeca25107d78eed3382",
        "size": 24000000000,
        "format": "safetensors"
      },
      "tags": [
        "flux2",
        "text-encoder",
        "mistral",
        "dev"
      ],
      "rating": 5.0,
      "downloads": 0,
      "added": "2025-07-01",
      "updated": "2025-07-01"
    },
    {
      "id": "flux2-qwen3-4b-text-encoder",
      "name": "FLUX.2 Qwen 3 4B Text Encoder",
      "type": "text_encoder",
      "architecture": "qwen",
      "author": "Comfy-Org",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/Comfy-Org/flux2-klein-4B",
      "description": "Qwen 3 4B text encoder for FLUX.2 Klein 4B models.\nRequired for both base and distilled 4B variants.\n",
      "file": {
        "url": "https://huggingface.co/Comfy-Org/flux2-klein-4B/resolve/main/split_files/text_encoders/qwen_3_4b.safetensors",
        "sha256": "f459cd74b7868799ea82f97601a650afcedc399596dc262f302e3505761c9995",
        "size": 9800000000,
        "format": "safetensors"
      },
      "tags": [
        "flux2",
        "text-encoder",
        "qwen",
        "klein-4b"
      ],
      "rating": 5.0,
      "downloads": 0,
      "added": "2025-07-01",
      "updated": "2025-07-01"
    },
    {
      "id": "flux2-qwen3-8b-text-encoder",
      "name": "FLUX.2 Qwen 3 8B Text Encoder",
      "type": "text_encoder",
      "architecture": "qwen",
      "author": "Comfy-Org",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/Comfy-Org/flux2-klein-9B",
      "description": "Qwen 3 8B text encoder (fp8 mixed) for FLUX.2 Klein 9B models.\nRequired for both base and distilled 9B variants.\n",
      "file": {
        "url": "https://huggingface.co/Comfy-Org/flux2-klein-9B/resolve/main/split_files/text_encoders/qwen_3_8b_fp8mixed.safetensors",
        "sha256": "334d028c0191eb19d32a26f45a2e426d0a33284b33d5da5595f402c206419105",
        "size": 8500000000,
        "format": "safetensors"
      },
      "tags": [
        "flux2",
        "text-encoder",
        "qwen",
        "klein-9b",
        "fp8"
      ],
      "rating": 5.0,
      "downloads": 0,
      "added": "2025-07-01",
      "updated": "2025-07-01"
    },
    {
      "id": "flux2-vae",
      "name": "FLUX.2 VAE",
      "type": "vae",
      "architecture": "flux2",
      "author": "black-forest-labs / Comfy-Org",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/Comfy-Org/flux2-dev",
      "description": "VAE for all FLUX.2 models (Dev, Klein 4B, Klein 9B).\nDifferent from FLUX.1 VAE â€” do not mix them.\nShared across all FLUX.2 model variants.\n",
      "file": {
        "url": "https://huggingface.co/Comfy-Org/flux2-dev/resolve/main/split_files/vae/flux2-vae.safetensors",
        "sha256": "bb534d41e8e6f92dc8636b914489b7167aeb950418183ffc10768c573185683a",
        "size": 335000000,
        "format": "safetensors"
      },
      "tags": [
        "flux2",
        "vae",
        "bfl"
      ],
      "rating": 5.0,
      "downloads": 0,
      "added": "2025-07-01",
      "updated": "2025-07-01"
    },
    {
      "id": "ip-adapter-faceid-sdxl",
      "name": "IP-Adapter FaceID (SDXL)",
      "type": "ipadapter",
      "architecture": "sdxl",
      "author": "h94",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/h94/IP-Adapter-FaceID",
      "description": "IP-Adapter with InsightFace face ID embedding for SDXL.\nFace-consistent generation from reference photos. Requires companion LoRA.\n",
      "base_models": [
        "sdxl-base-1.0"
      ],
      "file": {
        "url": "https://huggingface.co/h94/IP-Adapter-FaceID/resolve/main/ip-adapter-faceid_sdxl.bin",
        "sha256": "b924b678ef4ca408577e51faa08a4d281e3411fca24cb84a080a3751d65ed697",
        "size": 1148846080,
        "format": "bin"
      },
      "tags": [
        "sdxl",
        "ipadapter",
        "faceid",
        "face-consistent",
        "insightface"
      ],
      "rating": 4.5,
      "downloads": 800000,
      "added": "2024-01-15",
      "updated": "2024-06-01"
    },
    {
      "id": "ip-adapter-sdxl",
      "name": "IP-Adapter SDXL (ViT-H)",
      "type": "ipadapter",
      "architecture": "sdxl",
      "author": "h94",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/h94/IP-Adapter",
      "description": "IP-Adapter for SDXL using ViT-H image encoder. Enables image-prompted\ngeneration by conditioning on reference images. Style transfer and composition.\n",
      "base_models": [
        "sdxl-base-1.0"
      ],
      "file": {
        "url": "https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter_sdxl_vit-h.safetensors",
        "sha256": "ebf05d918348aec7abb02a5e9ecef77e0aaea6914a5c4ea13f50d45eb1681831",
        "size": 731906048,
        "format": "safetensors"
      },
      "tags": [
        "sdxl",
        "ipadapter",
        "image-prompt",
        "style-transfer"
      ],
      "rating": 4.7,
      "downloads": 1200000,
      "added": "2023-08-20",
      "updated": "2024-06-01"
    },
    {
      "id": "lcm-lora-sd15",
      "name": "LCM LoRA (SD 1.5)",
      "type": "lora",
      "architecture": "sd15",
      "author": "latent-consistency",
      "license": "openrail++",
      "homepage": "https://huggingface.co/latent-consistency/lcm-lora-sdv1-5",
      "description": "Latent Consistency Model LoRA for SD 1.5. Enables fast 2-8 step inference\nvia distillation. Drop-in acceleration for any SD 1.5 model.\n",
      "base_models": [
        "sd-1.5"
      ],
      "file": {
        "url": "https://huggingface.co/latent-consistency/lcm-lora-sdv1-5/resolve/main/pytorch_lora_weights.safetensors",
        "sha256": "8f90d840e075ff588a58e22c6586e2ae9a6f7922996ee6649a7f01072333afe4",
        "size": 134217728,
        "format": "safetensors"
      },
      "tags": [
        "sd15",
        "lora",
        "lcm",
        "fast-inference",
        "distillation"
      ],
      "rating": 4.5,
      "downloads": 1800000,
      "added": "2023-10-06",
      "updated": "2024-06-01"
    },
    {
      "id": "lcm-lora-sdxl",
      "name": "LCM LoRA (SDXL)",
      "type": "lora",
      "architecture": "sdxl",
      "author": "latent-consistency",
      "license": "openrail++",
      "homepage": "https://huggingface.co/latent-consistency/lcm-lora-sdxl",
      "description": "Latent Consistency Model LoRA for SDXL. Enables fast 2-8 step inference\nvia distillation. Drop-in acceleration for any SDXL model.\n",
      "base_models": [
        "sdxl-base-1.0",
        "sdxl-turbo"
      ],
      "file": {
        "url": "https://huggingface.co/latent-consistency/lcm-lora-sdxl/resolve/main/pytorch_lora_weights.safetensors",
        "sha256": "a764e6859b6e04047cd761c08ff0cee96413a8e004c9f07707530cd776b19141",
        "size": 393854976,
        "format": "safetensors"
      },
      "tags": [
        "sdxl",
        "lora",
        "lcm",
        "fast-inference",
        "distillation"
      ],
      "rating": 4.6,
      "downloads": 2200000,
      "added": "2023-10-06",
      "updated": "2024-06-01"
    },
    {
      "id": "ltx-2-19b",
      "name": "LTX-2 19B",
      "type": "diffusion_model",
      "architecture": "ltxv",
      "author": "Lightricks",
      "license": "ltx-video",
      "homepage": "https://huggingface.co/Lightricks/LTX-Video-2-19B-DEV",
      "description": "LTX-2 19B parameter video generation model. Supports text-to-video,\nimage-to-video, video-to-video, and audio generation. Available in\nfull precision and multiple GGUF quantizations.\n",
      "variants": [
        {
          "id": "bf16",
          "file": "ltx-2-19b-dev-BF16.gguf",
          "url": "https://huggingface.co/unsloth/LTX-2-GGUF/resolve/main/ltx-2-19b-dev-BF16.gguf",
          "sha256": "832e923e7a727a696e080b2de14c62268e47c79242d1197bf43a86ef65db2267",
          "size": 37771979232,
          "format": "gguf",
          "precision": "bf16",
          "vram_required": 40960
        },
        {
          "id": "gguf-q8-0",
          "file": "ltx-2-19b-dev-Q8_0.gguf",
          "url": "https://huggingface.co/unsloth/LTX-2-GGUF/resolve/main/ltx-2-19b-dev-Q8_0.gguf",
          "sha256": "3e377af6930a0795e464e62e40daa0044427dda6c204d03e9c3a00351ef83dc1",
          "size": 20407560672,
          "format": "gguf",
          "vram_required": 24576
        },
        {
          "id": "gguf-q6-k",
          "file": "ltx-2-19b-dev-Q6_K.gguf",
          "url": "https://huggingface.co/unsloth/LTX-2-GGUF/resolve/main/ltx-2-19b-dev-Q6_K.gguf",
          "sha256": "2fe65f4a86f3a24537df06d5a0f180f5f848ba4764487c4488677cfec7c2bff8",
          "size": 15966448096,
          "format": "gguf",
          "vram_required": 20480
        },
        {
          "id": "gguf-q5-k-m",
          "file": "ltx-2-19b-dev-Q5_K_M.gguf",
          "url": "https://huggingface.co/unsloth/LTX-2-GGUF/resolve/main/ltx-2-19b-dev-Q5_K_M.gguf",
          "sha256": "ce643ae23089dbaaa37ddeacd6b51cc975970fe875d2aea7a99da4565aab8892",
          "size": 14335388128,
          "format": "gguf",
          "vram_required": 16384
        },
        {
          "id": "gguf-q4-k-m",
          "file": "ltx-2-19b-dev-Q4_K_M.gguf",
          "url": "https://huggingface.co/unsloth/LTX-2-GGUF/resolve/main/ltx-2-19b-dev-Q4_K_M.gguf",
          "sha256": "e3332ee9e6e1a84b7cb5a558a3ebd63891487a7ce285a284ee9d1c48d41463d7",
          "size": 12840118752,
          "format": "gguf",
          "vram_required": 16384
        },
        {
          "id": "gguf-q4-k-s",
          "file": "ltx-2-19b-dev-Q4_K_S.gguf",
          "url": "https://huggingface.co/unsloth/LTX-2-GGUF/resolve/main/ltx-2-19b-dev-Q4_K_S.gguf",
          "sha256": "8028cfc03abf5dff618c1511b025f3e15ea73be03e315d971f8d20e50bee1e0f",
          "size": 11855505888,
          "format": "gguf",
          "vram_required": 12288
        },
        {
          "id": "gguf-q3-k-m",
          "file": "ltx-2-19b-dev-Q3_K_M.gguf",
          "url": "https://huggingface.co/unsloth/LTX-2-GGUF/resolve/main/ltx-2-19b-dev-Q3_K_M.gguf",
          "sha256": "a7d0975088816493ca97500739f66e272b151f04e33d13b469f0b272aa2c6cd4",
          "size": 10117491168,
          "format": "gguf",
          "vram_required": 12288
        },
        {
          "id": "gguf-q2-k",
          "file": "ltx-2-19b-dev-Q2_K.gguf",
          "url": "https://huggingface.co/unsloth/LTX-2-GGUF/resolve/main/ltx-2-19b-dev-Q2_K.gguf",
          "sha256": "93958494c70d5f594abc7b775aeb46f6eef7c5e2d06558b8414491cb8ffb7d05",
          "size": 8097016288,
          "format": "gguf",
          "vram_required": 10240
        }
      ],
      "requires": [
        {
          "id": "ltx-2-text-encoder",
          "type": "text_encoder",
          "reason": "LTX-2 embeddings connector for prompt processing"
        },
        {
          "id": "ltx-2-vae",
          "type": "vae",
          "reason": "LTX-2 video VAE for decoding"
        }
      ],
      "tags": [
        "ltx",
        "ltx-2",
        "video",
        "text-to-video",
        "image-to-video",
        "19b"
      ],
      "rating": 4.8,
      "downloads": 450000,
      "added": "2025-06-01",
      "updated": "2025-06-01"
    },
    {
      "id": "ltx-2-text-encoder",
      "name": "LTX-2 Embeddings Connector",
      "type": "text_encoder",
      "architecture": "ltxv",
      "author": "Lightricks",
      "license": "ltx-video",
      "homepage": "https://huggingface.co/unsloth/LTX-2-GGUF",
      "description": "Embeddings connector / text encoder for LTX-2 19B video model.\n",
      "file": {
        "url": "https://huggingface.co/unsloth/LTX-2-GGUF/resolve/main/text_encoders/ltx-2-19b-dev_embeddings_connectors.safetensors",
        "sha256": "50a1bd5e4cbbe35e1c87867b6dc693eb784ccd8e7c8fdb8d054a238a90bf8af1",
        "size": 2862981904,
        "format": "safetensors"
      },
      "tags": [
        "ltx",
        "ltx-2",
        "text-encoder",
        "video"
      ],
      "rating": 4.7,
      "downloads": 300000,
      "added": "2025-06-01",
      "updated": "2025-06-01"
    },
    {
      "id": "ltx-2-vae",
      "name": "LTX-2 Video VAE",
      "type": "vae",
      "architecture": "ltxv",
      "author": "Lightricks",
      "license": "ltx-video",
      "homepage": "https://huggingface.co/unsloth/LTX-2-GGUF",
      "description": "Video VAE for LTX-2 19B model. Handles video encoding/decoding.\n",
      "file": {
        "url": "https://huggingface.co/unsloth/LTX-2-GGUF/resolve/main/vae/ltx-2-19b-dev_video_vae.safetensors",
        "sha256": "54cb4c1dd7e74c3ea940617bcdc738779b0372f0a30acd1b3054149e741c2b7a",
        "size": 2445007026,
        "format": "safetensors"
      },
      "tags": [
        "ltx",
        "ltx-2",
        "vae",
        "video"
      ],
      "rating": 4.7,
      "downloads": 300000,
      "added": "2025-06-01",
      "updated": "2025-06-01"
    },
    {
      "id": "ltx-video-13b",
      "name": "LTX-Video 13B",
      "type": "diffusion_model",
      "architecture": "ltxv",
      "author": "Lightricks",
      "license": "ltx-video",
      "homepage": "https://huggingface.co/Lightricks/LTX-Video",
      "description": "LTX-Video 13B parameter model (v0.9.8). Higher quality than the 2B version.\nIncludes distilled variant for faster inference. Supports text-to-video,\nimage-to-video, and multi-frame control.\n",
      "variants": [
        {
          "id": "fp16",
          "file": "ltxv-13b-0.9.8-distilled.safetensors",
          "url": "https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltxv-13b-0.9.8-distilled.safetensors",
          "sha256": "2c5f814744f04d8118e0b5bbfc0742655b51e41a6c090da66fde2936d651a9c9",
          "size": 28579183564,
          "format": "safetensors",
          "precision": "fp16",
          "vram_required": 32768
        },
        {
          "id": "fp8",
          "file": "ltxv-13b-0.9.8-distilled-fp8.safetensors",
          "url": "https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltxv-13b-0.9.8-distilled-fp8.safetensors",
          "sha256": "111a3d07baa17f520e98b571e7916139ae0865c9a24b7534529d6b9e74264db3",
          "size": 15694280140,
          "format": "safetensors",
          "precision": "fp8",
          "vram_required": 16384
        },
        {
          "id": "dev-fp16",
          "file": "ltxv-13b-0.9.8-dev.safetensors",
          "url": "https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltxv-13b-0.9.8-dev.safetensors",
          "sha256": "56b39a874ef9d0c7ad4099954f0ee4fa7e600ce83a24b3736f9e12223e7c71be",
          "size": 28579183340,
          "format": "safetensors",
          "precision": "fp16",
          "vram_required": 32768
        },
        {
          "id": "dev-fp8",
          "file": "ltxv-13b-0.9.8-dev-fp8.safetensors",
          "url": "https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltxv-13b-0.9.8-dev-fp8.safetensors",
          "sha256": "b281bbb53b76d25a02285c148212b32daa6a57dfa46ce804c9bddba46f948c94",
          "size": 15694279916,
          "format": "safetensors",
          "precision": "fp8",
          "vram_required": 16384
        }
      ],
      "requires": [
        {
          "id": "t5-xxl-fp16",
          "type": "text_encoder",
          "reason": "T5-XXL for prompt processing",
          "optional_variant": "t5-xxl-fp8"
        }
      ],
      "tags": [
        "ltx",
        "ltx-video",
        "video",
        "text-to-video",
        "image-to-video",
        "13b"
      ],
      "rating": 4.7,
      "downloads": 520000,
      "added": "2025-04-01",
      "updated": "2025-06-01"
    },
    {
      "id": "ltx-video-2b",
      "name": "LTX-Video 2B",
      "type": "checkpoint",
      "architecture": "ltxv",
      "author": "Lightricks",
      "license": "ltx-video",
      "homepage": "https://huggingface.co/Lightricks/LTX-Video",
      "description": "Efficient 2B parameter video generation model by Lightricks. Fast inference,\nsupports text-to-video and image-to-video. Works best with long, descriptive prompts.\n",
      "file": {
        "url": "https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltx-video-2b-v0.9.5.safetensors",
        "sha256": "720d15c9f19f7d0f6b2a92bbbc34410e2cfb2f6856a100b38f734fbf973d4adf",
        "size": 6340729500,
        "format": "safetensors"
      },
      "requires": [
        {
          "id": "t5-xxl-fp16",
          "type": "text_encoder",
          "reason": "T5-XXL for prompt processing",
          "optional_variant": "t5-xxl-fp8"
        }
      ],
      "tags": [
        "ltx",
        "ltx-video",
        "video",
        "text-to-video",
        "image-to-video",
        "efficient"
      ],
      "rating": 4.6,
      "downloads": 1400000,
      "added": "2024-12-01",
      "updated": "2025-04-01"
    },
    {
      "id": "qwen-image",
      "name": "Qwen Image",
      "type": "diffusion_model",
      "architecture": "qwen-image",
      "author": "Qwen / city96",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/Qwen/Qwen-Image",
      "description": "Qwen Image â€” 20B parameter text-to-image generation model by Qwen (Alibaba).\nBased on Qwen 2.5 VL architecture. Generates high-quality images from text prompts.\nThis is the base text-to-image model, distinct from Qwen Image Edit which is\nspecialized for image editing tasks. Apache 2.0 license â€” free for commercial use.\nGGUF variants require the ComfyUI-GGUF custom node.\n",
      "variants": [
        {
          "id": "bf16",
          "file": "qwen-image-BF16.gguf",
          "url": "https://huggingface.co/city96/Qwen-Image-gguf/resolve/main/qwen-image-BF16.gguf",
          "sha256": "ebb2508748bc52ee5756fa5f43a4da1818bd495a3068c8d1021909f004976107",
          "size": 40872114720,
          "format": "gguf",
          "precision": "bf16",
          "vram_required": 40960,
          "vram_recommended": 49152,
          "note": "Full precision bf16 GGUF. Best quality, needs A100 40GB+."
        },
        {
          "id": "gguf-q8-0",
          "file": "qwen-image-Q8_0.gguf",
          "url": "https://huggingface.co/city96/Qwen-Image-gguf/resolve/main/qwen-image-Q8_0.gguf",
          "sha256": "d4e13114622b523027ec358e4d806f5c3db34dbcbeafaf0ce2420999a343f81d",
          "size": 21761817120,
          "format": "gguf",
          "precision": "q8_0",
          "vram_required": 24576,
          "vram_recommended": 32768,
          "note": "GGUF Q8_0 â€” best quantized quality."
        },
        {
          "id": "gguf-q6-k",
          "file": "qwen-image-Q6_K.gguf",
          "url": "https://huggingface.co/city96/Qwen-Image-gguf/resolve/main/qwen-image-Q6_K.gguf",
          "sha256": "1a35a094d6da1f1b170afcd93174db71d2d74adc5f1c99b3595961be507e99a5",
          "size": 16824990240,
          "format": "gguf",
          "precision": "q6_k",
          "vram_required": 20480,
          "vram_recommended": 24576,
          "note": "GGUF Q6_K â€” great quality/size balance."
        },
        {
          "id": "gguf-q5-k-m",
          "file": "qwen-image-Q5_K_M.gguf",
          "url": "https://huggingface.co/city96/Qwen-Image-gguf/resolve/main/qwen-image-Q5_K_M.gguf",
          "sha256": "196c8c14aaef4febd18432e8775104313439b842b9383d0b7d0d1c13f7eeca55",
          "size": 14934899232,
          "format": "gguf",
          "precision": "q5_k_m",
          "vram_required": 16384,
          "vram_recommended": 20480,
          "note": "GGUF Q5_K_M â€” recommended for 16GB+ GPUs."
        },
        {
          "id": "gguf-q4-k-m",
          "file": "qwen-image-Q4_K_M.gguf",
          "url": "https://huggingface.co/city96/Qwen-Image-gguf/resolve/main/qwen-image-Q4_K_M.gguf",
          "sha256": "a3cb9363825152ce6b7430d5cd7fe47dc84cbe6a30c6baffa1ba32b938c1e866",
          "size": 13065746976,
          "format": "gguf",
          "precision": "q4_k_m",
          "vram_required": 12288,
          "vram_recommended": 16384,
          "note": "GGUF Q4_K_M â€” good for 12GB GPUs."
        },
        {
          "id": "gguf-q3-k-m",
          "file": "qwen-image-Q3_K_M.gguf",
          "url": "https://huggingface.co/city96/Qwen-Image-gguf/resolve/main/qwen-image-Q3_K_M.gguf",
          "sha256": "f08d9e2cdc411cff1ca228eb5e8ea4c449a285d92545f81d4007100064b2c017",
          "size": 9679567392,
          "format": "gguf",
          "precision": "q3_k_m",
          "vram_required": 10240,
          "vram_recommended": 12288,
          "note": "GGUF Q3_K_M â€” budget option. Uses dynamic precision on first/last layers."
        },
        {
          "id": "gguf-q2-k",
          "file": "qwen-image-Q2_K.gguf",
          "url": "https://huggingface.co/city96/Qwen-Image-gguf/resolve/main/qwen-image-Q2_K.gguf",
          "sha256": "ab2ae71efaa260f45b6f503af18dd560c24b499e4ef674821bcb56dc312badfe",
          "size": 7062518304,
          "format": "gguf",
          "precision": "q2_k",
          "vram_required": 8192,
          "vram_recommended": 10240,
          "note": "GGUF Q2_K â€” smallest. Dynamic precision keeps it usable."
        }
      ],
      "requires": [
        {
          "id": "qwen-image-vae",
          "type": "vae",
          "reason": "Qwen Image VAE for decoding latents"
        },
        {
          "id": "qwen-image-clip",
          "type": "text_encoder",
          "reason": "Qwen 2.5 VL 7B text/vision encoder for prompt processing"
        }
      ],
      "defaults": {
        "steps": 30,
        "cfg": 7.0
      },
      "tags": [
        "qwen",
        "text-to-image",
        "20b",
        "gguf",
        "high-quality",
        "comfyui"
      ],
      "rating": 4.8,
      "downloads": 12000,
      "added": "2026-02-24",
      "updated": "2026-02-24"
    },
    {
      "id": "qwen-image-clip",
      "name": "Qwen 2.5 VL 7B Text Encoder",
      "type": "text_encoder",
      "architecture": "qwen-vl",
      "author": "Comfy-Org",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI",
      "description": "Qwen 2.5 Vision-Language 7B text/vision encoder for Qwen Image Edit.\nHandles prompt processing and image understanding for the Qwen Image editing pipeline.\nAvailable in full bf16 (16.6 GB) and fp8 quantized (9.4 GB) variants.\n",
      "variants": [
        {
          "id": "fp8",
          "file": "qwen_2.5_vl_7b_fp8_scaled.safetensors",
          "url": "https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/split_files/text_encoders/qwen_2.5_vl_7b_fp8_scaled.safetensors",
          "sha256": "cb5636d852a0ea6a9075ab1bef496c0db7aef13c02350571e388aea959c5c0b4",
          "size": 10071982080,
          "format": "safetensors",
          "precision": "fp8-e4m3fn",
          "vram_required": 10240,
          "vram_recommended": 12288,
          "note": "FP8 scaled quantization. Recommended â€” good quality at half the size."
        },
        {
          "id": "bf16",
          "file": "qwen_2.5_vl_7b.safetensors",
          "url": "https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/split_files/text_encoders/qwen_2.5_vl_7b.safetensors",
          "sha256": "7dc87a9c61db8168b119859940bce41cf3f737784e8c43d8e71a9f9720fa4051",
          "size": 17825792000,
          "format": "safetensors",
          "precision": "bf16",
          "vram_required": 16384,
          "vram_recommended": 20480,
          "note": "Full bf16 precision. Best quality, needs more VRAM."
        }
      ],
      "tags": [
        "qwen",
        "text-encoder",
        "vision-language",
        "clip",
        "comfyui"
      ],
      "rating": 4.8,
      "downloads": 275000,
      "added": "2025-07-01",
      "updated": "2025-07-01"
    },
    {
      "id": "qwen-image-edit",
      "name": "Qwen Image Edit",
      "type": "checkpoint",
      "architecture": "qwen-image",
      "author": "Comfy-Org / QuantStack",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI",
      "description": "AI-powered image editing model based on Qwen 2.5 VL architecture.\nSupports outpainting, format/ratio changes, AI scene generation, and in-place enhancement.\nBest with 8 steps using Lightning LoRA, euler sampler, CFG 1.0.\nAvailable as native safetensors (bf16/fp8) or GGUF quantizations (Q2_K through Q8_0).\n",
      "variants": [
        {
          "id": "bf16",
          "file": "qwen_image_bf16.safetensors",
          "url": "https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/split_files/diffusion_models/qwen_image_bf16.safetensors",
          "sha256": "106966bb02e7f08a19b56d74db2491793aeadc740c993bcdb7d0e9dcbc9d94ab",
          "size": 43927101440,
          "format": "safetensors",
          "precision": "bf16",
          "vram_required": 40960,
          "vram_recommended": 49152,
          "note": "Full precision bf16. Best quality, needs A100 40GB+ or similar."
        },
        {
          "id": "fp8-hq",
          "file": "qwen_image_fp8_hq.safetensors",
          "url": "https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/split_files/diffusion_models/qwen_image_fp8_hq.safetensors",
          "sha256": "83717dba9759964b678b82258fb775d713661743b37661c13dee4f256109dcac",
          "size": 24373903360,
          "format": "safetensors",
          "precision": "fp8-e4m3fn",
          "vram_required": 24576,
          "vram_recommended": 32768,
          "note": "High-quality fp8 quantization with sensitive layers in higher precision."
        },
        {
          "id": "fp8-mixed",
          "file": "qwen_image_fp8mixed.safetensors",
          "url": "https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/split_files/diffusion_models/qwen_image_fp8mixed.safetensors",
          "sha256": "c2e20d799a317959b2cc7f270bf174df801cbb0b2a00b5e4549ecf78220cf6fb",
          "size": 22011707392,
          "format": "safetensors",
          "precision": "fp8-e4m3fn",
          "vram_required": 24576,
          "vram_recommended": 24576,
          "note": "Mixed precision fp8 with comfy_quant layers. Sensitive layers kept in high precision."
        },
        {
          "id": "fp8",
          "file": "qwen_image_fp8_e4m3fn.safetensors",
          "url": "https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/split_files/diffusion_models/qwen_image_fp8_e4m3fn.safetensors",
          "sha256": "3c291b78a18ea30fc99c3301385ec9c7481a5102b8da5cf5449f725c5ff5ac5f",
          "size": 21902483456,
          "format": "safetensors",
          "precision": "fp8-e4m3fn",
          "vram_required": 20480,
          "vram_recommended": 24576,
          "note": "Standard fp8 quantization. Good balance of quality vs VRAM."
        },
        {
          "id": "nvfp4",
          "file": "qwen_image_nvfp4.safetensors",
          "url": "https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/split_files/diffusion_models/qwen_image_nvfp4.safetensors",
          "sha256": "e194b9c5c2aa18606532e5ffa92d91c5e3ad16dde382cd6801149398b9e3bc25",
          "size": 21260902400,
          "format": "safetensors",
          "precision": "nvfp4",
          "vram_required": 16384,
          "vram_recommended": 20480,
          "note": "NVIDIA fp4 quantization. Lower quality but fits on smaller GPUs."
        },
        {
          "id": "2512-bf16",
          "file": "qwen_image_2512_bf16.safetensors",
          "url": "https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/split_files/diffusion_models/qwen_image_2512_bf16.safetensors",
          "sha256": "3c27bbc92757da35e0963bc75c8635feeab41ee6adaae93d159c397d1b39cc98",
          "size": 43927101440,
          "format": "safetensors",
          "precision": "bf16",
          "vram_required": 40960,
          "vram_recommended": 49152,
          "note": "2512 revision, full bf16 precision."
        },
        {
          "id": "2512-fp8",
          "file": "qwen_image_2512_fp8_e4m3fn.safetensors",
          "url": "https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/split_files/diffusion_models/qwen_image_2512_fp8_e4m3fn.safetensors",
          "sha256": "f07f1483dc9b19752b6032576a82c94b6e6cce3e0cad77ae27e928b7072b1c05",
          "size": 21902483456,
          "format": "safetensors",
          "precision": "fp8-e4m3fn",
          "vram_required": 20480,
          "vram_recommended": 24576,
          "note": "2512 revision, fp8 quantized."
        },
        {
          "id": "gguf-q8-0",
          "file": "Qwen_Image_Edit-Q8_0.gguf",
          "url": "https://huggingface.co/QuantStack/Qwen-Image-Edit-GGUF/resolve/main/Qwen_Image_Edit-Q8_0.gguf",
          "sha256": "e875c08781c562182cdc64939aab70ccfae3c44d9484e3c3d9429bad8682cf71",
          "size": 23405215744,
          "format": "gguf",
          "precision": "q8_0",
          "vram_required": 24576,
          "vram_recommended": 32768,
          "note": "GGUF Q8_0 â€” best GGUF quality. Requires ComfyUI-GGUF custom node."
        },
        {
          "id": "gguf-q6-k",
          "file": "Qwen_Image_Edit-Q6_K.gguf",
          "url": "https://huggingface.co/QuantStack/Qwen-Image-Edit-GGUF/resolve/main/Qwen_Image_Edit-Q6_K.gguf",
          "sha256": "2a6952b23262b881dfb681791482f00cbd4dc2f59016e55c14a09d5c2a9cc8b2",
          "size": 18039808000,
          "format": "gguf",
          "precision": "q6_k",
          "vram_required": 20480,
          "vram_recommended": 24576,
          "note": "GGUF Q6_K â€” good quality/size balance."
        },
        {
          "id": "gguf-q5-k-m",
          "file": "Qwen_Image_Edit-Q5_K_M.gguf",
          "url": "https://huggingface.co/QuantStack/Qwen-Image-Edit-GGUF/resolve/main/Qwen_Image_Edit-Q5_K_M.gguf",
          "sha256": "b398a64e4334c2d8cae676ce64534a38d3ebdd3d0878b57071620485e56211ce",
          "size": 16000204800,
          "format": "gguf",
          "precision": "q5_k_m",
          "vram_required": 16384,
          "vram_recommended": 20480,
          "note": "GGUF Q5_K_M â€” recommended for 16GB+ GPUs."
        },
        {
          "id": "gguf-q5-k-s",
          "file": "Qwen_Image_Edit-Q5_K_S.gguf",
          "url": "https://huggingface.co/QuantStack/Qwen-Image-Edit-GGUF/resolve/main/Qwen_Image_Edit-Q5_K_S.gguf",
          "sha256": "286571d761951e43853a1cec129c741cc7f541fb70b203f4131c84cafd8585f0",
          "size": 15140249600,
          "format": "gguf",
          "precision": "q5_k_s",
          "vram_required": 16384,
          "vram_recommended": 20480,
          "note": "GGUF Q5_K_S â€” smaller Q5 variant."
        },
        {
          "id": "gguf-q5-0",
          "file": "Qwen_Image_Edit-Q5_0.gguf",
          "url": "https://huggingface.co/QuantStack/Qwen-Image-Edit-GGUF/resolve/main/Qwen_Image_Edit-Q5_0.gguf",
          "sha256": "a8ace3d71b9b8e87859b330923f2d40421116a4548ad5fff180fad730cb29352",
          "size": 15461882880,
          "format": "gguf",
          "precision": "q5_0",
          "vram_required": 16384,
          "vram_recommended": 20480,
          "note": "GGUF Q5_0."
        },
        {
          "id": "gguf-q5-1",
          "file": "Qwen_Image_Edit-Q5_1.gguf",
          "url": "https://huggingface.co/QuantStack/Qwen-Image-Edit-GGUF/resolve/main/Qwen_Image_Edit-Q5_1.gguf",
          "sha256": "4006179697444691f31e0fbb7a8beb6d373d1a0d6ff4f34a2e3efa5e8d9c0f85",
          "size": 16536027136,
          "format": "gguf",
          "precision": "q5_1",
          "vram_required": 16384,
          "vram_recommended": 20480,
          "note": "GGUF Q5_1."
        },
        {
          "id": "gguf-q4-k-m",
          "file": "Qwen_Image_Edit-Q4_K_M.gguf",
          "url": "https://huggingface.co/QuantStack/Qwen-Image-Edit-GGUF/resolve/main/Qwen_Image_Edit-Q4_K_M.gguf",
          "sha256": "bee346224c34ec00991aa1d75a0fd4c259c4ac375ad346edf4c3fff7ec30be1c",
          "size": 14066032640,
          "format": "gguf",
          "precision": "q4_k_m",
          "vram_required": 12288,
          "vram_recommended": 16384,
          "note": "GGUF Q4_K_M â€” good for 12GB GPUs."
        },
        {
          "id": "gguf-q4-k-s",
          "file": "Qwen_Image_Edit-Q4_K_S.gguf",
          "url": "https://huggingface.co/QuantStack/Qwen-Image-Edit-GGUF/resolve/main/Qwen_Image_Edit-Q4_K_S.gguf",
          "sha256": "44301250d40bfbe53c9d2b62028bab576c52605b471b1a11ea0f9f1036b19743",
          "size": 12994428928,
          "format": "gguf",
          "precision": "q4_k_s",
          "vram_required": 12288,
          "vram_recommended": 16384,
          "note": "GGUF Q4_K_S â€” smaller Q4 variant."
        },
        {
          "id": "gguf-q4-0",
          "file": "Qwen_Image_Edit-Q4_0.gguf",
          "url": "https://huggingface.co/QuantStack/Qwen-Image-Edit-GGUF/resolve/main/Qwen_Image_Edit-Q4_0.gguf",
          "sha256": "40d9298d5a7881afdf62e9bfa9581cbbe435fb2038448dde8f0a127a48c49fbb",
          "size": 12776923136,
          "format": "gguf",
          "precision": "q4_0",
          "vram_required": 12288,
          "vram_recommended": 16384,
          "note": "GGUF Q4_0."
        },
        {
          "id": "gguf-q4-1",
          "file": "Qwen_Image_Edit-Q4_1.gguf",
          "url": "https://huggingface.co/QuantStack/Qwen-Image-Edit-GGUF/resolve/main/Qwen_Image_Edit-Q4_1.gguf",
          "sha256": "55ed396b0ed764a5f237b87307ec0679e4d59b71e984ef61ea89d46457017eb5",
          "size": 13743095808,
          "format": "gguf",
          "precision": "q4_1",
          "vram_required": 12288,
          "vram_recommended": 16384,
          "note": "GGUF Q4_1."
        },
        {
          "id": "gguf-q3-k-m",
          "file": "Qwen_Image_Edit-Q3_K_M.gguf",
          "url": "https://huggingface.co/QuantStack/Qwen-Image-Edit-GGUF/resolve/main/Qwen_Image_Edit-Q3_K_M.gguf",
          "sha256": "056411f536e5b6ef6efa08d3c773911d9ec0945dbfce9be0f43c5519c525b3ee",
          "size": 10394116096,
          "format": "gguf",
          "precision": "q3_k_m",
          "vram_required": 10240,
          "vram_recommended": 12288,
          "note": "GGUF Q3_K_M â€” for 10GB+ GPUs. Noticeable quality reduction."
        },
        {
          "id": "gguf-q3-k-s",
          "file": "Qwen_Image_Edit-Q3_K_S.gguf",
          "url": "https://huggingface.co/QuantStack/Qwen-Image-Edit-GGUF/resolve/main/Qwen_Image_Edit-Q3_K_S.gguf",
          "sha256": "c2e82411c73d1cc98db87bfc640381eb211f9c28ba57e943e3fe71283cf97cf6",
          "size": 9609625600,
          "format": "gguf",
          "precision": "q3_k_s",
          "vram_required": 10240,
          "vram_recommended": 12288,
          "note": "GGUF Q3_K_S â€” smaller Q3 variant."
        },
        {
          "id": "gguf-q2-k",
          "file": "Qwen_Image_Edit-Q2_K.gguf",
          "url": "https://huggingface.co/QuantStack/Qwen-Image-Edit-GGUF/resolve/main/Qwen_Image_Edit-Q2_K.gguf",
          "sha256": "a449446a0fedad1949032f32f70ef5f8514e699ad0c3f27d8ab04b9f87b6d992",
          "size": 7580321792,
          "format": "gguf",
          "precision": "q2_k",
          "vram_required": 8192,
          "vram_recommended": 10240,
          "note": "GGUF Q2_K â€” minimum quality. Fits on 8GB GPUs but significant quality loss."
        }
      ],
      "requires": [
        {
          "id": "qwen-image-vae",
          "type": "vae",
          "reason": "Qwen Image Edit requires the Qwen-specific VAE"
        },
        {
          "id": "qwen-image-clip",
          "type": "text_encoder",
          "reason": "Qwen 2.5 VL 7B text/vision encoder for prompt processing"
        }
      ],
      "defaults": {
        "steps": 8,
        "cfg": 1.0,
        "sampler": "euler",
        "scheduler": "simple"
      },
      "tags": [
        "qwen",
        "image-editing",
        "outpainting",
        "scene-generation",
        "comfyui",
        "gguf"
      ],
      "rating": 4.8,
      "downloads": 275000,
      "added": "2025-07-01",
      "updated": "2026-01-15"
    },
    {
      "id": "qwen-image-edit-lightning",
      "name": "Qwen Image Edit Lightning LoRA",
      "type": "lora",
      "author": "lightx2v",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/lightx2v/Qwen-Image-Lightning",
      "description": "Distillation LoRA for Qwen Image Edit that reduces inference from ~50 steps to 4-8 steps.\nMultiple versions available: V1.0 and V2.0, in 4-step and 8-step variants.\nAlso includes Edit-specific variants for image editing (vs generation).\nbf16 variants are half the size (~850 MB) with no quality loss on bf16/fp8 base models.\n",
      "base_models": [
        "qwen-image-edit"
      ],
      "variants": [
        {
          "id": "edit-8step-v1",
          "file": "Qwen-Image-Edit-Lightning-8steps-V1.0.safetensors",
          "url": "https://huggingface.co/lightx2v/Qwen-Image-Lightning/resolve/main/Qwen-Image-Edit-Lightning-8steps-V1.0.safetensors",
          "sha256": "5910104f8922bd3fa359c675ba2a72681327f538cfa768eb33044055bd27a826",
          "size": 1825361920,
          "format": "safetensors",
          "precision": "fp32",
          "note": "Edit-specific, 8-step, V1.0. fp32 weights. Used by shopify-reframe-ai."
        },
        {
          "id": "edit-8step-v1-bf16",
          "file": "Qwen-Image-Edit-Lightning-8steps-V1.0-bf16.safetensors",
          "url": "https://huggingface.co/lightx2v/Qwen-Image-Lightning/resolve/main/Qwen-Image-Edit-Lightning-8steps-V1.0-bf16.safetensors",
          "sha256": "10b750b221b36c9d9f3a3d693f0489714a63b7ba84c45606de5c96ed367156d7",
          "size": 912680960,
          "format": "safetensors",
          "precision": "bf16",
          "note": "Edit-specific, 8-step, V1.0 in bf16. Half the size."
        },
        {
          "id": "edit-4step-v1",
          "file": "Qwen-Image-Edit-Lightning-4steps-V1.0.safetensors",
          "url": "https://huggingface.co/lightx2v/Qwen-Image-Lightning/resolve/main/Qwen-Image-Edit-Lightning-4steps-V1.0.safetensors",
          "sha256": "376a95559695f41bfa97349f6232e78193e05d404cc507c2cdfdaae58632bb4b",
          "size": 1825361920,
          "format": "safetensors",
          "precision": "fp32",
          "note": "Edit-specific, 4-step, V1.0. Fastest but slightly lower quality than 8-step."
        },
        {
          "id": "edit-4step-v1-bf16",
          "file": "Qwen-Image-Edit-Lightning-4steps-V1.0-bf16.safetensors",
          "url": "https://huggingface.co/lightx2v/Qwen-Image-Lightning/resolve/main/Qwen-Image-Edit-Lightning-4steps-V1.0-bf16.safetensors",
          "sha256": "d8132c32e7df906603dd6b072ff2fb0af88ab15ef0f3ac697a2011c8b47bbeb1",
          "size": 912680960,
          "format": "safetensors",
          "precision": "bf16",
          "note": "Edit-specific, 4-step, V1.0 in bf16."
        },
        {
          "id": "gen-8step-v2",
          "file": "Qwen-Image-Lightning-8steps-V2.0.safetensors",
          "url": "https://huggingface.co/lightx2v/Qwen-Image-Lightning/resolve/main/Qwen-Image-Lightning-8steps-V2.0.safetensors",
          "sha256": "47e96584c92ce971dc3e3db8626e065d2765924f29c637ccbdac79adc1d2c562",
          "size": 1825361920,
          "format": "safetensors",
          "precision": "fp32",
          "note": "Generation-focused, 8-step, V2.0. Latest version."
        },
        {
          "id": "gen-8step-v2-bf16",
          "file": "Qwen-Image-Lightning-8steps-V2.0-bf16.safetensors",
          "url": "https://huggingface.co/lightx2v/Qwen-Image-Lightning/resolve/main/Qwen-Image-Lightning-8steps-V2.0-bf16.safetensors",
          "sha256": "4d8ffbd8c5ddc8637cf7b1e1e987ffb58b9146b972d09ce2002f3f29cf6fc17d",
          "size": 912680960,
          "format": "safetensors",
          "precision": "bf16",
          "note": "Generation-focused, 8-step, V2.0 in bf16."
        },
        {
          "id": "gen-4step-v2",
          "file": "Qwen-Image-Lightning-4steps-V2.0.safetensors",
          "url": "https://huggingface.co/lightx2v/Qwen-Image-Lightning/resolve/main/Qwen-Image-Lightning-4steps-V2.0.safetensors",
          "sha256": "d06a178e7f47d4f6127284815bf93fc152f6c420a9c8ae370ec7d213f244a535",
          "size": 1825361920,
          "format": "safetensors",
          "precision": "fp32",
          "note": "Generation-focused, 4-step, V2.0. Fastest generation."
        },
        {
          "id": "gen-4step-v2-bf16",
          "file": "Qwen-Image-Lightning-4steps-V2.0-bf16.safetensors",
          "url": "https://huggingface.co/lightx2v/Qwen-Image-Lightning/resolve/main/Qwen-Image-Lightning-4steps-V2.0-bf16.safetensors",
          "sha256": "e8ca961a24c5dd7744eb0c8c2af152ce9b366f8cfabdaaca96d72acbca868f5d",
          "size": 912680960,
          "format": "safetensors",
          "precision": "bf16",
          "note": "Generation-focused, 4-step, V2.0 in bf16."
        },
        {
          "id": "fp8-gen-4step-v1-bf16",
          "file": "Qwen-Image-fp8-e4m3fn-Lightning-4steps-V1.0-bf16.safetensors",
          "url": "https://huggingface.co/lightx2v/Qwen-Image-Lightning/resolve/main/Qwen-Image-fp8-e4m3fn-Lightning-4steps-V1.0-bf16.safetensors",
          "sha256": "a900d5b842a25451fbdd14361cf730a0870f2df5221cbadee855ff3ac91a7bdc",
          "size": 912680960,
          "format": "safetensors",
          "precision": "bf16",
          "note": "Trained specifically for fp8 base model. 4-step, bf16."
        },
        {
          "id": "fp8-gen-4step-v1-fp32",
          "file": "Qwen-Image-fp8-e4m3fn-Lightning-4steps-V1.0-fp32.safetensors",
          "url": "https://huggingface.co/lightx2v/Qwen-Image-Lightning/resolve/main/Qwen-Image-fp8-e4m3fn-Lightning-4steps-V1.0-fp32.safetensors",
          "sha256": "acafb9d389bb2ec78d94276da4ff2897ecc7436bef283d81d041199a03a17e03",
          "size": 1825361920,
          "format": "safetensors",
          "precision": "fp32",
          "note": "Trained specifically for fp8 base model. 4-step, fp32."
        }
      ],
      "recommended_weight": 1.0,
      "weight_range": [
        0.8,
        1.0
      ],
      "tags": [
        "qwen",
        "lora",
        "lightning",
        "distillation",
        "fast-inference",
        "comfyui"
      ],
      "rating": 4.7,
      "downloads": 773000,
      "added": "2025-08-01",
      "updated": "2025-10-01"
    },
    {
      "id": "qwen-image-vae",
      "name": "Qwen Image VAE",
      "type": "vae",
      "architecture": "qwen-image",
      "author": "Comfy-Org",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI",
      "description": "VAE for the Qwen Image Edit pipeline. Single file, no variants.\nRequired by all Qwen Image Edit checkpoints.\n",
      "file": {
        "url": "https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/split_files/vae/qwen_image_vae.safetensors",
        "sha256": "a70580f0213e67967ee9c95f05bb400e8fb08307e017a924bf3441223e023d1f",
        "size": 266338304,
        "format": "safetensors"
      },
      "tags": [
        "qwen",
        "vae",
        "comfyui"
      ],
      "rating": 5.0,
      "downloads": 275000,
      "added": "2025-07-01",
      "updated": "2025-07-01"
    },
    {
      "id": "realesrgan-x4plus",
      "name": "RealESRGAN x4plus",
      "type": "upscaler",
      "author": "xinntao",
      "license": "bsd-3-clause",
      "homepage": "https://github.com/xinntao/Real-ESRGAN",
      "description": "General-purpose 4x upscaler from the Real-ESRGAN project.\nGood balance of quality and speed. Works well for both\nphotos and AI-generated images.\n",
      "scale_factor": 4,
      "file": {
        "url": "https://huggingface.co/ai-forever/Real-ESRGAN/resolve/main/RealESRGAN_x4.pth",
        "sha256": "aa00f09ad753d88576b21ed977e97d634976377031b178acc3b5b238df463400",
        "size": 67040989,
        "format": "pth"
      },
      "tags": [
        "upscaler",
        "4x",
        "realesrgan",
        "general-purpose"
      ],
      "rating": 4.7,
      "downloads": 4500000,
      "added": "2022-06-01",
      "updated": "2024-01-01"
    },
    {
      "id": "sd-1.5",
      "name": "Stable Diffusion 1.5",
      "type": "checkpoint",
      "architecture": "sd15",
      "author": "runwayml",
      "license": "creativeml-openrail-m",
      "homepage": "https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5",
      "description": "The original Stable Diffusion 1.5. Lightweight, fast, huge ecosystem\nof LoRAs, embeddings, and ControlNets. 512x512 native resolution.\n",
      "variants": [
        {
          "id": "fp16",
          "file": "v1-5-pruned-emaonly.safetensors",
          "url": "https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors",
          "sha256": "2ac63bfb6186057d88b65c3aa47ec90fd8d9aa3269164fc37fed1cb6f1a1efd0",
          "size": 4265146304,
          "format": "safetensors",
          "precision": "fp16",
          "vram_required": 4096,
          "vram_recommended": 8192
        }
      ],
      "requires": [
        {
          "id": "sd-vae-ft-mse",
          "type": "vae",
          "reason": "Recommended VAE for SD 1.5 (sharper outputs)"
        }
      ],
      "defaults": {
        "steps": 25,
        "cfg": 7.5,
        "sampler": "dpmpp_2m",
        "scheduler": "karras"
      },
      "tags": [
        "sd15",
        "text-to-image",
        "stable-diffusion",
        "lightweight"
      ],
      "rating": 4.3,
      "downloads": 12000000,
      "added": "2022-10-20",
      "updated": "2024-01-01",
      "preview_images": [
        "https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly-demo-images/image_01.png"
      ]
    },
    {
      "id": "sd-2.1",
      "name": "Stable Diffusion 2.1",
      "type": "checkpoint",
      "architecture": "sd21",
      "author": "stabilityai",
      "license": "openrail++",
      "homepage": "https://huggingface.co/stabilityai/stable-diffusion-2-1",
      "description": "Stable Diffusion 2.1, fine-tuned from SD 2.0 with improved aesthetics.\nGenerates at 768x768 resolution. Uses OpenCLIP ViT-H/14 text encoder.\n",
      "file": {
        "url": "https://huggingface.co/stabilityai/stable-diffusion-2-1/resolve/main/v2-1_768-ema-pruned.safetensors",
        "sha256": "ad2a33c361c1f593c4a1571e8b1f328c6b1c7b7d8d5eabe3cee16e241c1f3b50",
        "size": 5214865152,
        "format": "safetensors"
      },
      "defaults": {
        "steps": 30,
        "cfg": 7.5,
        "sampler": "dpmpp_2m",
        "scheduler": "karras"
      },
      "tags": [
        "sd21",
        "text-to-image",
        "stable-diffusion",
        "768"
      ],
      "rating": 4.4,
      "downloads": 3200000,
      "added": "2022-12-07",
      "updated": "2024-01-01"
    },
    {
      "id": "sd-vae-ft-mse",
      "name": "SD VAE ft-MSE",
      "type": "vae",
      "architecture": "sd15",
      "author": "stabilityai",
      "license": "creativeml-openrail-m",
      "homepage": "https://huggingface.co/stabilityai/sd-vae-ft-mse",
      "description": "Fine-tuned VAE for Stable Diffusion 1.5. Produces sharper, more\ndetailed outputs compared to the default VAE. MSE-optimized.\n",
      "file": {
        "url": "https://huggingface.co/stabilityai/sd-vae-ft-mse/resolve/main/diffusion_pytorch_model.safetensors",
        "sha256": "a1d993488569e928462932c8c38a0760b874d166399b14414135bd9c42df5815",
        "size": 334643276,
        "format": "safetensors"
      },
      "tags": [
        "sd15",
        "vae",
        "fine-tuned",
        "mse"
      ],
      "rating": 4.7,
      "downloads": 6800000,
      "added": "2023-01-01",
      "updated": "2024-01-01"
    },
    {
      "id": "sd15-controlnet-canny",
      "name": "ControlNet v1.1 Canny (SD 1.5)",
      "type": "controlnet",
      "architecture": "sd15",
      "author": "lllyasviel",
      "license": "openrail",
      "homepage": "https://huggingface.co/lllyasviel/ControlNet-v1-1",
      "description": "ControlNet v1.1 for SD 1.5 â€” Canny edge detection conditioned generation.\nProvides structural control using canny edge maps. The most popular ControlNet.\n",
      "base_models": [
        "sd-1.5"
      ],
      "preprocessor": "canny",
      "file": {
        "url": "https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11p_sd15_canny.pth",
        "sha256": "f99cfe4c70910e38e3fece9918a4979ed7d3dcf9b81cee293e1755363af5406a",
        "size": 1557135360,
        "format": "pth"
      },
      "tags": [
        "sd15",
        "controlnet",
        "canny",
        "edge-detection"
      ],
      "rating": 4.8,
      "downloads": 4200000,
      "added": "2023-04-14",
      "updated": "2024-01-01"
    },
    {
      "id": "sd15-controlnet-depth",
      "name": "ControlNet v1.1 Depth (SD 1.5)",
      "type": "controlnet",
      "architecture": "sd15",
      "author": "lllyasviel",
      "license": "openrail",
      "homepage": "https://huggingface.co/lllyasviel/ControlNet-v1-1",
      "description": "ControlNet v1.1 for SD 1.5 â€” Depth map conditioned generation.\nUses MiDaS depth estimation maps for structural control.\n",
      "base_models": [
        "sd-1.5"
      ],
      "preprocessor": "depth_midas",
      "file": {
        "url": "https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11f1p_sd15_depth.pth",
        "sha256": "c48b0e8e0e22db42b8c6532b75cae8e8e8e8e0e25c8e4c2b8e6b1c0a0e0e0e0",
        "size": 1557135360,
        "format": "pth"
      },
      "tags": [
        "sd15",
        "controlnet",
        "depth",
        "midas"
      ],
      "rating": 4.7,
      "downloads": 3500000,
      "added": "2023-04-14",
      "updated": "2024-01-01"
    },
    {
      "id": "sd15-controlnet-openpose",
      "name": "ControlNet v1.1 OpenPose (SD 1.5)",
      "type": "controlnet",
      "architecture": "sd15",
      "author": "lllyasviel",
      "license": "openrail",
      "homepage": "https://huggingface.co/lllyasviel/ControlNet-v1-1",
      "description": "ControlNet v1.1 for SD 1.5 â€” OpenPose body pose conditioned generation.\nUses human pose skeleton for structural control. Great for character posing.\n",
      "base_models": [
        "sd-1.5"
      ],
      "preprocessor": "openpose",
      "file": {
        "url": "https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11p_sd15_openpose.pth",
        "sha256": "8383832959b1e8b37660c5f236ab449c0d760a9df1db4c481cb881debf110d21",
        "size": 1557135360,
        "format": "pth"
      },
      "tags": [
        "sd15",
        "controlnet",
        "openpose",
        "pose"
      ],
      "rating": 4.7,
      "downloads": 3100000,
      "added": "2023-04-14",
      "updated": "2024-01-01"
    },
    {
      "id": "sdxl-base-1.0",
      "name": "Stable Diffusion XL Base 1.0",
      "type": "checkpoint",
      "architecture": "sdxl",
      "author": "stabilityai",
      "license": "openrail++",
      "homepage": "https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0",
      "description": "Stable Diffusion XL base model. High resolution text-to-image generation\nat 1024x1024. Works great with LoRAs and ControlNets.\n",
      "variants": [
        {
          "id": "fp16",
          "file": "sd_xl_base_1.0.safetensors",
          "url": "https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors",
          "sha256": "31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b",
          "size": 6938078334,
          "format": "safetensors",
          "precision": "fp16",
          "vram_required": 8192,
          "vram_recommended": 12288
        }
      ],
      "requires": [
        {
          "id": "sdxl-vae-fp16-fix",
          "type": "vae",
          "reason": "Recommended VAE for SDXL (fixes fp16 NaN issues)"
        }
      ],
      "defaults": {
        "steps": 30,
        "cfg": 7.0,
        "sampler": "dpmpp_2m",
        "scheduler": "karras"
      },
      "preview_images": [
        "https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/output.png"
      ],
      "tags": [
        "sdxl",
        "text-to-image",
        "stable-diffusion",
        "high-resolution"
      ],
      "rating": 4.6,
      "downloads": 5400000,
      "added": "2023-07-26",
      "updated": "2024-06-01"
    },
    {
      "id": "sdxl-controlnet-canny",
      "name": "ControlNet Canny (SDXL)",
      "type": "controlnet",
      "architecture": "sdxl",
      "author": "diffusers",
      "license": "openrail++",
      "homepage": "https://huggingface.co/diffusers/controlnet-canny-sdxl-1.0",
      "description": "ControlNet trained on SDXL for Canny edge detection conditioning.\nSingle-file diffusers-format model for structural control in SDXL.\n",
      "base_models": [
        "sdxl-base-1.0"
      ],
      "preprocessor": "canny",
      "variants": [
        {
          "id": "fp16",
          "file": "diffusion_pytorch_model.fp16.safetensors",
          "url": "https://huggingface.co/diffusers/controlnet-canny-sdxl-1.0/resolve/main/diffusion_pytorch_model.fp16.safetensors",
          "sha256": "b2e7d3921058a442cc80430d1ec8847f42599c705e2451c95e77cf4dcf8d6c25",
          "size": 2502139904,
          "format": "safetensors",
          "precision": "fp16",
          "vram_required": 8192
        },
        {
          "id": "fp32",
          "file": "diffusion_pytorch_model.safetensors",
          "url": "https://huggingface.co/diffusers/controlnet-canny-sdxl-1.0/resolve/main/diffusion_pytorch_model.safetensors",
          "sha256": "661481e7dbef1c09faa589c334b9d6d6595e2b7c85c77dbd8e0ec7f2a2ab2d75",
          "size": 5004279808,
          "format": "safetensors",
          "precision": "fp32",
          "vram_required": 16384
        }
      ],
      "tags": [
        "sdxl",
        "controlnet",
        "canny",
        "edge-detection"
      ],
      "rating": 4.6,
      "downloads": 1500000,
      "added": "2023-09-01",
      "updated": "2024-06-01"
    },
    {
      "id": "sdxl-refiner-1.0",
      "name": "SDXL Refiner 1.0",
      "type": "checkpoint",
      "architecture": "sdxl",
      "author": "stabilityai",
      "license": "openrail++",
      "homepage": "https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0",
      "description": "SDXL 1.0 Refiner â€” small-detail expert model. Used as a second pass\nto add fine details to images generated by SDXL Base.\n",
      "file": {
        "url": "https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/resolve/main/sd_xl_refiner_1.0.safetensors",
        "sha256": "7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f",
        "size": 6527995904,
        "format": "safetensors"
      },
      "requires": [
        {
          "id": "sdxl-vae-fp16-fix",
          "type": "vae",
          "reason": "Recommended VAE for SDXL"
        }
      ],
      "defaults": {
        "steps": 20,
        "cfg": 3.5,
        "sampler": "dpmpp_2m",
        "scheduler": "karras"
      },
      "tags": [
        "sdxl",
        "refiner",
        "detail",
        "stable-diffusion"
      ],
      "rating": 4.5,
      "downloads": 1800000,
      "added": "2023-07-26",
      "updated": "2024-06-01"
    },
    {
      "id": "sdxl-turbo",
      "name": "SDXL Turbo",
      "type": "checkpoint",
      "architecture": "sdxl",
      "author": "stabilityai",
      "license": "sai-nc-community",
      "homepage": "https://huggingface.co/stabilityai/sdxl-turbo",
      "description": "SDXL Turbo â€” distilled from SDXL 1.0 using Adversarial Diffusion Distillation.\nGenerates images in 1-4 steps at 512x512. Great for real-time applications.\n",
      "variants": [
        {
          "id": "fp16",
          "file": "sd_xl_turbo_1.0_fp16.safetensors",
          "url": "https://huggingface.co/stabilityai/sdxl-turbo/resolve/main/sd_xl_turbo_1.0_fp16.safetensors",
          "sha256": "e869ac7d6942cb327d68d5ed83a40447aadf20e0c3358d98b2cc9e270db0da26",
          "size": 6451034624,
          "format": "safetensors",
          "precision": "fp16",
          "vram_required": 6144,
          "vram_recommended": 8192
        },
        {
          "id": "fp32",
          "file": "sd_xl_turbo_1.0.safetensors",
          "url": "https://huggingface.co/stabilityai/sdxl-turbo/resolve/main/sd_xl_turbo_1.0.safetensors",
          "sha256": "d1bc2e4175e6ed0d0e8de98c42671384b8467b9bbdbcf558b4df66a427823d74",
          "size": 13902070784,
          "format": "safetensors",
          "precision": "fp32",
          "vram_required": 12288,
          "vram_recommended": 16384
        }
      ],
      "requires": [
        {
          "id": "sdxl-vae-fp16-fix",
          "type": "vae",
          "reason": "Recommended VAE for SDXL"
        }
      ],
      "defaults": {
        "steps": 1,
        "cfg": 0.0,
        "sampler": "euler_ancestral",
        "scheduler": "normal"
      },
      "tags": [
        "sdxl",
        "turbo",
        "fast",
        "text-to-image",
        "distilled"
      ],
      "rating": 4.7,
      "downloads": 2800000,
      "added": "2023-11-28",
      "updated": "2024-06-01"
    },
    {
      "id": "sdxl-vae-fp16-fix",
      "name": "SDXL VAE (fp16 NaN fix)",
      "type": "vae",
      "architecture": "sdxl",
      "author": "madebyollin",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/madebyollin/sdxl-vae-fp16-fix",
      "description": "Fixed SDXL VAE that works correctly in fp16 precision.\nThe original SDXL VAE produces NaN values in fp16 mode â€” this version\nfixes that issue. Recommended for all SDXL workflows.\n",
      "file": {
        "url": "https://huggingface.co/madebyollin/sdxl-vae-fp16-fix/resolve/main/diffusion_pytorch_model.safetensors",
        "sha256": "63aeecb90ff7bc1c115395962d3e803571385b61938377bc7089b36e81e92e2e",
        "size": 334643268,
        "format": "safetensors"
      },
      "tags": [
        "sdxl",
        "vae",
        "fp16-fix"
      ],
      "rating": 4.9,
      "downloads": 4100000,
      "added": "2023-08-15",
      "updated": "2024-01-01"
    },
    {
      "id": "t5-xxl-fp16",
      "name": "T5-XXL Text Encoder (fp16)",
      "type": "text_encoder",
      "architecture": "t5",
      "author": "comfyanonymous",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/comfyanonymous/flux_text_encoders",
      "description": "T5-XXL text encoder in fp16 precision. Required by FLUX models\nfor prompt processing. Large model â€” 9.8 GB.\nUse fp8 variant if you have less than 24 GB VRAM.\n",
      "variants": [
        {
          "id": "fp16",
          "file": "t5xxl_fp16.safetensors",
          "url": "https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors",
          "sha256": "eb88d1baeef3a46f6b723f62a69e91a116a34c60b0c0a32c9571d43551615213",
          "size": 9787849216,
          "format": "safetensors",
          "precision": "fp16",
          "vram_required": 12288,
          "vram_recommended": 16384
        }
      ],
      "tags": [
        "t5",
        "text-encoder",
        "flux",
        "fp16"
      ],
      "rating": 5.0,
      "downloads": 2400000,
      "added": "2024-08-01",
      "updated": "2024-08-01"
    },
    {
      "id": "t5-xxl-fp8",
      "name": "T5-XXL Text Encoder (fp8)",
      "type": "text_encoder",
      "architecture": "t5",
      "author": "comfyanonymous",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/comfyanonymous/flux_text_encoders",
      "description": "T5-XXL text encoder quantized to fp8 precision. Uses half the VRAM\nof the fp16 version with minimal quality impact.\nRecommended for 12-16 GB VRAM setups using FLUX models.\n",
      "variants": [
        {
          "id": "fp8",
          "file": "t5xxl_fp8_e4m3fn.safetensors",
          "url": "https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors",
          "sha256": "7d330da4816157540d6bb7838bf63a0f02f573fc48ca4d8de34bb0cbfd514f09",
          "size": 4893843456,
          "format": "safetensors",
          "precision": "fp8-e4m3fn",
          "vram_required": 6144,
          "vram_recommended": 8192
        }
      ],
      "tags": [
        "t5",
        "text-encoder",
        "flux",
        "fp8",
        "quantized"
      ],
      "rating": 4.8,
      "downloads": 1800000,
      "added": "2024-08-01",
      "updated": "2024-08-01"
    },
    {
      "id": "umt5-xxl",
      "name": "UMT5-XXL Text Encoder",
      "type": "text_encoder",
      "architecture": "umt5",
      "author": "Comfy-Org",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged",
      "description": "UMT5-XXL multilingual text encoder for Wan 2.1/2.2 video generation models.\nHandles prompt encoding for all Wan model variants.\n",
      "variants": [
        {
          "id": "fp16",
          "file": "umt5_xxl_fp16.safetensors",
          "url": "https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors",
          "sha256": "7b8850f1961e1cf8a77cca4c964a358d303f490833c6c087d0cff4b2f99db2af",
          "size": 11366399385,
          "format": "safetensors",
          "precision": "fp16",
          "vram_required": 24576
        },
        {
          "id": "fp8",
          "file": "umt5_xxl_fp8_e4m3fn_scaled.safetensors",
          "url": "https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors",
          "sha256": "c3355d30191f1f066b26d93fba017ae9809dce6c627dda5f6a66eaa651204f68",
          "size": 6735906897,
          "format": "safetensors",
          "precision": "fp8-e4m3fn",
          "vram_required": 8192
        }
      ],
      "tags": [
        "umt5",
        "text-encoder",
        "wan",
        "multilingual",
        "video"
      ],
      "rating": 4.8,
      "downloads": 1500000,
      "added": "2025-03-01",
      "updated": "2025-06-01"
    },
    {
      "id": "wan21-vae",
      "name": "Wan 2.1 VAE",
      "type": "vae",
      "architecture": "wan",
      "author": "Wan-AI",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged",
      "description": "VAE for Wan 2.1/2.2 14B video generation models. Compact 254 MB VAE\nused by the T2V and I2V 14B models.\n",
      "file": {
        "url": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors",
        "sha256": "2fc39d31359a4b0a64f55876d8ff7fa8d780956ae2cb13463b0223e15148976b",
        "size": 253815318,
        "format": "safetensors"
      },
      "tags": [
        "wan",
        "wan2.1",
        "wan2.2",
        "vae",
        "video"
      ],
      "rating": 4.8,
      "downloads": 1200000,
      "added": "2025-03-01",
      "updated": "2025-06-01"
    },
    {
      "id": "wan22-i2v-high-noise-14b",
      "name": "Wan 2.2 I2V 14B",
      "type": "diffusion_model",
      "architecture": "wan",
      "author": "Wan-AI",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B",
      "description": "Wan 2.2 14B image-to-video model (high-noise expert). Converts static images\ninto dynamic videos with smooth motion. Uses MoE architecture with separate\nhigh-noise and low-noise experts.\n",
      "variants": [
        {
          "id": "fp16",
          "file": "wan2.2_i2v_high_noise_14B_fp16.safetensors",
          "url": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_i2v_high_noise_14B_fp16.safetensors",
          "sha256": "c21c21efa368d529a982a7d89dd11d86608b5937cae9a125d0ffe5491918a100",
          "size": 28577914792,
          "format": "safetensors",
          "precision": "fp16",
          "vram_required": 32768
        },
        {
          "id": "fp8",
          "file": "wan2.2_i2v_high_noise_14B_fp8_scaled.safetensors",
          "url": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_i2v_high_noise_14B_fp8_scaled.safetensors",
          "sha256": "6122e79d55e0f235698d11d657f3b196c5273c830da00b2b013c5a048d5e6a42",
          "size": 14294742832,
          "format": "safetensors",
          "precision": "fp8-e4m3fn",
          "vram_required": 16384
        }
      ],
      "requires": [
        {
          "id": "wan22-i2v-low-noise-14b",
          "type": "diffusion_model",
          "reason": "Low-noise expert â€” required for Wan 2.2 MoE architecture"
        },
        {
          "id": "umt5-xxl",
          "type": "text_encoder",
          "reason": "UMT5-XXL text encoder for prompt processing"
        },
        {
          "id": "wan21-vae",
          "type": "vae",
          "reason": "Wan 2.1 VAE for video decoding"
        }
      ],
      "defaults": {
        "steps": 30,
        "cfg": 5.0
      },
      "tags": [
        "wan",
        "wan2.2",
        "video",
        "image-to-video",
        "14b",
        "moe",
        "high-quality"
      ],
      "rating": 4.9,
      "downloads": 720000,
      "added": "2025-06-01",
      "updated": "2025-06-01"
    },
    {
      "id": "wan22-i2v-low-noise-14b",
      "name": "Wan 2.2 I2V Low Noise Expert 14B",
      "type": "diffusion_model",
      "architecture": "wan",
      "author": "Wan-AI",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B",
      "description": "Low-noise expert for Wan 2.2 14B image-to-video MoE architecture.\nUsed together with the high-noise expert for video generation.\n",
      "variants": [
        {
          "id": "fp16",
          "file": "wan2.2_i2v_low_noise_14B_fp16.safetensors",
          "url": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_i2v_low_noise_14B_fp16.safetensors",
          "sha256": "edb89340c8a6fbf1a70e76a839ae01eaf7d289f05ea1ebd6b1a3fc6f533826e9",
          "size": 28577914792,
          "format": "safetensors",
          "precision": "fp16",
          "vram_required": 32768
        },
        {
          "id": "fp8",
          "file": "wan2.2_i2v_low_noise_14B_fp8_scaled.safetensors",
          "url": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_i2v_low_noise_14B_fp8_scaled.safetensors",
          "sha256": "5471a457b6ac404202a5fbe6c11595a3d5641fc766b00f38763f72303fffc21e",
          "size": 14294742832,
          "format": "safetensors",
          "precision": "fp8-e4m3fn",
          "vram_required": 16384
        }
      ],
      "tags": [
        "wan",
        "wan2.2",
        "video",
        "image-to-video",
        "14b",
        "moe",
        "low-noise"
      ],
      "rating": 4.9,
      "downloads": 600000,
      "added": "2025-06-01",
      "updated": "2025-06-01"
    },
    {
      "id": "wan22-t2v-high-noise-14b",
      "name": "Wan 2.2 T2V 14B",
      "type": "diffusion_model",
      "architecture": "wan",
      "author": "Wan-AI",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B",
      "description": "Wan 2.2 14B text-to-video model (high-noise expert). Uses MoE architecture\nwith separate high-noise and low-noise experts for cinematic-quality video.\nInstall this to get the full T2V 14B pipeline with all dependencies.\n",
      "variants": [
        {
          "id": "fp16",
          "file": "wan2.2_t2v_high_noise_14B_fp16.safetensors",
          "url": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_t2v_high_noise_14B_fp16.safetensors",
          "sha256": "c793e1515320fc8883ac8f97e2a45270c35edd8c4e334a6d42d3e95455bd8da2",
          "size": 28577095592,
          "format": "safetensors",
          "precision": "fp16",
          "vram_required": 32768
        },
        {
          "id": "fp8",
          "file": "wan2.2_t2v_high_noise_14B_fp8_scaled.safetensors",
          "url": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_t2v_high_noise_14B_fp8_scaled.safetensors",
          "sha256": "cad711ae211c8b23455ec68cd6a190a33a3d874234a77eb57266d73f8f0e6c9f",
          "size": 14293923632,
          "format": "safetensors",
          "precision": "fp8-e4m3fn",
          "vram_required": 16384
        }
      ],
      "requires": [
        {
          "id": "wan22-t2v-low-noise-14b",
          "type": "diffusion_model",
          "reason": "Low-noise expert â€” required for Wan 2.2 MoE architecture"
        },
        {
          "id": "umt5-xxl",
          "type": "text_encoder",
          "reason": "UMT5-XXL text encoder for prompt processing"
        },
        {
          "id": "wan21-vae",
          "type": "vae",
          "reason": "Wan 2.1 VAE for video decoding"
        }
      ],
      "defaults": {
        "steps": 30,
        "cfg": 5.0
      },
      "tags": [
        "wan",
        "wan2.2",
        "video",
        "text-to-video",
        "14b",
        "moe",
        "high-quality"
      ],
      "rating": 4.9,
      "downloads": 780000,
      "added": "2025-06-01",
      "updated": "2025-06-01"
    },
    {
      "id": "wan22-t2v-low-noise-14b",
      "name": "Wan 2.2 T2V Low Noise Expert 14B",
      "type": "diffusion_model",
      "architecture": "wan",
      "author": "Wan-AI",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B",
      "description": "Low-noise expert for Wan 2.2 14B text-to-video MoE architecture.\nUsed together with the high-noise expert for video generation.\n",
      "variants": [
        {
          "id": "fp16",
          "file": "wan2.2_t2v_low_noise_14B_fp16.safetensors",
          "url": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_t2v_low_noise_14B_fp16.safetensors",
          "sha256": "431d1613ffa809ae1f735b661a01788c6d74991f51efd01f45d5aee955ccd224",
          "size": 28577095592,
          "format": "safetensors",
          "precision": "fp16",
          "vram_required": 32768
        },
        {
          "id": "fp8",
          "file": "wan2.2_t2v_low_noise_14B_fp8_scaled.safetensors",
          "url": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_t2v_low_noise_14B_fp8_scaled.safetensors",
          "sha256": "e71b96d7c82e638694c5e7fb98fac4bfb0e4ddc5fbbb4b1df40da8f0f1278a97",
          "size": 14293923632,
          "format": "safetensors",
          "precision": "fp8-e4m3fn",
          "vram_required": 16384
        }
      ],
      "tags": [
        "wan",
        "wan2.2",
        "video",
        "text-to-video",
        "14b",
        "moe",
        "low-noise"
      ],
      "rating": 4.9,
      "downloads": 680000,
      "added": "2025-06-01",
      "updated": "2025-06-01"
    },
    {
      "id": "wan22-ti2v-5b",
      "name": "Wan 2.2 TI2V 5B",
      "type": "diffusion_model",
      "architecture": "wan",
      "author": "Wan-AI",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B",
      "description": "Wan 2.2 hybrid text-to-video and image-to-video 5B model. Fits on 8GB VRAM\nwith ComfyUI native offloading. Single model handles both T2V and I2V tasks.\nApache 2.0 license â€” free for commercial use.\n",
      "variants": [
        {
          "id": "fp16",
          "file": "wan2.2_ti2v_5B_fp16.safetensors",
          "url": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_ti2v_5B_fp16.safetensors",
          "sha256": "456f901338bd9eadbded3828b819109a9b68e8a525ca5cf8d0049a69fcfeca1e",
          "size": 9999658848,
          "format": "safetensors",
          "precision": "fp16",
          "vram_required": 10240
        },
        {
          "id": "gguf-q8-0",
          "file": "Wan2.2-TI2V-5B-Q8_0.gguf",
          "url": "https://huggingface.co/QuantStack/Wan2.2-TI2V-5B-GGUF/resolve/main/Wan2.2-TI2V-5B-Q8_0.gguf",
          "sha256": "57bece983817ab2f957546683bb670f13be7d99022d45674840cd999a050ea8f",
          "size": 5400179040,
          "format": "gguf",
          "vram_required": 8192
        },
        {
          "id": "gguf-q5-k-m",
          "file": "Wan2.2-TI2V-5B-Q5_K_M.gguf",
          "url": "https://huggingface.co/QuantStack/Wan2.2-TI2V-5B-GGUF/resolve/main/Wan2.2-TI2V-5B-Q5_K_M.gguf",
          "sha256": "4424633a876511b9be58a41119f7c9d762ea92b3cb74649cdb43cac850e42dba",
          "size": 3810603360,
          "format": "gguf",
          "vram_required": 6144
        },
        {
          "id": "gguf-q4-k-m",
          "file": "Wan2.2-TI2V-5B-Q4_K_M.gguf",
          "url": "https://huggingface.co/QuantStack/Wan2.2-TI2V-5B-GGUF/resolve/main/Wan2.2-TI2V-5B-Q4_K_M.gguf",
          "sha256": "95b19697b7f98e65b0a543640e9ca7b4dfec32e2a6e3731e8e10708be52655e2",
          "size": 3433116000,
          "format": "gguf",
          "vram_required": 5120
        },
        {
          "id": "gguf-q3-k-m",
          "file": "Wan2.2-TI2V-5B-Q3_K_M.gguf",
          "url": "https://huggingface.co/QuantStack/Wan2.2-TI2V-5B-GGUF/resolve/main/Wan2.2-TI2V-5B-Q3_K_M.gguf",
          "sha256": "93cab80a36db70e9f2152915870d6cb9bdc7fdbf61811153222b5251b92839e6",
          "size": 2547790176,
          "format": "gguf",
          "vram_required": 4096
        }
      ],
      "requires": [
        {
          "id": "umt5-xxl",
          "type": "text_encoder",
          "reason": "UMT5-XXL text encoder for prompt processing"
        },
        {
          "id": "wan22-vae",
          "type": "vae",
          "reason": "Wan 2.2 VAE for video decoding"
        }
      ],
      "defaults": {
        "steps": 30,
        "cfg": 5.0
      },
      "tags": [
        "wan",
        "wan2.2",
        "video",
        "text-to-video",
        "image-to-video",
        "5b"
      ],
      "rating": 4.8,
      "downloads": 920000,
      "added": "2025-06-01",
      "updated": "2025-06-01"
    },
    {
      "id": "wan22-vae",
      "name": "Wan 2.2 VAE",
      "type": "vae",
      "architecture": "wan",
      "author": "Wan-AI",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged",
      "description": "VAE for Wan 2.2 video generation models. New high-compression VAE\nused by the 5B TI2V model. 1.4 GB.\n",
      "file": {
        "url": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/vae/wan2.2_vae.safetensors",
        "sha256": "e40321bd36b9709991dae2530eb4ac303dd168276980d3e9bc4b6e2b75fed156",
        "size": 1409400960,
        "format": "safetensors"
      },
      "tags": [
        "wan",
        "wan2.2",
        "vae",
        "video"
      ],
      "rating": 4.8,
      "downloads": 850000,
      "added": "2025-06-01",
      "updated": "2025-06-01"
    },
    {
      "id": "z-image-text-encoder",
      "name": "Z-Image Qwen 3 4B Text Encoder",
      "type": "text_encoder",
      "architecture": "qwen3",
      "author": "Comfy-Org",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/Comfy-Org/z_image_turbo",
      "description": "Qwen 3 4B text encoder used by Z-Image-Turbo.\nAvailable in bf16 (full precision), fp8_mixed (reduced VRAM), and fp4_mixed (minimum VRAM).\nPlace in models/text_encoders/ and load with CLIPLoader node.\n",
      "variants": [
        {
          "id": "bf16",
          "file": "qwen_3_4b.safetensors",
          "url": "https://huggingface.co/Comfy-Org/z_image_turbo/resolve/main/split_files/text_encoders/qwen_3_4b.safetensors",
          "sha256": "6c671498573ac2f7a5501502ccce8d2b08ea6ca2f661c458e708f36b36edfc5a",
          "size": 8633139200,
          "format": "safetensors",
          "precision": "bf16",
          "vram_required": 10240,
          "vram_recommended": 12288,
          "note": "Full precision bf16. Best quality text encoding."
        },
        {
          "id": "fp8-mixed",
          "file": "qwen_3_4b_fp8_mixed.safetensors",
          "url": "https://huggingface.co/Comfy-Org/z_image_turbo/resolve/main/split_files/text_encoders/qwen_3_4b_fp8_mixed.safetensors",
          "sha256": "6cbe9dd2d57e7b59146d40aafeb6ed18d84c9f0e2169636109e56b55be6aadf6",
          "size": 6044237824,
          "format": "safetensors",
          "precision": "fp8-e4m3fn",
          "vram_required": 8192,
          "vram_recommended": 10240,
          "note": "Mixed fp8 quantization. Good quality with reduced VRAM."
        },
        {
          "id": "fp4-mixed",
          "file": "qwen_3_4b_fp4_mixed.safetensors",
          "url": "https://huggingface.co/Comfy-Org/z_image_turbo/resolve/main/split_files/text_encoders/qwen_3_4b_fp4_mixed.safetensors",
          "sha256": "fff344328a4fd638d3ab7a73f6cb4570ee9d269dab51e74a532d6267d5ed9ae3",
          "size": 3736076288,
          "format": "safetensors",
          "precision": "fp4",
          "vram_required": 4096,
          "vram_recommended": 6144,
          "note": "Mixed fp4 quantization. Minimum VRAM, some quality reduction."
        }
      ],
      "tags": [
        "z-image",
        "text-encoder",
        "qwen3",
        "comfyui"
      ],
      "rating": 4.8
    },
    {
      "id": "z-image-turbo",
      "name": "Z-Image-Turbo",
      "type": "diffusion_model",
      "architecture": "z-image",
      "author": "Tongyi-MAI / Comfy-Org / jayn7",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/Comfy-Org/z_image_turbo",
      "description": "Distilled 6B parameter text-to-image model from Alibaba Tongyi Lab.\nUses Scalable Single-Stream DiT (S3-DiT) architecture for maximum parameter efficiency.\nSub-second inference latency on H800 GPUs, fits within 16GB VRAM consumer devices.\nOnly 8 NFEs (steps) needed. Use euler sampler, guidance_scale 0.0.\nExcels at photorealistic generation and accurate bilingual text rendering (English & Chinese).\nAvailable as native safetensors (bf16/nvfp4) or GGUF quantizations.\n",
      "variants": [
        {
          "id": "bf16",
          "file": "z_image_turbo_bf16.safetensors",
          "url": "https://huggingface.co/Comfy-Org/z_image_turbo/resolve/main/split_files/diffusion_models/z_image_turbo_bf16.safetensors",
          "sha256": "2407613050b809ffdff18a4ac99af83ea6b95443ecebdf80e064a79c825574a6",
          "size": 13207024640,
          "format": "safetensors",
          "precision": "bf16",
          "vram_required": 16384,
          "vram_recommended": 24576,
          "note": "Full precision bf16. Best quality."
        },
        {
          "id": "nvfp4",
          "file": "z_image_turbo_nvfp4.safetensors",
          "url": "https://huggingface.co/Comfy-Org/z_image_turbo/resolve/main/split_files/diffusion_models/z_image_turbo_nvfp4.safetensors",
          "sha256": "6eabbfe5ae5a0c9b22cefffc83d5968f9f1ee77d68f32defe179ee7603e1470e",
          "size": 4843151360,
          "format": "safetensors",
          "precision": "nvfp4",
          "vram_required": 8192,
          "vram_recommended": 12288,
          "note": "NVIDIA fp4 quantization. Lower VRAM usage with minimal quality loss."
        },
        {
          "id": "gguf-q8-0",
          "file": "z_image_turbo-Q8_0.gguf",
          "url": "https://huggingface.co/jayn7/Z-Image-Turbo-GGUF/resolve/main/z_image_turbo-Q8_0.gguf",
          "sha256": "f163d60b0eb427469510b8226243d196574a18139a2e40c017409cfbda95ecfe",
          "size": 7755268096,
          "format": "gguf",
          "precision": "q8_0",
          "vram_required": 10240,
          "vram_recommended": 12288,
          "note": "GGUF Q8_0 â€” best GGUF quality. Requires ComfyUI-GGUF custom node."
        },
        {
          "id": "gguf-q6-k",
          "file": "z_image_turbo-Q6_K.gguf",
          "url": "https://huggingface.co/jayn7/Z-Image-Turbo-GGUF/resolve/main/z_image_turbo-Q6_K.gguf",
          "sha256": "fc137d87b49e06fdd5230d67d6c8cfa42a9e1fd38b65ccd355882450c3eb1c82",
          "size": 6346129408,
          "format": "gguf",
          "precision": "q6_k",
          "vram_required": 8192,
          "vram_recommended": 10240,
          "note": "GGUF Q6_K â€” good quality/size balance."
        },
        {
          "id": "gguf-q5-k-m",
          "file": "z_image_turbo-Q5_K_M.gguf",
          "url": "https://huggingface.co/jayn7/Z-Image-Turbo-GGUF/resolve/main/z_image_turbo-Q5_K_M.gguf",
          "sha256": "ea1b2f30b28fd52c73e631f0823494bf013f642d937e0ee1bdec4c1f1abc0103",
          "size": 5926789120,
          "format": "gguf",
          "precision": "q5_k_m",
          "vram_required": 8192,
          "vram_recommended": 10240,
          "note": "GGUF Q5_K_M â€” recommended for 8GB+ GPUs."
        },
        {
          "id": "gguf-q5-k-s",
          "file": "z_image_turbo-Q5_K_S.gguf",
          "url": "https://huggingface.co/jayn7/Z-Image-Turbo-GGUF/resolve/main/z_image_turbo-Q5_K_S.gguf",
          "sha256": "f00f7063d0d300cb9efbe82764edae2e956b34cd65220faa5f56cafff03065b5",
          "size": 5572034560,
          "format": "gguf",
          "precision": "q5_k_s",
          "vram_required": 8192,
          "vram_recommended": 10240,
          "note": "GGUF Q5_K_S â€” smaller Q5 variant."
        },
        {
          "id": "gguf-q4-k-m",
          "file": "z_image_turbo-Q4_K_M.gguf",
          "url": "https://huggingface.co/jayn7/Z-Image-Turbo-GGUF/resolve/main/z_image_turbo-Q4_K_M.gguf",
          "sha256": "8e2673db987bdc9248c336053ee773fb454eea3d3ef551b639f19812b6273503",
          "size": 5346283520,
          "format": "gguf",
          "precision": "q4_k_m",
          "vram_required": 6144,
          "vram_recommended": 8192,
          "note": "GGUF Q4_K_M â€” good for 8GB GPUs."
        },
        {
          "id": "gguf-q4-k-s",
          "file": "z_image_turbo-Q4_K_S.gguf",
          "url": "https://huggingface.co/jayn7/Z-Image-Turbo-GGUF/resolve/main/z_image_turbo-Q4_K_S.gguf",
          "sha256": "49cdba8a5db6ab01b8bed5fbb6796eacb83ed68cc2cc6f446d6bb0ec5fba982a",
          "size": 5003804672,
          "format": "gguf",
          "precision": "q4_k_s",
          "vram_required": 6144,
          "vram_recommended": 8192,
          "note": "GGUF Q4_K_S â€” smaller Q4 variant."
        },
        {
          "id": "gguf-q3-k-m",
          "file": "z_image_turbo-Q3_K_M.gguf",
          "url": "https://huggingface.co/jayn7/Z-Image-Turbo-GGUF/resolve/main/z_image_turbo-Q3_K_M.gguf",
          "sha256": "e660e8d66c1f41c8f2861d674a09492f64b97cd1c2c4e6ea0f39e9ceee02b969",
          "size": 4424990720,
          "format": "gguf",
          "precision": "q3_k_m",
          "vram_required": 6144,
          "vram_recommended": 8192,
          "note": "GGUF Q3_K_M â€” for 6GB+ GPUs. Noticeable quality reduction."
        },
        {
          "id": "gguf-q3-k-s",
          "file": "z_image_turbo-Q3_K_S.gguf",
          "url": "https://huggingface.co/jayn7/Z-Image-Turbo-GGUF/resolve/main/z_image_turbo-Q3_K_S.gguf",
          "sha256": "3d281cc0878d8a1f7184a63eef2ef13a0495bb9aaa25140886bf1068a13fc252",
          "size": 4069449728,
          "format": "gguf",
          "precision": "q3_k_s",
          "vram_required": 6144,
          "vram_recommended": 8192,
          "note": "GGUF Q3_K_S â€” smaller Q3 variant."
        }
      ],
      "requires": [
        {
          "id": "z-image-text-encoder",
          "type": "text_encoder",
          "reason": "Qwen 3 4B text encoder for prompt processing"
        },
        {
          "id": "z-image-vae",
          "type": "vae",
          "reason": "Z-Image VAE for decoding latents to images"
        }
      ],
      "recommended": {
        "steps": 8,
        "cfg": 0.0,
        "sampler": "euler",
        "scheduler": "simple"
      },
      "tags": [
        "z-image",
        "text-to-image",
        "turbo",
        "fast-inference",
        "bilingual",
        "comfyui",
        "gguf"
      ],
      "preview_images": [
        "https://huggingface.co/Comfy-Org/z_image_turbo/resolve/main/z_image_turbo_example.png"
      ],
      "rating": 4.9
    },
    {
      "id": "z-image-turbo-controlnet-union",
      "name": "Z-Image-Turbo Fun ControlNet Union",
      "type": "controlnet",
      "architecture": "z-image",
      "author": "alibaba-pai",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/alibaba-pai/Z-Image-Turbo-Fun-Controlnet-Union",
      "description": "ControlNet union model for Z-Image-Turbo.\nSupports Canny, HED, Depth, Pose, and MLSD control conditions.\nAdjust control_context_scale (0.65â€“0.80) for best results.\nUse with detailed prompts for better stability.\nV2.0 with inpaint mode also available separately.\n",
      "file": {
        "name": "Z-Image-Turbo-Fun-Controlnet-Union.safetensors",
        "url": "https://huggingface.co/alibaba-pai/Z-Image-Turbo-Fun-Controlnet-Union/resolve/main/Z-Image-Turbo-Fun-Controlnet-Union.safetensors",
        "sha256": "86c085c0d7853f12ce5183499934b54d08371c60f549c5a6b20615cd23989388",
        "size": 3328599040,
        "format": "safetensors"
      },
      "requires": [
        {
          "id": "z-image-turbo",
          "type": "diffusion_model",
          "reason": "Requires Z-Image-Turbo as the base diffusion model"
        }
      ],
      "tags": [
        "z-image",
        "controlnet",
        "canny",
        "depth",
        "pose",
        "comfyui"
      ],
      "rating": 4.6
    },
    {
      "id": "z-image-turbo-distill-lora",
      "name": "Z-Image-Turbo Distill Patch LoRA",
      "type": "lora",
      "architecture": "z-image",
      "author": "Comfy-Org",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/Comfy-Org/z_image_turbo",
      "description": "Distillation patch LoRA for Z-Image-Turbo.\nUsed to enable the base Z-Image model to run with fewer steps (turbo mode).\nApply via LoraLoader node with strength 1.0.\n",
      "file": {
        "name": "z_image_turbo_distill_patch_lora_bf16.safetensors",
        "url": "https://huggingface.co/Comfy-Org/z_image_turbo/resolve/main/split_files/loras/z_image_turbo_distill_patch_lora_bf16.safetensors",
        "sha256": "7247e6ad300373aacf1233d206223381727d9a9d19af1f2ff9db54ff1668a814",
        "size": 166723584,
        "format": "safetensors"
      },
      "tags": [
        "z-image",
        "lora",
        "distillation",
        "comfyui"
      ],
      "rating": 4.7
    },
    {
      "id": "z-image-vae",
      "name": "Z-Image VAE",
      "type": "vae",
      "architecture": "z-image",
      "author": "Comfy-Org",
      "license": "apache-2.0",
      "homepage": "https://huggingface.co/Comfy-Org/z_image_turbo",
      "description": "VAE for Z-Image-Turbo. Decodes latents to images.\nPlace in models/vae/ and load with VAELoader node.\n",
      "file": {
        "name": "z-image-vae.safetensors",
        "url": "https://huggingface.co/Comfy-Org/z_image_turbo/resolve/main/split_files/vae/ae.safetensors",
        "sha256": "afc8e28272cd15db3919bacdb6918ce9c1ed22e96cb12c4d5ed0fba823529e38",
        "size": 351272960,
        "format": "safetensors"
      },
      "tags": [
        "z-image",
        "vae",
        "comfyui"
      ],
      "rating": 5.0
    }
  ]
}
</file>

<file path="src/pages/models.astro">
---
import registry from '../data/registry.json';

interface RegistryItem {
  id: string;
  name: string;
  type: string;
  architecture?: string;
  author: string;
  license: string;
  homepage: string;
  description: string;
  file?: { url: string; sha256: string; size: number; format: string };
  variants?: { id: string; file: { size: number } }[];
  tags: string[];
  rating: number;
  downloads: number;
  added: string;
  updated: string;
  requires?: { id: string; type: string; reason: string }[];
  preview_images?: string[];
}

const items: RegistryItem[] = registry.items as RegistryItem[];

// Collect unique types for filter chips
const types = [...new Set(items.map(i => i.type))].sort();

// Helper: format file size
function formatSize(bytes: number): string {
  if (bytes >= 1e9) return `${(bytes / 1e9).toFixed(1)} GB`;
  if (bytes >= 1e6) return `${(bytes / 1e6).toFixed(0)} MB`;
  return `${(bytes / 1e3).toFixed(0)} KB`;
}

// Helper: get size display for a model
function getSize(item: RegistryItem): string {
  if (item.file) return formatSize(item.file.size);
  if (item.variants && item.variants.length > 0) {
    const sizes = item.variants
      .map((v: any) => v.file?.size)
      .filter((s: any) => s && s > 0);
    if (sizes.length === 0) return 'â€”';
    const min = Math.min(...sizes);
    const max = Math.max(...sizes);
    if (min === max) return formatSize(min);
    return `${formatSize(min)}â€“${formatSize(max)}`;
  }
  return 'â€”';
}

// Helper: format download count
function formatDownloads(n: number | undefined): string {
  if (!n) return 'â€”';
  if (n >= 1e6) return `${(n / 1e6).toFixed(1)}M`;
  if (n >= 1e3) return `${(n / 1e3).toFixed(0)}K`;
  return n.toString();
}

// Helper: stars display
function formatRating(r: number | undefined): string {
  if (!r) return 'â˜†â˜†â˜†â˜†â˜†';
  const full = Math.floor(r);
  const half = r - full >= 0.3;
  return 'â˜…'.repeat(full) + (half ? 'Â½' : '') + 'â˜†'.repeat(5 - full - (half ? 1 : 0));
}

// Type colors (same as landing page)
const typeColors: Record<string, string> = {
  checkpoint: '#3b82f6',
  diffusion_model: '#8b5cf6',
  text_encoder: '#06b6d4',
  lora: '#f59e0b',
  controlnet: '#10b981',
  ipadapter: '#ec4899',
  upscaler: '#f97316',
  vae: '#6366f1',
  embedding: '#84cc16',
  segmentation: '#14b8a6',
};

// Type display names
const typeLabels: Record<string, string> = {
  checkpoint: 'Checkpoint',
  diffusion_model: 'Diffusion Model',
  text_encoder: 'Text Encoder',
  lora: 'LoRA',
  controlnet: 'ControlNet',
  ipadapter: 'IP-Adapter',
  upscaler: 'Upscaler',
  vae: 'VAE',
  embedding: 'Embedding',
  segmentation: 'Segmentation',
};

// Sort: recently updated first (default)
const sorted = [...items].sort((a, b) => (b.updated || '').localeCompare(a.updated || ''));

// Pre-compute sort orders for client-side switching
// We store added/updated dates as data attributes on cards
---

<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Models â€” mods registry</title>
  <meta name="description" content={`Browse ${items.length} AI models. Checkpoints, LoRAs, VAEs, ControlNets, and more â€” all installable with a single command.`} />
  <link rel="icon" type="image/svg+xml" href="/favicon.svg" />
  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-3341W5L66B"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-3341W5L66B');
  </script>
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;700&family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet" />
</head>
<body>

  <!-- Nav -->
  <nav class="nav">
    <div class="nav-inner">
      <a href="/" class="nav-brand"><span class="gradient-text">mods</span></a>
      <div class="nav-links">
        <a href="/models" class="nav-link active">Models</a>
        <a href="https://github.com/modshq-org/mods" class="nav-link" target="_blank" rel="noopener">Docs</a>
        <a href="https://github.com/modshq-org/mods" class="nav-link" target="_blank" rel="noopener">GitHub</a>
      </div>
    </div>
  </nav>

  <!-- Header -->
  <header class="page-header">
    <div class="container">
      <h1>Model Library</h1>
      <p class="page-subtitle">
        {items.length} models for FLUX, Wan 2.2, LTX-Video, Stable Diffusion &amp; more.
        <br/>Install any model with <code>mods install &lt;name&gt;</code>
      </p>

      <!-- Search -->
      <div class="search-bar">
        <svg class="search-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="11" cy="11" r="8"/><path d="m21 21-4.3-4.3"/></svg>
        <input type="text" id="search" placeholder="Search models..." autocomplete="off" />
      </div>

      <!-- Filter chips -->
      <div class="filters" id="filters">
        <button class="chip active" data-type="all">All <span class="chip-count">{items.length}</span></button>
        {types.map(t => (
          <button
            class="chip"
            data-type={t}
            style={`--chip-color: ${typeColors[t] || '#888'}`}
          >
            {typeLabels[t] || t} <span class="chip-count">{items.filter(i => i.type === t).length}</span>
          </button>
        ))}
      </div>

      <!-- Sort dropdown -->
      <div class="sort-bar">
        <label class="sort-label" for="sort-select">Sort by</label>
        <select id="sort-select" class="sort-select">
          <option value="updated" selected>Recently Updated</option>
          <option value="popular">Most Popular</option>
          <option value="recent">Recently Added</option>
          <option value="rating">Highest Rated</option>
          <option value="name">A â†’ Z</option>
        </select>
      </div>
    </div>
  </header>

  <!-- Model Grid -->
  <main class="container">
    <div class="model-count" id="model-count">Showing {sorted.length} models</div>
    <div class="models-grid" id="models-grid">
      {sorted.map(model => (
        <article
          class="model-card"
          data-type={model.type}
          data-id={model.id}
          data-name={model.name.toLowerCase()}
          data-tags={model.tags.join(' ')}
          data-arch={model.architecture || ''}
          data-author={model.author.toLowerCase()}
          data-downloads={model.downloads || 0}
          data-rating={model.rating || 0}
          data-added={model.added || ''}
          data-updated={model.updated || ''}
        >
          <div class="card-top">
            <div class="card-header">
              <span
                class="type-badge"
                style={`background: ${typeColors[model.type] || '#666'}18; color: ${typeColors[model.type] || '#666'}; border-color: ${typeColors[model.type] || '#666'}30`}
              >
                {typeLabels[model.type] || model.type}
              </span>
              {model.variants && model.variants.length > 1 && (
                <span class="variants-badge">{model.variants.length} variants</span>
              )}
            </div>

            <h3 class="card-name">
              <a href={`/models/${model.id}`}>{model.name}</a>
            </h3>

            <p class="card-author">by {model.author}</p>

            <p class="card-desc">{model.description.split('\n')[0]}</p>
          </div>

          <div class="card-bottom">
            <div class="card-meta">
              <span class="meta-item" title={`${model.rating}/5`}>
                <span class="meta-stars">{formatRating(model.rating)}</span>
                <span class="meta-value">{model.rating}</span>
              </span>
              <span class="meta-item" title={`${(model.downloads || 0).toLocaleString()} downloads`}>
                <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4"/><polyline points="7 10 12 15 17 10"/><line x1="12" y1="15" x2="12" y2="3"/></svg>
                {formatDownloads(model.downloads)}
              </span>
              <span class="meta-item meta-size" title="File size">
                {getSize(model)}
              </span>
            </div>

            <div class="card-tags">
              {model.tags.slice(0, 4).map(tag => (
                <span class="tag">#{tag}</span>
              ))}
            </div>

            <div class="card-install">
              <code>mods install {model.id}</code>
              <button class="copy-btn" data-cmd={`mods install ${model.id}`} title="Copy to clipboard">
                <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect width="14" height="14" x="8" y="8" rx="2"/><path d="M4 16c-1.1 0-2-.9-2-2V4c0-1.1.9-2 2-2h10c1.1 0 2 .9 2 2"/></svg>
              </button>
            </div>

            {model.requires && model.requires.length > 0 && (
              <div class="card-deps">
                <span class="deps-label">Requires:</span>
                {model.requires.map((dep: any) => (
                  <a href={`#${dep.id}`} class="dep-link">{dep.id}</a>
                ))}
              </div>
            )}
          </div>
        </article>
      ))}
    </div>

    <div class="no-results" id="no-results" style="display: none;">
      <p>No models found matching your search.</p>
      <button class="btn-reset" id="btn-reset">Clear filters</button>
    </div>
  </main>

  <!-- Footer -->
  <footer>
    <div class="container">
      <div class="footer-content">
        <div class="footer-brand">
          <span class="gradient-text footer-logo">mods</span>
          <p>Open source model manager for AI image generation.</p>
        </div>
        <div class="footer-links">
          <a href="/">Home</a>
          <a href="/models">Models</a>
          <a href="https://github.com/modshq-org/mods" target="_blank" rel="noopener">CLI</a>
          <a href="https://github.com/modshq-org/mods-registry" target="_blank" rel="noopener">Registry</a>
        </div>
      </div>
      <div class="footer-bottom">
        <p>Built with Rust. Licensed MIT.</p>
      </div>
    </div>
  </footer>

  <script>
    // Client-side filtering and search
    const searchInput = document.getElementById('search') as HTMLInputElement;
    const filtersEl = document.getElementById('filters')!;
    const grid = document.getElementById('models-grid')!;
    const countEl = document.getElementById('model-count')!;
    const noResults = document.getElementById('no-results')!;
    const resetBtn = document.getElementById('btn-reset')!;
    const chips = filtersEl.querySelectorAll('.chip');
    const cards = grid.querySelectorAll('.model-card') as NodeListOf<HTMLElement>;
    const sortSelect = document.getElementById('sort-select') as HTMLSelectElement;

    let activeType = 'all';

    function sortCards() {
      const sortBy = sortSelect.value;
      const cardArray = Array.from(cards);

      cardArray.sort((a, b) => {
        switch (sortBy) {
          case 'popular':
            return (Number(b.dataset.downloads) || 0) - (Number(a.dataset.downloads) || 0);
          case 'recent':
            return (b.dataset.added || '').localeCompare(a.dataset.added || '');
          case 'updated':
            return (b.dataset.updated || '').localeCompare(a.dataset.updated || '');
          case 'rating':
            return (Number(b.dataset.rating) || 0) - (Number(a.dataset.rating) || 0);
          case 'name':
            return (a.dataset.name || '').localeCompare(b.dataset.name || '');
          default:
            return 0;
        }
      });

      cardArray.forEach(card => grid.appendChild(card));
    }

    function filterModels() {
      const q = searchInput.value.toLowerCase().trim();
      let visible = 0;

      cards.forEach(card => {
        const type = card.dataset.type || '';
        const name = card.dataset.name || '';
        const tags = card.dataset.tags || '';
        const id = card.dataset.id || '';
        const author = card.dataset.author || '';
        const arch = card.dataset.arch || '';

        const matchesType = activeType === 'all' || type === activeType;
        const matchesSearch = !q ||
          name.includes(q) ||
          id.includes(q) ||
          tags.includes(q) ||
          author.includes(q) ||
          arch.includes(q) ||
          type.replace('_', ' ').includes(q);

        if (matchesType && matchesSearch) {
          card.style.display = '';
          visible++;
        } else {
          card.style.display = 'none';
        }
      });

      countEl.textContent = `Showing ${visible} model${visible !== 1 ? 's' : ''}`;
      noResults.style.display = visible === 0 ? '' : 'none';
      grid.style.display = visible === 0 ? 'none' : '';
    }

    // Search input
    searchInput.addEventListener('input', filterModels);

    // Sort select
    sortSelect.addEventListener('change', () => {
      sortCards();
      filterModels();
    });

    // Filter chips
    chips.forEach(chip => {
      chip.addEventListener('click', () => {
        chips.forEach(c => c.classList.remove('active'));
        chip.classList.add('active');
        activeType = (chip as HTMLElement).dataset.type || 'all';
        filterModels();
      });
    });

    // Reset button
    resetBtn.addEventListener('click', () => {
      searchInput.value = '';
      activeType = 'all';
      chips.forEach(c => c.classList.remove('active'));
      chips[0]?.classList.add('active');
      filterModels();
    });

    // Copy buttons
    document.querySelectorAll('.copy-btn').forEach(btn => {
      btn.addEventListener('click', async (e) => {
        e.preventDefault();
        const cmd = (btn as HTMLElement).dataset.cmd || '';
        await navigator.clipboard.writeText(cmd);
        const svg = btn.querySelector('svg')!;
        const origHTML = svg.outerHTML;
        svg.outerHTML = '<svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="#22c55e" stroke-width="2"><polyline points="20 6 9 17 4 12"/></svg>';
        setTimeout(() => {
          btn.querySelector('svg')!.outerHTML = origHTML;
        }, 1500);
      });
    });

    // Keyboard shortcut: / to focus search
    document.addEventListener('keydown', (e) => {
      if (e.key === '/' && document.activeElement !== searchInput) {
        e.preventDefault();
        searchInput.focus();
      }
      if (e.key === 'Escape') {
        searchInput.blur();
        searchInput.value = '';
        filterModels();
      }
    });
  </script>
</body>
</html>

<style>
  :root {
    --bg: #0a0a0f;
    --bg-card: #12121a;
    --bg-card-hover: #1a1a28;
    --border: #1e1e2e;
    --border-light: #2a2a3e;
    --text: #e8e8f0;
    --text-dim: #a0a0b8;
    --text-dimmer: #6a6a82;
    --accent: #7c3aed;
    --accent-light: #a78bfa;
    --success: #22c55e;
    --warning: #eab308;
    --font-sans: 'Inter', system-ui, -apple-system, sans-serif;
    --font-mono: 'JetBrains Mono', 'Fira Code', monospace;
  }

  * { margin: 0; padding: 0; box-sizing: border-box; }

  html { scroll-behavior: smooth; }

  body {
    background: var(--bg);
    color: var(--text);
    font-family: var(--font-sans);
    line-height: 1.6;
    -webkit-font-smoothing: antialiased;
    min-height: 100vh;
    display: flex;
    flex-direction: column;
  }

  main { flex: 1; }

  .container {
    max-width: 1200px;
    margin: 0 auto;
    padding: 0 24px;
  }

  .gradient-text {
    background: linear-gradient(135deg, var(--accent-light), #c084fc, #f472b6);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
  }

  /* Nav */
  .nav {
    position: sticky;
    top: 0;
    z-index: 100;
    background: rgba(10, 10, 15, 0.85);
    backdrop-filter: blur(12px);
    border-bottom: 1px solid var(--border);
  }

  .nav-inner {
    max-width: 1200px;
    margin: 0 auto;
    padding: 0 24px;
    height: 60px;
    display: flex;
    align-items: center;
    justify-content: space-between;
  }

  .nav-brand {
    font-size: 1.5rem;
    font-weight: 800;
    text-decoration: none;
  }

  .nav-links {
    display: flex;
    gap: 24px;
  }

  .nav-link {
    color: var(--text-dim);
    text-decoration: none;
    font-size: 0.9rem;
    font-weight: 500;
    transition: color 0.2s;
  }

  .nav-link:hover, .nav-link.active {
    color: var(--text);
  }

  .nav-link.active {
    color: var(--accent-light);
  }

  /* Page Header */
  .page-header {
    padding: 60px 0 32px;
    text-align: center;
    border-bottom: 1px solid var(--border);
    margin-bottom: 32px;
  }

  .page-header h1 {
    font-size: 2.8rem;
    font-weight: 800;
    letter-spacing: -0.03em;
    margin-bottom: 12px;
  }

  .page-subtitle {
    color: var(--text-dim);
    font-size: 1.1rem;
    margin-bottom: 32px;
    line-height: 1.7;
  }

  .page-subtitle code {
    font-family: var(--font-mono);
    color: var(--accent-light);
    font-size: 0.95rem;
    background: rgba(124, 58, 237, 0.1);
    padding: 2px 8px;
    border-radius: 4px;
  }

  /* Search */
  .search-bar {
    position: relative;
    max-width: 480px;
    margin: 0 auto 24px;
  }

  .search-icon {
    position: absolute;
    left: 16px;
    top: 50%;
    transform: translateY(-50%);
    color: var(--text-dimmer);
    pointer-events: none;
  }

  .search-bar input {
    width: 100%;
    padding: 14px 16px 14px 48px;
    font-size: 1rem;
    font-family: var(--font-sans);
    background: var(--bg-card);
    border: 1px solid var(--border);
    border-radius: 12px;
    color: var(--text);
    outline: none;
    transition: border-color 0.2s, box-shadow 0.2s;
  }

  .search-bar input::placeholder {
    color: var(--text-dimmer);
  }

  .search-bar input:focus {
    border-color: var(--accent);
    box-shadow: 0 0 0 3px rgba(124, 58, 237, 0.1);
  }

  /* Filter Chips */
  .filters {
    display: flex;
    flex-wrap: wrap;
    gap: 8px;
    justify-content: center;
    padding-bottom: 32px;
  }

  .chip {
    display: inline-flex;
    align-items: center;
    gap: 6px;
    padding: 6px 14px;
    font-size: 0.82rem;
    font-weight: 500;
    font-family: var(--font-sans);
    border: 1px solid var(--border);
    border-radius: 100px;
    background: var(--bg-card);
    color: var(--text-dim);
    cursor: pointer;
    transition: all 0.15s;
    white-space: nowrap;
  }

  .chip:hover {
    border-color: var(--border-light);
    color: var(--text);
    background: var(--bg-card-hover);
  }

  .chip.active {
    background: rgba(124, 58, 237, 0.12);
    border-color: rgba(124, 58, 237, 0.3);
    color: var(--accent-light);
  }

  .chip.active[data-type]:not([data-type="all"]) {
    background: color-mix(in srgb, var(--chip-color) 12%, transparent);
    border-color: color-mix(in srgb, var(--chip-color) 30%, transparent);
    color: var(--chip-color);
  }

  .chip-count {
    font-size: 0.72rem;
    opacity: 0.6;
  }

  /* Sort bar */
  .sort-bar {
    display: flex;
    align-items: center;
    gap: 8px;
    justify-content: center;
    padding-bottom: 8px;
  }

  .sort-label {
    font-size: 0.82rem;
    color: var(--text-dimmer);
    font-weight: 500;
  }

  .sort-select {
    font-family: var(--font-sans);
    font-size: 0.82rem;
    padding: 6px 12px;
    background: var(--bg-card);
    border: 1px solid var(--border);
    border-radius: 8px;
    color: var(--text);
    cursor: pointer;
    outline: none;
    transition: border-color 0.2s;
  }

  .sort-select:hover { border-color: var(--border-light); }
  .sort-select:focus { border-color: var(--accent); }

  /* Model Count */
  .model-count {
    font-size: 0.85rem;
    color: var(--text-dimmer);
    margin-bottom: 20px;
  }

  /* Models Grid */
  .models-grid {
    display: grid;
    grid-template-columns: repeat(auto-fill, minmax(340px, 1fr));
    gap: 20px;
    padding-bottom: 60px;
  }

  /* Model Card */
  .model-card {
    display: flex;
    flex-direction: column;
    border: 1px solid var(--border);
    border-radius: 14px;
    background: var(--bg-card);
    transition: all 0.2s;
    overflow: hidden;
  }

  .model-card:hover {
    border-color: var(--border-light);
    background: var(--bg-card-hover);
    transform: translateY(-2px);
    box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
  }

  .card-top {
    padding: 24px 24px 16px;
    flex: 1;
  }

  .card-bottom {
    padding: 0 24px 20px;
  }

  .card-header {
    display: flex;
    align-items: center;
    gap: 8px;
    margin-bottom: 12px;
  }

  .type-badge {
    display: inline-block;
    padding: 3px 10px;
    border-radius: 100px;
    font-size: 0.72rem;
    font-weight: 600;
    letter-spacing: 0.02em;
    text-transform: uppercase;
    border: 1px solid;
  }

  .variants-badge {
    font-size: 0.72rem;
    color: var(--text-dimmer);
    background: rgba(255,255,255,0.04);
    padding: 3px 8px;
    border-radius: 100px;
    border: 1px solid var(--border);
  }

  .card-name {
    font-size: 1.15rem;
    font-weight: 700;
    margin-bottom: 4px;
    line-height: 1.3;
  }

  .card-name a {
    color: var(--text);
    text-decoration: none;
    transition: color 0.15s;
  }

  .card-name a:hover {
    color: var(--accent-light);
  }

  .card-author {
    font-size: 0.82rem;
    color: var(--text-dimmer);
    margin-bottom: 10px;
  }

  .card-desc {
    font-size: 0.88rem;
    color: var(--text-dim);
    line-height: 1.5;
    display: -webkit-box;
    -webkit-line-clamp: 2;
    -webkit-box-orient: vertical;
    overflow: hidden;
  }

  /* Card meta row */
  .card-meta {
    display: flex;
    align-items: center;
    gap: 16px;
    margin-bottom: 12px;
    padding-top: 14px;
    border-top: 1px solid var(--border);
  }

  .meta-item {
    display: inline-flex;
    align-items: center;
    gap: 4px;
    font-size: 0.78rem;
    color: var(--text-dim);
  }

  .meta-stars {
    color: var(--warning);
    font-size: 0.7rem;
    letter-spacing: -1px;
  }

  .meta-value {
    font-weight: 600;
  }

  .meta-size {
    margin-left: auto;
    font-family: var(--font-mono);
    font-size: 0.75rem;
    color: var(--text-dimmer);
  }

  /* Tags */
  .card-tags {
    display: flex;
    flex-wrap: wrap;
    gap: 6px;
    margin-bottom: 14px;
  }

  .tag {
    font-size: 0.72rem;
    color: var(--text-dimmer);
    background: rgba(255, 255, 255, 0.03);
    padding: 2px 8px;
    border-radius: 4px;
  }

  /* Install line */
  .card-install {
    display: flex;
    align-items: center;
    gap: 8px;
    padding: 10px 14px;
    background: #0a0a12;
    border: 1px solid var(--border);
    border-radius: 8px;
  }

  .card-install code {
    flex: 1;
    font-family: var(--font-mono);
    font-size: 0.78rem;
    color: var(--accent-light);
    white-space: nowrap;
    overflow: hidden;
    text-overflow: ellipsis;
  }

  .copy-btn {
    background: none;
    border: none;
    color: var(--text-dimmer);
    cursor: pointer;
    padding: 4px;
    border-radius: 4px;
    transition: color 0.15s, background 0.15s;
    display: flex;
    align-items: center;
    flex-shrink: 0;
  }

  .copy-btn:hover {
    color: var(--text);
    background: rgba(255,255,255,0.05);
  }

  /* Dependencies */
  .card-deps {
    margin-top: 10px;
    display: flex;
    align-items: center;
    flex-wrap: wrap;
    gap: 6px;
    font-size: 0.75rem;
  }

  .deps-label {
    color: var(--text-dimmer);
  }

  .dep-link {
    color: var(--accent-light);
    text-decoration: none;
    padding: 1px 6px;
    background: rgba(124, 58, 237, 0.08);
    border-radius: 4px;
    font-family: var(--font-mono);
    font-size: 0.7rem;
    transition: background 0.15s;
  }

  .dep-link:hover {
    background: rgba(124, 58, 237, 0.16);
  }

  /* No results */
  .no-results {
    text-align: center;
    padding: 80px 0;
    color: var(--text-dim);
  }

  .btn-reset {
    margin-top: 16px;
    padding: 8px 20px;
    font-size: 0.9rem;
    font-family: var(--font-sans);
    background: var(--bg-card);
    border: 1px solid var(--border);
    border-radius: 8px;
    color: var(--text);
    cursor: pointer;
    transition: all 0.15s;
  }

  .btn-reset:hover {
    background: var(--bg-card-hover);
    border-color: var(--border-light);
  }

  /* Footer */
  footer {
    padding: 60px 0 40px;
    border-top: 1px solid var(--border);
    margin-top: auto;
  }

  .footer-content {
    display: flex;
    justify-content: space-between;
    align-items: flex-start;
    margin-bottom: 32px;
  }

  .footer-logo {
    font-size: 1.8rem;
    font-weight: 800;
    display: block;
    margin-bottom: 8px;
  }

  .footer-brand p {
    color: var(--text-dim);
    font-size: 0.9rem;
  }

  .footer-links {
    display: flex;
    gap: 24px;
  }

  .footer-links a {
    color: var(--text-dim);
    text-decoration: none;
    font-size: 0.9rem;
    transition: color 0.2s;
  }

  .footer-links a:hover {
    color: var(--accent-light);
  }

  .footer-bottom {
    text-align: center;
    color: var(--text-dim);
    font-size: 0.8rem;
    padding-top: 24px;
    border-top: 1px solid var(--border);
  }

  /* Responsive */
  @media (max-width: 768px) {
    .page-header h1 { font-size: 2rem; }
    .models-grid { grid-template-columns: 1fr; }
    .filters { justify-content: flex-start; overflow-x: auto; flex-wrap: nowrap; padding-bottom: 24px; }
    .footer-content { flex-direction: column; gap: 24px; }
    .nav-links { gap: 16px; }
  }
</style>
</file>

<file path="src/pages/index.astro">
---
import registry from '../data/registry.json';
const modelCount = registry.items.length;

const features = [
  {
    icon: "ðŸ“¦",
    title: "One Command Install",
    description: "Install any model with a single command. Dependencies resolve automatically."
  },
  {
    icon: "ðŸ”—",
    title: "Content-Addressed Storage",
    description: "Models stored once, symlinked everywhere. No duplicates across ComfyUI, A1111, or InvokeAI."
  },
  {
    icon: "âš¡",
    title: "Smart Variant Selection",
    description: "Automatically picks the right fp16/fp8/GGUF variant for your GPU's VRAM."
  },
  {
    icon: "ðŸ”„",
    title: "Dependency Resolution",
    description: "Install a ControlNet and its base model, text encoder, and VAE all come along."
  },
  {
    icon: "ðŸ”",
    title: "Search & Discover",
    description: `Browse ${modelCount} models and growing. Filter by type, architecture, tags, or rating.`
  },
  {
    icon: "ðŸ©º",
    title: "Health Checks",
    description: "mods doctor finds broken symlinks, missing deps, and corrupt files instantly."
  }
];

// Featured model families â€” hero cards
const families = [
  {
    name: "FLUX",
    tagline: "State-of-the-art image generation",
    color: "#7c3aed",
    models: ["FLUX.1 Dev", "FLUX.1 Schnell", "FLUX.2 Dev", "FLUX.2 Klein 4B", "FLUX.2 Klein 9B", "FLUX Fill Dev", "FLUX Canny Dev", "FLUX Redux Dev"],
    install: "mods install flux2-dev",
  },
  {
    name: "Qwen",
    tagline: "Multimodal editing & generation",
    color: "#3b82f6",
    models: ["Qwen Image", "Qwen Image Edit", "Qwen Image Edit Lightning LoRA"],
    install: "mods install qwen-image",
  },
  {
    name: "Wan 2.2",
    tagline: "Video generation",
    color: "#14b8a6",
    models: ["Wan 2.2 TI2V 5B", "Wan 2.2 T2V 14B", "Wan 2.2 I2V 14B"],
    install: "mods install wan22-ti2v-5b",
  },
  {
    name: "LTX-Video",
    tagline: "Fast video generation",
    color: "#f59e0b",
    models: ["LTX-Video 2B", "LTX-Video 13B", "LTX-2 19B"],
    install: "mods install ltx-video-2b",
  },
];

// Other notable models
const moreModels = [
  { name: "SDXL Base 1.0", type: "checkpoint" },
  { name: "SD 1.5", type: "checkpoint" },
  { name: "Z-Image Turbo", type: "diffusion_model" },
  { name: "ControlNets", type: "controlnet" },
  { name: "IP-Adapters", type: "ipadapter" },
  { name: "4x UltraSharp", type: "upscaler" },
  { name: "BiRefNet", type: "segmentation" },
  { name: "LCM LoRAs", type: "lora" },
];

const typeColors: Record<string, string> = {
  checkpoint: "#3b82f6",
  diffusion_model: "#8b5cf6",
  text_encoder: "#06b6d4",
  lora: "#f59e0b",
  controlnet: "#10b981",
  ipadapter: "#ec4899",
  upscaler: "#f97316",
  vae: "#6366f1",
  segmentation: "#14b8a6",
};
---

<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>mods â€” Model Manager for AI Image Generation</title>
  <meta name="description" content="The ollama for image generation. Install, manage, and share Stable Diffusion, FLUX, Wan 2.2, and LTX-Video models with a single command." />
  <link rel="icon" type="image/svg+xml" href="/favicon.svg" />
  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-3341W5L66B"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-3341W5L66B');
  </script>
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;700&family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet" />
</head>
<body>
  <!-- Nav -->
  <nav class="nav">
    <div class="nav-inner">
      <a href="/" class="nav-brand"><span class="gradient-text">mods</span></a>
      <div class="nav-links">
        <a href="/models" class="nav-link">Models</a>
        <a href="https://github.com/modshq-org/mods" class="nav-link" target="_blank" rel="noopener">Docs</a>
        <a href="https://github.com/modshq-org/mods" class="nav-link" target="_blank" rel="noopener">GitHub</a>
      </div>
    </div>
  </nav>

  <!-- Hero -->
  <section class="hero">
    <div class="container">
      <div class="hero-badge">Open Source Model Manager</div>
      <h1>
        <span class="gradient-text">mods</span>
      </h1>
      <p class="hero-subtitle">
        The package manager for AI models.<br/>
        Install checkpoints, LoRAs, VAEs, and ControlNets with a single command.
      </p>
      <div class="terminal">
        <div class="terminal-header">
          <div class="terminal-dots">
            <span class="dot red"></span>
            <span class="dot yellow"></span>
            <span class="dot green"></span>
          </div>
          <span class="terminal-title">Terminal</span>
        </div>
        <div class="terminal-body">
          <div class="line"><span class="prompt">$</span> <span class="cmd">mods install</span> <span class="arg">z-image-turbo</span> <span class="flag">--variant</span> <span class="arg">gguf-q4-k-m</span></div>
          <div class="line output">â†’ Install plan for z-image-turbo:</div>
          <div class="line output">&nbsp;</div>
          <div class="line output">  <span class="success">â†“</span> Z-Image Text Encoder (text_encoder) [bf16] 8.04 GiB</div>
          <div class="line output">  <span class="success">â†“</span> Z-Image VAE (vae) 335.00 MiB</div>
          <div class="line output">  <span class="success">â†“</span> Z-Image-Turbo (diffusion_model) [gguf-q4-k-m] 4.98 GiB</div>
          <div class="line output">&nbsp;</div>
          <div class="line output">  Total download: <span class="highlight">5.31 GiB</span></div>
          <div class="line output">&nbsp;</div>
          <div class="line output"><span class="success">âœ“</span> Installed z-image-turbo successfully.</div>
        </div>
      </div>
      <div class="hero-actions">
        <a href="#install" class="btn btn-primary">Get Started</a>
        <a href="https://github.com/modshq-org/mods" class="btn btn-secondary" target="_blank" rel="noopener">
          <svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>
          GitHub
        </a>
      </div>
    </div>
  </section>

  <!-- Features -->
  <section class="features" id="features">
    <div class="container">
      <h2>Why <span class="gradient-text">mods</span>?</h2>
      <p class="section-subtitle">Stop manually downloading models. Stop juggling folders. Start shipping.</p>
      <div class="features-grid">
        {features.map(f => (
          <div class="feature-card">
            <div class="feature-icon">{f.icon}</div>
            <h3>{f.title}</h3>
            <p>{f.description}</p>
          </div>
        ))}
      </div>
    </div>
  </section>

  <!-- How it works -->
  <section class="how-it-works">
    <div class="container">
      <h2>How It Works</h2>
      <div class="steps">
        <div class="step">
          <div class="step-number">1</div>
          <div class="step-content">
            <h3>Search</h3>
            <div class="step-terminal">
              <code><span class="prompt">$</span> mods search flux</code>
            </div>
            <p>Browse {modelCount} models across checkpoints, LoRAs, VAEs, ControlNets, and more.</p>
          </div>
        </div>
        <div class="step">
          <div class="step-number">2</div>
          <div class="step-content">
            <h3>Install</h3>
            <div class="step-terminal">
              <code><span class="prompt">$</span> mods install flux-dev --variant fp8</code>
            </div>
            <p>One command. Dependencies auto-resolve. Smart VRAM-based variant selection.</p>
          </div>
        </div>
        <div class="step">
          <div class="step-number">3</div>
          <div class="step-content">
            <h3>Use</h3>
            <div class="step-terminal">
              <code><span class="prompt">$</span> mods list</code>
            </div>
            <p>Models are symlinked into your ComfyUI, A1111, or InvokeAI folders. Zero config needed.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Models -->
  <section class="models" id="models">
    <div class="container">
      <h2>Model Registry</h2>
      <p class="section-subtitle">{modelCount} models and growing. Image, video, upscalers, LoRAs â€” all one command away.</p>

      <!-- Featured families -->
      <div class="families-grid">
        {families.map(f => (
          <div class={`family-card ${f.comingSoon ? 'coming-soon' : ''}`} style={`--family-color: ${f.color}`}>
            <div class="family-header">
              <h3 class="family-name">{f.name}</h3>
              {f.comingSoon && <span class="coming-badge">Coming Soon</span>}
            </div>
            <p class="family-tagline">{f.tagline}</p>
            {f.models.length > 0 && (
              <ul class="family-models">
                {f.models.map(m => <li>{m}</li>)}
              </ul>
            )}
            {f.install && (
              <div class="family-install">
                <code><span class="prompt">$</span> {f.install}</code>
              </div>
            )}
          </div>
        ))}
      </div>

      <!-- More models row -->
      <div class="more-models">
        <h4 class="more-label">Plus {modelCount - families.reduce((s, f) => s + f.models.length, 0)}+ more</h4>
        <div class="more-chips">
          {moreModels.map(m => (
            <span class="more-chip" style={`--chip-bg: ${typeColors[m.type] || '#666'}`}>
              {m.name}
            </span>
          ))}
        </div>
      </div>

      <p class="models-count">Browse all {modelCount} models in the <a href="/models">model library</a></p>
    </div>
  </section>

  <!-- Install -->
  <section class="install-section" id="install">
    <div class="container">
      <h2>Get Started in 30 Seconds</h2>
      <div class="install-steps">
        <div class="install-card">
          <h3>Install</h3>
          <div class="code-block">
            <code>curl -fsSL https://mods.sh/install | sh</code>
          </div>
          <p class="install-alt">Or build from source:</p>
          <div class="code-block">
            <code>git clone https://github.com/modshq-org/mods<br/>cd mods && cargo install --path .</code>
          </div>
        </div>
        <div class="install-card">
          <h3>Link your tools</h3>
          <div class="code-block">
            <code>mods link --comfyui ~/ComfyUI</code>
          </div>
        </div>
        <div class="install-card">
          <h3>Start installing</h3>
          <div class="code-block">
            <code>mods install sdxl-base-1.0</code>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Comparison -->
  <section class="comparison">
    <div class="container">
      <h2>Before & After</h2>
      <div class="comparison-grid">
        <div class="comparison-card before">
          <h3>Without mods</h3>
          <ul>
            <li>Search HuggingFace/Civitai manually</li>
            <li>Download 10+ files for one workflow</li>
            <li>Figure out which folder each goes into</li>
            <li>Duplicate files across A1111 and ComfyUI</li>
            <li>Forget which VAE goes with which model</li>
            <li>Run out of disk space from duplicates</li>
          </ul>
        </div>
        <div class="comparison-card after">
          <h3>With mods</h3>
          <ul>
            <li><code>mods search flux</code></li>
            <li><code>mods install flux-dev</code> â€” deps included</li>
            <li>Auto-placed in the right folders</li>
            <li>Symlinks share one copy across tools</li>
            <li>Dependencies tracked automatically</li>
            <li><code>mods gc</code> reclaims unused space</li>
          </ul>
        </div>
      </div>
    </div>
  </section>

  <!-- Footer -->
  <footer>
    <div class="container">
      <div class="footer-content">
        <div class="footer-brand">
          <span class="gradient-text footer-logo">mods</span>
          <p>Open source model manager for AI image generation.</p>
        </div>
        <div class="footer-links">
          <a href="/models">Models</a>
          <a href="https://github.com/modshq-org/mods" target="_blank" rel="noopener">CLI</a>
          <a href="https://github.com/modshq-org/mods-registry" target="_blank" rel="noopener">Registry</a>
          <a href="https://github.com/modshq-org/mods/issues" target="_blank" rel="noopener">Issues</a>
        </div>
      </div>
      <div class="footer-bottom">
        <p>Built with Rust. Licensed MIT.</p>
      </div>
    </div>
  </footer>
</body>
</html>

<style>
  :root {
    --bg: #0a0a0f;
    --bg-card: #12121a;
    --bg-card-hover: #1a1a28;
    --border: #1e1e2e;
    --text: #e8e8f0;
    --text-dim: #a0a0b8;
    --accent: #7c3aed;
    --accent-light: #a78bfa;
    --success: #22c55e;
    --warning: #eab308;
    --font-sans: 'Inter', system-ui, -apple-system, sans-serif;
    --font-mono: 'JetBrains Mono', 'Fira Code', monospace;
  }

  * {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
  }

  html {
    scroll-behavior: smooth;
  }

  body {
    background: var(--bg);
    color: var(--text);
    font-family: var(--font-sans);
    line-height: 1.6;
    -webkit-font-smoothing: antialiased;
  }

  .container {
    max-width: 1100px;
    margin: 0 auto;
    padding: 0 24px;
  }

  .gradient-text {
    background: linear-gradient(135deg, var(--accent-light), #c084fc, #f472b6);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
  }

  /* Nav */
  .nav {
    position: sticky;
    top: 0;
    z-index: 100;
    background: rgba(10, 10, 15, 0.85);
    backdrop-filter: blur(12px);
    border-bottom: 1px solid var(--border);
  }

  .nav-inner {
    max-width: 1100px;
    margin: 0 auto;
    padding: 0 24px;
    height: 60px;
    display: flex;
    align-items: center;
    justify-content: space-between;
  }

  .nav-brand {
    font-size: 1.5rem;
    font-weight: 800;
    text-decoration: none;
  }

  .nav-links {
    display: flex;
    gap: 24px;
  }

  .nav-link {
    color: var(--text-dim);
    text-decoration: none;
    font-size: 0.95rem;
    font-weight: 500;
    transition: color 0.2s;
  }

  .nav-link:hover {
    color: var(--text);
  }

  /* Hero */
  .hero {
    padding: 80px 0 80px;
    text-align: center;
    position: relative;
    overflow: hidden;
  }

  .hero::before {
    content: '';
    position: absolute;
    top: -50%;
    left: 50%;
    transform: translateX(-50%);
    width: 800px;
    height: 800px;
    background: radial-gradient(circle, rgba(124, 58, 237, 0.12) 0%, transparent 70%);
    pointer-events: none;
  }

  .hero-badge {
    display: inline-block;
    padding: 6px 16px;
    border: 1px solid var(--border);
    border-radius: 100px;
    font-size: 0.85rem;
    color: var(--text-dim);
    margin-bottom: 32px;
    background: var(--bg-card);
  }

  .hero h1 {
    font-size: clamp(4rem, 12vw, 8rem);
    font-weight: 800;
    letter-spacing: -0.04em;
    line-height: 1;
    margin-bottom: 24px;
  }

  .hero-subtitle {
    font-size: 1.3rem;
    color: var(--text-dim);
    max-width: 560px;
    margin: 0 auto 48px;
    line-height: 1.7;
  }

  /* Terminal */
  .terminal {
    max-width: 680px;
    margin: 0 auto 48px;
    border-radius: 12px;
    border: 1px solid var(--border);
    background: #0d0d14;
    overflow: hidden;
    text-align: left;
    box-shadow: 0 24px 48px rgba(0, 0, 0, 0.4);
  }

  .terminal-header {
    display: flex;
    align-items: center;
    gap: 12px;
    padding: 12px 16px;
    background: #151520;
    border-bottom: 1px solid var(--border);
  }

  .terminal-dots {
    display: flex;
    gap: 6px;
  }

  .dot {
    width: 10px;
    height: 10px;
    border-radius: 50%;
  }

  .dot.red { background: #ff5f57; }
  .dot.yellow { background: #febc2e; }
  .dot.green { background: #28c840; }

  .terminal-title {
    font-size: 0.8rem;
    color: var(--text-dim);
    font-family: var(--font-mono);
  }

  .terminal-body {
    padding: 20px;
    font-family: var(--font-mono);
    font-size: 0.85rem;
    line-height: 1.8;
  }

  .line { white-space: pre; }
  .prompt { color: var(--success); }
  .cmd { color: var(--accent-light); }
  .arg { color: #f472b6; }
  .flag { color: var(--text-dim); }
  .output { color: var(--text-dim); }
  .success { color: var(--success); }
  .highlight { color: var(--warning); font-weight: 500; }

  /* Buttons */
  .hero-actions {
    display: flex;
    gap: 16px;
    justify-content: center;
    flex-wrap: wrap;
  }

  .btn {
    display: inline-flex;
    align-items: center;
    gap: 8px;
    padding: 12px 28px;
    border-radius: 8px;
    font-size: 1rem;
    font-weight: 600;
    text-decoration: none;
    transition: all 0.2s;
    cursor: pointer;
  }

  .btn-primary {
    background: var(--accent);
    color: white;
  }

  .btn-primary:hover {
    background: #6d28d9;
    transform: translateY(-1px);
    box-shadow: 0 8px 24px rgba(124, 58, 237, 0.3);
  }

  .btn-secondary {
    background: var(--bg-card);
    color: var(--text);
    border: 1px solid var(--border);
  }

  .btn-secondary:hover {
    background: var(--bg-card-hover);
    border-color: #2e2e40;
  }

  /* Features */
  .features, .how-it-works, .models, .install-section, .comparison {
    padding: 100px 0;
  }

  .features {
    border-top: 1px solid var(--border);
  }

  h2 {
    font-size: 2.5rem;
    font-weight: 700;
    text-align: center;
    margin-bottom: 12px;
    letter-spacing: -0.02em;
  }

  .section-subtitle {
    text-align: center;
    color: var(--text-dim);
    font-size: 1.15rem;
    margin-bottom: 56px;
  }

  .features-grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
    gap: 24px;
  }

  .feature-card {
    padding: 32px;
    border-radius: 12px;
    border: 1px solid var(--border);
    background: var(--bg-card);
    transition: all 0.2s;
  }

  .feature-card:hover {
    border-color: #2e2e40;
    background: var(--bg-card-hover);
    transform: translateY(-2px);
  }

  .feature-icon {
    font-size: 2rem;
    margin-bottom: 16px;
  }

  .feature-card h3 {
    font-size: 1.2rem;
    margin-bottom: 8px;
    font-weight: 600;
  }

  .feature-card p {
    color: var(--text-dim);
    font-size: 1rem;
    line-height: 1.6;
  }

  /* Steps */
  .steps {
    display: flex;
    flex-direction: column;
    gap: 48px;
    max-width: 640px;
    margin: 56px auto 0;
  }

  .step {
    display: flex;
    gap: 24px;
    align-items: flex-start;
  }

  .step-number {
    width: 48px;
    height: 48px;
    min-width: 48px;
    border-radius: 12px;
    background: linear-gradient(135deg, var(--accent), #a855f7);
    display: flex;
    align-items: center;
    justify-content: center;
    font-weight: 700;
    font-size: 1.2rem;
  }

  .step-content h3 {
    font-size: 1.3rem;
    margin-bottom: 8px;
  }

  .step-terminal {
    padding: 12px 16px;
    background: #0d0d14;
    border: 1px solid var(--border);
    border-radius: 8px;
    font-family: var(--font-mono);
    font-size: 0.85rem;
    margin: 12px 0;
  }

  .step-content p {
    color: var(--text-dim);
    font-size: 1rem;
  }

  /* Models / Families */
  .models {
    border-top: 1px solid var(--border);
  }

  .families-grid {
    display: grid;
    grid-template-columns: repeat(3, 1fr);
    gap: 24px;
    margin-bottom: 48px;
  }

  .family-card {
    padding: 28px;
    border-radius: 14px;
    border: 1px solid var(--border);
    background: var(--bg-card);
    transition: all 0.25s;
    position: relative;
    overflow: hidden;
  }

  .family-card::before {
    content: '';
    position: absolute;
    top: 0;
    left: 0;
    right: 0;
    height: 3px;
    background: var(--family-color);
    opacity: 0.7;
  }

  .family-card:hover {
    border-color: color-mix(in srgb, var(--family-color) 40%, transparent);
    background: var(--bg-card-hover);
    transform: translateY(-2px);
    box-shadow: 0 12px 32px rgba(0, 0, 0, 0.3);
  }

  .family-card.coming-soon {
    opacity: 0.6;
  }

  .family-header {
    display: flex;
    align-items: center;
    gap: 10px;
    margin-bottom: 6px;
  }

  .family-name {
    font-size: 1.5rem;
    font-weight: 700;
    color: var(--text);
  }

  .coming-badge {
    font-size: 0.7rem;
    font-weight: 600;
    padding: 3px 10px;
    border-radius: 100px;
    background: rgba(255, 255, 255, 0.08);
    color: var(--text-dim);
    text-transform: uppercase;
    letter-spacing: 0.05em;
  }

  .family-tagline {
    color: var(--text-dim);
    font-size: 1rem;
    margin-bottom: 16px;
  }

  .family-models {
    list-style: none;
    display: flex;
    flex-wrap: wrap;
    gap: 6px;
    margin-bottom: 16px;
  }

  .family-models li {
    font-size: 0.82rem;
    padding: 3px 10px;
    border-radius: 6px;
    background: rgba(255, 255, 255, 0.05);
    color: var(--text-dim);
    border: 1px solid var(--border);
  }

  .family-install {
    padding: 10px 14px;
    background: #0d0d14;
    border: 1px solid var(--border);
    border-radius: 8px;
    font-family: var(--font-mono);
    font-size: 0.82rem;
    color: var(--accent-light);
  }

  .family-install .prompt {
    color: var(--success);
  }

  /* More models */
  .more-models {
    text-align: center;
    margin-bottom: 24px;
  }

  .more-label {
    font-size: 1rem;
    color: var(--text-dim);
    font-weight: 500;
    margin-bottom: 16px;
  }

  .more-chips {
    display: flex;
    flex-wrap: wrap;
    gap: 8px;
    justify-content: center;
  }

  .more-chip {
    font-size: 0.85rem;
    padding: 5px 14px;
    border-radius: 100px;
    background: color-mix(in srgb, var(--chip-bg) 12%, transparent);
    color: var(--chip-bg);
    font-weight: 500;
  }

  .models-count {
    text-align: center;
    margin-top: 32px;
    color: var(--text-dim);
  }

  .models-count a {
    color: var(--accent-light);
    text-decoration: none;
  }

  .models-count a:hover {
    text-decoration: underline;
  }

  /* Install section */
  .install-section {
    border-top: 1px solid var(--border);
    background: linear-gradient(180deg, transparent, rgba(124, 58, 237, 0.04));
  }

  .install-steps {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
    gap: 24px;
    margin-top: 56px;
  }

  .install-card {
    padding: 32px;
    border-radius: 12px;
    border: 1px solid var(--border);
    background: var(--bg-card);
  }

  .install-card h3 {
    font-size: 1.2rem;
    margin-bottom: 16px;
    font-weight: 600;
  }

  .code-block {
    padding: 14px 18px;
    background: #0a0a12;
    border: 1px solid var(--border);
    border-radius: 8px;
    font-family: var(--font-mono);
    font-size: 0.82rem;
    overflow-x: auto;
    color: var(--accent-light);
  }

  .install-alt {
    font-size: 0.85rem;
    color: var(--text-dim);
    margin: 12px 0 8px;
  }

  /* Comparison */
  .comparison-grid {
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 24px;
    margin-top: 56px;
  }

  .comparison-card {
    padding: 32px;
    border-radius: 12px;
    border: 1px solid var(--border);
  }

  .comparison-card.before {
    background: rgba(239, 68, 68, 0.04);
    border-color: rgba(239, 68, 68, 0.15);
  }

  .comparison-card.after {
    background: rgba(34, 197, 94, 0.04);
    border-color: rgba(34, 197, 94, 0.15);
  }

  .comparison-card h3 {
    font-size: 1.2rem;
    margin-bottom: 20px;
    font-weight: 600;
  }

  .before h3 { color: #f87171; }
  .after h3 { color: var(--success); }

  .comparison-card ul {
    list-style: none;
  }

  .comparison-card li {
    padding: 8px 0;
    color: var(--text-dim);
    font-size: 1rem;
    border-bottom: 1px solid var(--border);
  }

  .comparison-card li:last-child {
    border-bottom: none;
  }

  .comparison-card code {
    font-family: var(--font-mono);
    color: var(--accent-light);
    font-size: 0.85rem;
  }

  /* Footer */
  footer {
    padding: 60px 0 40px;
    border-top: 1px solid var(--border);
  }

  .footer-content {
    display: flex;
    justify-content: space-between;
    align-items: flex-start;
    margin-bottom: 32px;
  }

  .footer-logo {
    font-size: 1.8rem;
    font-weight: 800;
    display: block;
    margin-bottom: 8px;
  }

  .footer-brand p {
    color: var(--text-dim);
    font-size: 0.9rem;
  }

  .footer-links {
    display: flex;
    gap: 24px;
  }

  .footer-links a {
    color: var(--text-dim);
    text-decoration: none;
    font-size: 0.9rem;
    transition: color 0.2s;
  }

  .footer-links a:hover {
    color: var(--accent-light);
  }

  .footer-bottom {
    text-align: center;
    color: var(--text-dim);
    font-size: 0.8rem;
    padding-top: 24px;
    border-top: 1px solid var(--border);
  }

  /* Responsive */
  @media (max-width: 768px) {
    .hero { padding: 80px 0 60px; }
    .hero h1 { font-size: 4rem; }
    .terminal-body { font-size: 0.75rem; overflow-x: auto; }
    .comparison-grid { grid-template-columns: 1fr; }
    .families-grid { grid-template-columns: 1fr; }
    .footer-content { flex-direction: column; gap: 24px; }
    .features, .how-it-works, .models, .install-section, .comparison { padding: 64px 0; }
  }
</style>
</file>

</files>
